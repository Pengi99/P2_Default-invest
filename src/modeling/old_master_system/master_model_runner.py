"""
마스터 모델 실행 스크립트
Normal/SMOTE 데이터에 대해 3개 모델(LogisticRegression, RandomForest, XGBoost)을 자동 실행
"""

import pandas as pd
import numpy as np
import json
import os
import warnings
from datetime import datetime
from pathlib import Path
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

# 한글 폰트 설정
plt.rcParams['font.family'] = 'AppleGothic' if plt.matplotlib.get_backend() != 'Agg' else 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score, balanced_accuracy_score
)
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
from imblearn.over_sampling import BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks
from ensemble_model import EnsembleModel

class MasterModelRunner:
    def __init__(self, config):
        """
        마스터 모델 러너 초기화
        
        Args:
            config (dict): 설정 딕셔너리
        """
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"{config['run_name']}_{self.timestamp}"
        
        # 결과 저장 경로 설정
        self.output_dir = Path(config['output_base_dir']) / self.run_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # 하위 디렉토리 생성
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'results').mkdir(exist_ok=True)
        (self.output_dir / 'visualizations').mkdir(exist_ok=True)
        
        # 데이터 저장 변수
        self.data = {}
        self.models = {}
        self.results = {}
        self.selected_features = None
        
        print(f"🚀 마스터 모델 러너 초기화 완료")
        print(f"📁 실행 이름: {self.run_name}")
        print(f"📁 출력 경로: {self.output_dir}")
        
    def load_data(self):
        """데이터 로드"""
        print("\n📂 데이터 로드")
        print("="*60)
        
        data_path = Path(self.config['data_path'])
        
        # 원본 데이터 (SMOTE는 동적으로 적용)
        # self.data['normal'] = {
        #     'X_train': pd.read_csv(data_path / 'X_train_100_normal.csv'),
        #     'X_valid': pd.read_csv(data_path / 'X_valid_100_normal.csv'),
        #     'X_test': pd.read_csv(data_path / 'X_test_100_normal.csv'),
        #     'y_train': pd.read_csv(data_path / 'y_train_100_normal.csv').iloc[:, 0],
        #     'y_valid': pd.read_csv(data_path / 'y_valid_100_normal.csv').iloc[:, 0],
        #     'y_test': pd.read_csv(data_path / 'y_test_100_normal.csv').iloc[:, 0]
        # }
        self.data['normal'] = {
            'X_train': pd.read_csv(data_path / 'X_train.csv'),
            'X_valid': pd.read_csv(data_path / 'X_val.csv'),
            'X_test': pd.read_csv(data_path / 'X_test.csv'),
            'y_train': pd.read_csv(data_path / 'y_train.csv').iloc[:, 0],
            'y_valid': pd.read_csv(data_path / 'y_val.csv').iloc[:, 0],
            'y_test': pd.read_csv(data_path / 'y_test.csv').iloc[:, 0]
        }
        
        # 활성화된 데이터 타입별로 복사 (동적 샘플링 적용)
        enabled_data_types = [dt for dt, config in self.config['data_types'].items() if config['enabled']]
        for data_type in enabled_data_types:
            if data_type != 'normal':
                self.data[data_type] = self.data['normal'].copy()
        
        # Normal 데이터 정보 출력
        data = self.data['normal']
        print(f"✅ NORMAL 데이터:")
        print(f"   Train: {data['X_train'].shape}, 부실비율: {data['y_train'].mean():.2%}")
        print(f"   Valid: {data['X_valid'].shape}, 부실비율: {data['y_valid'].mean():.2%}")
        print(f"   Test: {data['X_test'].shape}, 부실비율: {data['y_test'].mean():.2%}")
        
        # 활성화된 데이터 타입별 정보 출력
        for data_type in enabled_data_types:
            if data_type == 'normal':
                continue
            elif data_type == 'smote':
                config = self.config['data_types']['smote']
                print(f"✅ SMOTE 데이터:")
                print(f"   원본과 동일한 크기 (동적 적용): {data['X_train'].shape}")
                print(f"   🔄 SMOTE는 CV 및 최종 훈련 시 동적으로 적용됩니다")
                print(f"   🎯 목표 부실비율: {config['sampling_strategy']*100:.0f}% (BorderlineSMOTE)")
                print(f"   🚫 Data Leakage 방지: CV 내부에서만 적용")
            elif data_type == 'undersampling':
                config = self.config['data_types']['undersampling']
                print(f"✅ UNDERSAMPLING 데이터:")
                print(f"   원본과 동일한 크기 (동적 적용): {data['X_train'].shape}")
                print(f"   🔄 언더샘플링은 CV 및 최종 훈련 시 동적으로 적용됩니다")
                print(f"   🎯 방법: {config['method']} (sampling_strategy: {config['sampling_strategy']})")
                print(f"   🚫 Data Leakage 방지: CV 내부에서만 적용")
            elif data_type == 'combined':
                config = self.config['data_types']['combined']
                print(f"✅ COMBINED 데이터:")
                print(f"   원본과 동일한 크기 (동적 적용): {data['X_train'].shape}")
                print(f"   🔄 SMOTE + 언더샘플링 조합이 동적으로 적용됩니다")
                print(f"   🎯 SMOTE 비율: {config['smote_ratio']*100:.0f}%, 언더샘플링 비율: {config['undersampling_ratio']*100:.0f}%")
                print(f"   🚫 Data Leakage 방지: CV 내부에서만 적용")
    
    def apply_sampling_strategy(self, X, y, data_type):
        """
        데이터 타입에 따른 샘플링 전략 적용
        
        Args:
            X: 특성 데이터
            y: 타겟 데이터
            data_type: 데이터 타입 ('normal', 'smote', 'undersampling', 'combined')
            
        Returns:
            tuple: (X_resampled, y_resampled)
        """
        if data_type == 'normal':
            return X, y
        
        elif data_type == 'smote':
            config = self.config['data_types']['smote']
            smote = BorderlineSMOTE(
                sampling_strategy=config['sampling_strategy'],
                random_state=self.config['random_state'],
                k_neighbors=config['k_neighbors'],
                m_neighbors=config['m_neighbors']
            )
            return smote.fit_resample(X, y)
        
        elif data_type == 'undersampling':
            config = self.config['data_types']['undersampling']
            
            if config['method'] == 'random':
                undersampler = RandomUnderSampler(
                    sampling_strategy=config['sampling_strategy'],
                    random_state=config['random_state']
                )
            elif config['method'] == 'edited_nearest_neighbours':
                undersampler = EditedNearestNeighbours(
                    sampling_strategy=config['sampling_strategy']
                )
            elif config['method'] == 'tomek':
                undersampler = TomekLinks(
                    sampling_strategy=config['sampling_strategy']
                )
            else:
                print(f"⚠️ 지원하지 않는 언더샘플링 방법: {config['method']}, Random 사용")
                undersampler = RandomUnderSampler(
                    sampling_strategy=config['sampling_strategy'],
                    random_state=config['random_state']
                )
            
            return undersampler.fit_resample(X, y)
        
        elif data_type == 'combined':
            config = self.config['data_types']['combined']
            
            # 1단계: SMOTE 적용
            smote = BorderlineSMOTE(
                sampling_strategy=config['smote_ratio'],
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            X_smote, y_smote = smote.fit_resample(X, y)
            
            # 2단계: 언더샘플링 적용
            undersampler = RandomUnderSampler(
                sampling_strategy=config['undersampling_ratio'],
                random_state=self.config['random_state']
            )
            X_combined, y_combined = undersampler.fit_resample(X_smote, y_smote)
            
            return X_combined, y_combined
        
        else:
            print(f"⚠️ 알 수 없는 데이터 타입: {data_type}, 원본 데이터 반환")
            return X, y

    def apply_lasso_feature_selection(self, data_type):
        """Lasso 특성 선택 적용"""
        if not self.config['lasso']['enabled']:
            return
            
        print(f"\n🔍 Lasso 특성 선택 적용 ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        # 스케일링
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Lasso CV
        lasso_cv = LassoCV(
            alphas=self.config['lasso']['alphas'],
            cv=self.config['lasso']['cv_folds'],
            random_state=self.config['random_state'],
            n_jobs=-1
        )
        lasso_cv.fit(X_train_scaled, y_train)
        
        # 특성 선택
        threshold = self.config['lasso']['threshold']
        if threshold == 'median':
            threshold_value = np.median(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        elif threshold == 'mean':
            threshold_value = np.mean(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        else:
            threshold_value = float(threshold)
        
        selected_mask = np.abs(lasso_cv.coef_) >= threshold_value
        selected_features = X_train.columns[selected_mask].tolist()
        
        print(f"✅ 최적 alpha: {lasso_cv.alpha_:.6f}")
        print(f"📊 선택된 특성: {len(selected_features)}/{len(X_train.columns)}")
        print(f"🎯 선택된 특성: {selected_features}")
        
        # 데이터 업데이트
        for split in ['X_train', 'X_valid', 'X_test']:
            self.data[data_type][split] = self.data[data_type][split][selected_features]
        
        # 결과 저장
        lasso_results = {
            'optimal_alpha': float(lasso_cv.alpha_),
            'threshold_value': float(threshold_value),
            'original_features': len(X_train.columns),
            'selected_features': len(selected_features),
            'selected_feature_names': selected_features,
            'feature_coefficients': dict(zip(X_train.columns, lasso_cv.coef_))
        }
        
        # results 디렉토리 존재 확인 및 생성
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(parents=True, exist_ok=True)
        
        with open(results_dir / f'lasso_selection_{data_type}.json', 'w') as f:
            json.dump(lasso_results, f, indent=2, ensure_ascii=False)
    
    def optimize_logistic_regression(self, data_type):
        """로지스틱 회귀 최적화"""
        print(f"\n🔍 로지스틱 회귀 최적화 ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            # penalty와 solver를 하나의 조합으로 선택 (동적 선택지 문제 해결)
            penalty_solver_combinations = []
            
            # 가능한 모든 penalty-solver 조합 생성
            for penalty in self.config['models']['logistic']['penalty']:
                if penalty == 'l1':
                    for solver in ['liblinear', 'saga']:
                        penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'l2':
                    for solver in self.config['models']['logistic']['l2_solvers']:
                        if solver in ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']:
                            penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'elasticnet':
                    penalty_solver_combinations.append(f"{penalty}_saga")
            
            # 조합 선택
            combination = trial.suggest_categorical('penalty_solver', penalty_solver_combinations)
            penalty, solver = combination.split('_', 1)
            
            C = trial.suggest_float('C', *self.config['models']['logistic']['C_range'], log=True)
            max_iter = trial.suggest_int('max_iter', *self.config['models']['logistic']['max_iter_range'])
            
            params = {
                'penalty': penalty,
                'C': C,
                'solver': solver,
                'max_iter': max_iter,
                'random_state': self.config['random_state']
            }
            
            if penalty == 'elasticnet':
                params['l1_ratio'] = trial.suggest_float('l1_ratio', *self.config['models']['logistic']['l1_ratio_range'])
            
            model = LogisticRegression(**params)
            
            # Data Leakage 방지를 위한 올바른 CV (샘플링 데이터 타입인 경우)
            if data_type != 'normal':
                # 원본 데이터 로드 (샘플링 적용 전)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_sampling(model, X_train_original, y_train_original, data_type, cv_folds=5)
            else:
                # Normal 데이터는 기존 방식 사용
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['logistic']['n_trials'])
        
        # 최적 모델 훈련
        best_params = study.best_params.copy()
        
        # penalty_solver 조합을 분리
        penalty_solver = best_params.pop('penalty_solver')
        penalty, solver = penalty_solver.split('_', 1)
        
        # 분리된 penalty와 solver 추가
        best_params['penalty'] = penalty
        best_params['solver'] = solver
        
        model = LogisticRegression(**best_params)
        
        # 샘플링 데이터 타입인 경우 최종 훈련에도 샘플링 적용
        if data_type != 'normal':
            X_train_resampled, y_train_resampled = self.apply_sampling_strategy(X_train, y_train, data_type)
            model.fit(X_train_resampled, y_train_resampled)
            print(f"✅ {data_type.upper()} 적용 후 훈련: {len(X_train_resampled):,}개 샘플")
            print(f"   부실비율: {y_train_resampled.mean():.2%}")
        else:
            model.fit(X_train, y_train)
        
        # 저장
        model_key = f'LogisticRegression_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type
        }
        
        print(f"✅ 최적 AUC: {study.best_value:.4f}")
        print(f"📊 최적 파라미터: {best_params}")
        
        return model
    
    def optimize_random_forest(self, data_type):
        """랜덤 포레스트 최적화"""
        print(f"\n🌲 랜덤 포레스트 최적화 ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *self.config['models']['random_forest']['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *self.config['models']['random_forest']['max_depth_range']),
                'min_samples_split': trial.suggest_int('min_samples_split', *self.config['models']['random_forest']['min_samples_split_range']),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', *self.config['models']['random_forest']['min_samples_leaf_range']),
                'max_features': trial.suggest_float('max_features', *self.config['models']['random_forest']['max_features_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1
            }
            
            model = RandomForestClassifier(**params)
            
            # Data Leakage 방지를 위한 올바른 CV (샘플링 데이터 타입인 경우)
            if data_type != 'normal':
                # 원본 데이터 로드 (샘플링 적용 전)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_sampling(model, X_train_original, y_train_original, data_type, cv_folds=5)
            else:
                # Normal 데이터는 기존 방식 사용
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['random_forest']['n_trials'])
        
        # 최적 모델 훈련
        best_params = study.best_params
        model = RandomForestClassifier(**best_params)
        
        # 샘플링 데이터 타입인 경우 최종 훈련에도 샘플링 적용
        if data_type != 'normal':
            X_train_resampled, y_train_resampled = self.apply_sampling_strategy(X_train, y_train, data_type)
            model.fit(X_train_resampled, y_train_resampled)
            print(f"✅ {data_type.upper()} 적용 후 훈련: {len(X_train_resampled):,}개 샘플")
            print(f"   부실비율: {y_train_resampled.mean():.2%}")
        else:
            model.fit(X_train, y_train)
        
        # 저장
        model_key = f'RandomForest_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type,
            'feature_importances': dict(zip(X_train.columns, model.feature_importances_))
        }
        
        print(f"✅ 최적 AUC: {study.best_value:.4f}")
        print(f"📊 최적 파라미터: {best_params}")
        
        return model
    
    def optimize_xgboost(self, data_type):
        """XGBoost 최적화"""
        print(f"\n🚀 XGBoost 최적화 ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'n_estimators': trial.suggest_int('n_estimators', *self.config['models']['xgboost']['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *self.config['models']['xgboost']['max_depth_range']),
                'learning_rate': trial.suggest_float('learning_rate', *self.config['models']['xgboost']['learning_rate_range']),
                'subsample': trial.suggest_float('subsample', *self.config['models']['xgboost']['subsample_range']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *self.config['models']['xgboost']['colsample_bytree_range']),
                'reg_alpha': trial.suggest_float('reg_alpha', *self.config['models']['xgboost']['reg_alpha_range']),
                'reg_lambda': trial.suggest_float('reg_lambda', *self.config['models']['xgboost']['reg_lambda_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1,
                'verbosity': 0
            }
            
            model = xgb.XGBClassifier(**params)
            
            # Data Leakage 방지를 위한 올바른 CV (샘플링 데이터 타입인 경우)
            if data_type != 'normal':
                # 원본 데이터 로드 (샘플링 적용 전)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_sampling(model, X_train_original, y_train_original, data_type, cv_folds=5)
            else:
                # Normal 데이터는 기존 방식 사용
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['xgboost']['n_trials'])
        
        # 최적 모델 훈련
        best_params = study.best_params
        model = xgb.XGBClassifier(**best_params)
        
        # 샘플링 데이터 타입인 경우 최종 훈련에도 샘플링 적용
        if data_type != 'normal':
            X_train_resampled, y_train_resampled = self.apply_sampling_strategy(X_train, y_train, data_type)
            model.fit(X_train_resampled, y_train_resampled)
            print(f"✅ {data_type.upper()} 적용 후 훈련: {len(X_train_resampled):,}개 샘플")
            print(f"   부실비율: {y_train_resampled.mean():.2%}")
        else:
            model.fit(X_train, y_train)
        
        # 저장
        model_key = f'XGBoost_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type,
            'feature_importances': dict(zip(X_train.columns, model.feature_importances_))
        }
        
        print(f"✅ 최적 AUC: {study.best_value:.4f}")
        print(f"📊 최적 파라미터: {best_params}")
        
        return model
    
    def find_optimal_threshold(self, model_key):
        """각 모델별 최적 threshold 찾기"""
        print(f"\n🎯 {model_key} 최적 Threshold 탐색")
        print("="*60)
        
        model = self.models[model_key]
        data_type = self.results[model_key]['data_type']
        
        X_valid = self.data[data_type]['X_valid']
        y_valid = self.data[data_type]['y_valid']
        
        # 검증 데이터에 대한 예측 확률
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
        
        # Precision-Recall 곡선과 다양한 threshold에서의 성능 계산
        thresholds = np.arange(0.05, 0.5, 0.05)  # 0.05부터 0.5까지 0.05 간격
        
        threshold_results = []
        
        for threshold in thresholds:
            y_valid_pred = (y_valid_proba >= threshold).astype(int)
            
            # 예측값이 모두 0이거나 1인 경우 스킵
            if len(np.unique(y_valid_pred)) == 1:
                continue
            
            try:
                metrics = {
                    'threshold': threshold,
                    'precision': precision_score(y_valid, y_valid_pred, zero_division=0),
                    'recall': recall_score(y_valid, y_valid_pred, zero_division=0),
                    'f1': f1_score(y_valid, y_valid_pred, zero_division=0),
                    'balanced_accuracy': balanced_accuracy_score(y_valid, y_valid_pred)
                }
                threshold_results.append(metrics)
            except:
                continue
        
        if not threshold_results:
            print("⚠️ 최적 threshold 찾기 실패, 기본값 0.5 사용")
            return 0.5, {}
        
        # 결과를 DataFrame으로 변환
        threshold_df = pd.DataFrame(threshold_results)
        
        # 주요 메트릭별 최적 threshold 찾기
        metric_priority = self.config.get('threshold_optimization', {}).get('metric_priority', 'f1')
        
        optimal_thresholds = {}
        for metric in ['f1', 'precision', 'recall', 'balanced_accuracy']:
            if metric in threshold_df.columns:
                best_idx = threshold_df[metric].idxmax()
                optimal_thresholds[metric] = {
                    'threshold': threshold_df.loc[best_idx, 'threshold'],
                    'value': threshold_df.loc[best_idx, metric]
                }
        
        # 우선순위 메트릭으로 최종 threshold 선택
        if metric_priority in optimal_thresholds:
            final_threshold = optimal_thresholds[metric_priority]['threshold']
            final_value = optimal_thresholds[metric_priority]['value']
        else:
            final_threshold = optimal_thresholds['f1']['threshold']
            final_value = optimal_thresholds['f1']['value']
        
        print(f"📈 Threshold 최적화 결과:")
        for metric, result in optimal_thresholds.items():
            marker = "🎯" if metric == metric_priority else "  "
            print(f"{marker} {metric.upper()}: {result['threshold']:.3f} (값: {result['value']:.4f})")
        
        print(f"\n✅ 최종 선택: {final_threshold:.3f} ({metric_priority.upper()}: {final_value:.4f})")
        
        # Precision-Recall 곡선 데이터 저장
        precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_valid, y_valid_proba)
        
        threshold_analysis = {
            'all_thresholds': threshold_results,
            'optimal_by_metric': optimal_thresholds,
            'final_threshold': final_threshold,
            'final_metric': metric_priority,
            'final_value': final_value,
            'pr_curve': {
                'precision': precision_vals.tolist(),
                'recall': recall_vals.tolist(),
                'thresholds': pr_thresholds.tolist()
            }
        }
        
        return final_threshold, threshold_analysis
    
    def evaluate_model(self, model_key):
        """모델 평가 (최적 threshold 사용)"""
        # 최적 threshold 찾기
        optimal_threshold, threshold_analysis = self.find_optimal_threshold(model_key)
        
        model = self.models[model_key]
        data_type = self.results[model_key]['data_type']
        
        X_valid = self.data[data_type]['X_valid']
        y_valid = self.data[data_type]['y_valid']
        X_test = self.data[data_type]['X_test']
        y_test = self.data[data_type]['y_test']
        
        # 예측 (최적 threshold 사용)
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
        y_valid_pred = (y_valid_proba >= optimal_threshold).astype(int)
        
        y_test_proba = model.predict_proba(X_test)[:, 1]
        y_test_pred = (y_test_proba >= optimal_threshold).astype(int)
        
        # 검증 성능
        valid_metrics = {
            'auc': roc_auc_score(y_valid, y_valid_proba),
            'precision': precision_score(y_valid, y_valid_pred, zero_division=0),
            'recall': recall_score(y_valid, y_valid_pred, zero_division=0),
            'f1': f1_score(y_valid, y_valid_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_valid, y_valid_pred),
            'average_precision': average_precision_score(y_valid, y_valid_proba)
        }
        
        # 테스트 성능
        test_metrics = {
            'auc': roc_auc_score(y_test, y_test_proba),
            'precision': precision_score(y_test, y_test_pred, zero_division=0),
            'recall': recall_score(y_test, y_test_pred, zero_division=0),
            'f1': f1_score(y_test, y_test_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),
            'average_precision': average_precision_score(y_test, y_test_proba)
        }
        
        # 결과 업데이트
        self.results[model_key].update({
            'optimal_threshold': optimal_threshold,
            'threshold_analysis': threshold_analysis,
            'valid_metrics': valid_metrics,
            'test_metrics': test_metrics,
            'predictions': {
                'y_valid_proba': y_valid_proba.tolist(),
                'y_test_proba': y_test_proba.tolist()
            }
        })
        
        print(f"\n📊 {model_key} 최종 평가 (Threshold: {optimal_threshold:.3f}):")
        print(f"   검증 - AUC: {valid_metrics['auc']:.4f}, F1: {valid_metrics['f1']:.4f}, Precision: {valid_metrics['precision']:.4f}, Recall: {valid_metrics['recall']:.4f}")
        print(f"   테스트 - AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1']:.4f}, Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}")
    
    def run_all_models(self):
        """모든 모델 실행"""
        print("\n🚀 모든 모델 실행 시작")
        print("="*80)
        
        # 활성화된 데이터 타입 확인
        enabled_data_types = [dt for dt, config in self.config['data_types'].items() if config['enabled']]
        print(f"🎯 활성화된 데이터 타입: {enabled_data_types}")
        
        # Lasso 특성 선택 (한 번만 수행)
        if self.config['lasso']['enabled']:
            self.apply_lasso_feature_selection('normal')  # normal 데이터로 특성 선택
            
            # 선택된 특성을 다른 데이터 타입에도 적용
            selected_features = self.data['normal']['X_train'].columns.tolist()
            self.selected_features = selected_features
            
            # 다른 데이터 타입에도 동일한 특성 적용
            for data_type in enabled_data_types:
                if data_type != 'normal':
                    for split in ['X_train', 'X_valid', 'X_test']:
                        self.data[data_type][split] = self.data[data_type][split][selected_features]
        
        # 활성화된 데이터 타입별로 실행
        for data_type in enabled_data_types:
            print(f"\n📊 {data_type.upper()} 데이터 처리")
            print("="*60)
            
            # 모델별 최적화
            models_to_run = []
            if self.config['models']['logistic']['enabled']:
                models_to_run.append(('logistic', self.optimize_logistic_regression))
            if self.config['models']['random_forest']['enabled']:
                models_to_run.append(('random_forest', self.optimize_random_forest))
            if self.config['models']['xgboost']['enabled']:
                models_to_run.append(('xgboost', self.optimize_xgboost))
            
            for model_name, optimize_func in models_to_run:
                optimize_func(data_type)
        
        # 모든 모델 평가 (최적 threshold 자동 탐색)
        print(f"\n📊 모든 모델 평가 및 Threshold 최적화")
        print("="*60)
        
        for model_key in self.models.keys():
            self.evaluate_model(model_key)
        
        # 앙상블 모델 실행
        if self.config.get('ensemble', {}).get('enabled', False):
            self.run_ensemble_model()
    
    def run_ensemble_model(self):
        """앙상블 모델 실행"""
        print(f"\n🎭 앙상블 모델 실행")
        print("="*60)
        
        ensemble_config = self.config['ensemble']
        
        # 앙상블에 포함할 모델들 필터링
        ensemble_models = {}
        enabled_models = ensemble_config.get('models', [])
        enabled_data_types = ensemble_config.get('data_types', ['normal', 'smote'])
        
        # 모델 이름 매핑 (설정명 -> 실제 모델 키명)
        model_name_mapping = {
            'logistic': 'LogisticRegression',
            'random_forest': 'RandomForest', 
            'xgboost': 'XGBoost'
        }
        
        print(f"🔍 현재 훈련된 모델들: {list(self.models.keys())}")
        print(f"🎯 앙상블 설정 - 모델: {enabled_models}, 데이터 타입: {enabled_data_types}")
        
        for model_key, model_obj in self.models.items():
            # 모델 키에서 정보 추출 (예: LogisticRegression_normal)
            model_parts = model_key.split('_')
            if len(model_parts) >= 2:
                model_name = model_parts[0]  # LogisticRegression, RandomForest, XGBoost
                data_type = model_parts[1].lower()  # normal, smote
            else:
                continue  # 올바르지 않은 키 형식은 건너뛰기
            
            # 설정의 모델명을 실제 모델명으로 변환하여 비교
            enabled_model_names = [model_name_mapping.get(em, em) for em in enabled_models]
            
            # 설정에 따라 모델 선택
            if model_name in enabled_model_names and data_type in enabled_data_types:
                ensemble_models[model_key] = model_obj
                print(f"✅ 앙상블에 포함: {model_key} (모델: {model_name}, 데이터: {data_type})")
        
        if not ensemble_models:
            print("⚠️ 앙상블에 포함할 모델이 없습니다.")
            print(f"💡 디버깅 정보:")
            print(f"   - 설정된 모델: {enabled_models}")
            print(f"   - 매핑된 모델명: {[model_name_mapping.get(em, em) for em in enabled_models]}")
            print(f"   - 설정된 데이터 타입: {enabled_data_types}")
            print(f"   - 실제 모델 키들: {list(self.models.keys())}")
            return
        
        # 앙상블 모델 생성
        ensemble = EnsembleModel(self.config, ensemble_models)
        
        # 검증 및 테스트 데이터 (normal 데이터 사용)
        X_valid = self.data['normal']['X_valid']
        y_valid = self.data['normal']['y_valid']
        X_test = self.data['normal']['X_test']
        y_test = self.data['normal']['y_test']
        
        print(f"\n🎯 앙상블 예측 수행")
        print("="*40)
        
        # 검증 데이터로 앙상블 예측 (자동 가중치 계산 포함)
        ensemble_valid_proba = ensemble.ensemble_predict_proba(
            X_valid, X_valid, y_valid
        )
        
        # 테스트 데이터 예측
        ensemble_test_proba = ensemble.ensemble_predict_proba(X_test)
        
        # 최적 threshold 찾기
        if ensemble_config.get('threshold_optimization', {}).get('enabled', True):
            metric = ensemble_config.get('threshold_optimization', {}).get('metric_priority', 'f1')
            optimal_threshold, threshold_metrics = ensemble.find_optimal_threshold(
                X_valid, y_valid, metric=metric
            )
        else:
            optimal_threshold = 0.5
            threshold_metrics = {}
        
        # 최종 성능 평가
        ensemble_metrics = ensemble.evaluate_ensemble(X_test, y_test, optimal_threshold)
        
        # 앙상블 모델 저장
        ensemble_key = 'ensemble_model'
        self.models[ensemble_key] = ensemble
        
        # 결과 저장
        self.results[ensemble_key] = {
            'model_type': 'ensemble',
            'data_type': 'mixed',
            'method': ensemble_config.get('method', 'weighted_average'),
            'auto_weight': ensemble_config.get('auto_weight', False),
            'included_models': list(ensemble_models.keys()),
            'weights': ensemble.weights,
            'optimal_threshold': optimal_threshold,
            'threshold_metrics': threshold_metrics,
            'cv_score': np.mean([self.results[mk]['cv_score'] for mk in ensemble_models.keys()]),
            'valid_metrics': {
                'auc': roc_auc_score(y_valid, ensemble_valid_proba),
                'precision': precision_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'recall': recall_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'f1': f1_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'balanced_accuracy': balanced_accuracy_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int)),
                'average_precision': average_precision_score(y_valid, ensemble_valid_proba)
            },
            'test_metrics': ensemble_metrics,
            'predictions': {
                'y_valid_proba': ensemble_valid_proba.tolist(),
                'y_test_proba': ensemble_test_proba.tolist()
            }
        }
        
        print(f"\n🏆 앙상블 모델 최종 성능:")
        print(f"   방법: {ensemble_config.get('method', 'weighted_average')}")
        print(f"   포함 모델: {len(ensemble_models)}개")
        print(f"   최적 Threshold: {optimal_threshold:.3f}")
        print(f"   테스트 AUC: {ensemble_metrics['auc']:.4f}")
        print(f"   테스트 F1: {ensemble_metrics['f1']:.4f}")
        print(f"   테스트 Precision: {ensemble_metrics['precision']:.4f}")
        print(f"   테스트 Recall: {ensemble_metrics['recall']:.4f}")
        
        # 앙상블 시각화 생성
        viz_dir = self.output_dir / 'visualizations'
        ensemble.create_ensemble_report(viz_dir)
    
    def save_all_results(self):
        """모든 결과 저장"""
        print(f"\n💾 결과 저장")
        print("="*60)
        
        # 모델 저장
        models_dir = self.output_dir / 'models'
        models_dir.mkdir(exist_ok=True)
        
        for model_key, model in self.models.items():
            model_path = models_dir / f'{model_key.lower()}_model.joblib'
            joblib.dump(model, model_path)
            print(f"✅ {model_key} 모델 저장: {model_path}")
        
        # 결과 저장 (JSON 직렬화 가능하도록 변환)
        def convert_to_serializable(obj):
            """JSON 직렬화 가능한 형태로 변환"""
            if isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(v) for v in obj]
            elif isinstance(obj, (np.float32, np.float64)):
                return float(obj)
            elif isinstance(obj, (np.int32, np.int64)):
                return int(obj)
            else:
                return obj
        
        serializable_results = convert_to_serializable(self.results)
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(exist_ok=True)
        
        results_path = results_dir / 'all_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        print(f"✅ 전체 결과 저장: {results_path}")
        
        # 설정 저장
        config_path = self.output_dir / 'config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        print(f"✅ 설정 저장: {config_path}")
        
        # 요약 테이블 생성
        self.create_summary_table()
        
        # 시각화 생성
        self.create_visualizations()
    
    def create_summary_table(self):
        """요약 테이블 생성"""
        summary_data = []
        
        for model_key, result in self.results.items():
            model_name = model_key.split('_')[0]
            data_type = result['data_type']
            
            summary_data.append({
                'Model': model_name,
                'Data_Type': data_type.upper(),
                'Optimal_Threshold': result.get('optimal_threshold', 0.5),
                'CV_AUC': result['cv_score'],
                'Valid_AUC': result['valid_metrics']['auc'],
                'Valid_F1': result['valid_metrics']['f1'],
                'Test_AUC': result['test_metrics']['auc'],
                'Test_Precision': result['test_metrics']['precision'],
                'Test_Recall': result['test_metrics']['recall'],
                'Test_F1': result['test_metrics']['f1'],
                'Test_Balanced_Acc': result['test_metrics'].get('balanced_accuracy', 0),
                'Average_Precision': result['test_metrics'].get('average_precision', 0)
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # 저장
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(exist_ok=True)
        summary_path = results_dir / 'summary_table.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        print(f"\n🏆 실행 결과 요약:")
        print(summary_df.round(4))
        print(f"✅ 요약 테이블 저장: {summary_path}")
        
        return summary_df
    
    def create_visualizations(self):
        """모든 시각화 생성"""
        print(f"\n📈 시각화 생성")
        print("="*60)
        
        viz_dir = self.output_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        # 요약 테이블 생성 (시각화에서 사용)
        summary_df = self.create_summary_table()
        
        # 1. 성능 지표 비교 차트
        self.plot_performance_comparison(summary_df, viz_dir)
        
        # 2. ROC 곡선 비교
        self.plot_roc_curves(viz_dir)
        
        # 3. 특성 중요도 비교 (RF, XGBoost)
        self.plot_feature_importance_comparison(viz_dir)
        
        # 4. Normal vs SMOTE 비교
        self.plot_normal_vs_smote_comparison(summary_df, viz_dir)
        
        # 5. CV vs Test 성능 비교
        self.plot_cv_vs_test_comparison(summary_df, viz_dir)
        
        # 6. Threshold 최적화 결과 시각화
        self.plot_threshold_optimization(viz_dir)
        
        # 7. Precision-Recall 곡선
        self.plot_precision_recall_curves(viz_dir)
        
        print(f"✅ 모든 시각화 완료: {viz_dir}")
    
    def plot_performance_comparison(self, summary_df, viz_dir):
        """성능 지표 비교 차트"""
        print("📊 성능 지표 비교 차트 생성...")
        
        # 지표별 비교
        metrics = ['CV_AUC', 'Test_AUC', 'Test_Precision', 'Test_Recall', 'Test_F1']
        metric_names = ['CV AUC', 'Test AUC', 'Test Precision', 'Test Recall', 'Test F1']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('모델 성능 지표 비교 (앙상블 포함)', fontsize=16, fontweight='bold')
        
        # 색상 설정
        colors = {'NORMAL': 'skyblue', 'SMOTE': 'lightcoral', 'MIXED': 'gold', 'ENSEMBLE': 'purple'}
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            row, col = i // 3, i % 3
            ax = axes[row, col]
            
            # 앙상블과 일반 모델 분리
            ensemble_data = summary_df[summary_df['Model'] == 'ensemble']
            regular_data = summary_df[summary_df['Model'] != 'ensemble']
            
            if not regular_data.empty:
                # 일반 모델들 - 기존 방식
                pivot_data = regular_data.pivot(index='Model', columns='Data_Type', values=metric)
                
                # 바 차트
                x = np.arange(len(pivot_data.index))
                width = 0.35
                
                if 'NORMAL' in pivot_data.columns:
                    bars1 = ax.bar(x - width/2, pivot_data['NORMAL'], width, 
                                  label='Normal', alpha=0.8, color=colors['NORMAL'])
                if 'SMOTE' in pivot_data.columns:
                    bars2 = ax.bar(x + width/2, pivot_data['SMOTE'], width, 
                                  label='SMOTE', alpha=0.8, color=colors['SMOTE'])
                
                # 값 표시
                container_idx = 0
                for col_name in pivot_data.columns:
                    if col_name in ['NORMAL', 'SMOTE']:
                        if container_idx < len(ax.containers):
                            bars = ax.containers[container_idx]
                            for bar in bars:
                                height = bar.get_height()
                                if not np.isnan(height):
                                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)
                            container_idx += 1
                
                ax.set_xticks(x)
                ax.set_xticklabels(pivot_data.index, rotation=45)
                
                # 앙상블 추가
                if not ensemble_data.empty:
                    ensemble_x = len(pivot_data.index)
                    ensemble_value = ensemble_data[metric].iloc[0]
                    bars3 = ax.bar(ensemble_x, ensemble_value, width*2, 
                                  label='Ensemble', alpha=0.9, color=colors['ENSEMBLE'])
                    
                    # 앙상블 값 표시
                    ax.text(ensemble_x, ensemble_value + 0.01,
                           f'{ensemble_value:.3f}', ha='center', va='bottom', fontsize=9)
                    
                    # x축 라벨 업데이트
                    all_labels = list(pivot_data.index) + ['Ensemble']
                    ax.set_xticks(list(range(len(all_labels))))
                    ax.set_xticklabels(all_labels, rotation=45)
            
            elif not ensemble_data.empty:
                # 앙상블만 있는 경우
                ensemble_value = ensemble_data[metric].iloc[0]
                ax.bar(0, ensemble_value, width=0.5, 
                      label='Ensemble', alpha=0.9, color=colors['ENSEMBLE'])
                ax.text(0, ensemble_value + 0.01,
                       f'{ensemble_value:.3f}', ha='center', va='bottom', fontsize=9)
                ax.set_xticks([0])
                ax.set_xticklabels(['Ensemble'])
            
            ax.set_title(f'{name}', fontsize=12, fontweight='bold')
            ax.set_ylabel(name)
            ax.set_xlabel('모델')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1.1 if metric != 'Test_F1' else max(summary_df[metric].max() * 1.2, 0.5))
        
        # 마지막 subplot 제거
        axes[1, 2].remove()
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ 성능 비교 차트 저장: performance_comparison.png")
    
    def plot_roc_curves(self, viz_dir):
        """ROC 곡선 비교"""
        print("📈 ROC 곡선 생성...")
        
        from sklearn.metrics import roc_curve, auc
        
        # 실제 사용 가능한 데이터 타입만 가져오기
        available_data_types = list(self.data.keys())
        if not available_data_types:
            print("  ⚠️ ROC 곡선 생성을 위한 데이터 없음")
            return
            
        # 최대 2개까지만 표시 (subplot 구조상)
        data_types = available_data_types[:2]
        titles = [f'{dt.upper()} 데이터' for dt in data_types]
        colors = ['blue', 'red', 'green']
        
        fig, axes = plt.subplots(1, len(data_types), figsize=(8*len(data_types), 6))
        if len(data_types) == 1:
            axes = [axes]  # 단일 subplot인 경우 리스트로 변환
        fig.suptitle('ROC 곡선 비교', fontsize=16, fontweight='bold')
        
        for idx, (data_type, title) in enumerate(zip(data_types, titles)):
            ax = axes[idx]
            
            X_test = self.data[data_type]['X_test']
            y_test = self.data[data_type]['y_test']
            
            model_names = ['LogisticRegression', 'RandomForest', 'XGBoost']
            
            for i, model_name in enumerate(model_names):
                model_key = f'{model_name}_{data_type}'
                if model_key in self.models:
                    model = self.models[model_key]
                    y_proba = model.predict_proba(X_test)[:, 1]
                    
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    roc_auc = auc(fpr, tpr)
                    
                    ax.plot(fpr, tpr, color=colors[i], lw=2, 
                           label=f'{model_name} (AUC = {roc_auc:.4f})')
            
            # 대각선 (랜덤 분류기)
            ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', 
                   label='Random (AUC = 0.5000)')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(title)
            ax.legend(loc="lower right")
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'roc_curves_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ ROC 곡선 저장: roc_curves_comparison.png")
    
    def plot_feature_importance_comparison(self, viz_dir):
        """특성 중요도 비교 (RF, XGBoost만)"""
        print("🔍 특성 중요도 비교 생성...")
        
        tree_models = ['RandomForest', 'XGBoost']
        # 실제 사용 가능한 데이터 타입만 가져오기
        available_data_types = list(self.data.keys())[:2]  # 최대 2개
        
        if not available_data_types:
            print("  ⚠️ 특성 중요도 비교를 위한 데이터 없음")
            return
        
        fig, axes = plt.subplots(2, len(available_data_types), figsize=(8*len(available_data_types), 12))
        if len(available_data_types) == 1:
            axes = axes.reshape(-1, 1)  # 2D 배열로 유지
        fig.suptitle('특성 중요도 비교 (Tree-based 모델)', fontsize=16, fontweight='bold')
        
        for i, model_name in enumerate(tree_models):
            for j, data_type in enumerate(available_data_types):
                ax = axes[i, j]
                model_key = f'{model_name}_{data_type}'
                
                if model_key in self.results and 'feature_importances' in self.results[model_key]:
                    importances = self.results[model_key]['feature_importances']
                    
                    # 상위 10개 특성
                    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]
                    features, values = zip(*sorted_features)
                    
                    # 색상 설정
                    color = 'green' if model_name == 'RandomForest' else 'purple'
                    
                    bars = ax.barh(features, values, color=color, alpha=0.7)
                    
                    # 값 표시
                    for k, v in enumerate(values):
                        ax.text(v + 0.001, k, f'{v:.3f}', va='center', fontsize=9)
                    
                    ax.set_title(f'{model_name} - {data_type.upper()}\n특성 중요도 (Top 10)', 
                               fontsize=12, fontweight='bold')
                    ax.set_xlabel('중요도')
                    ax.grid(True, alpha=0.3)
                else:
                    ax.text(0.5, 0.5, 'No data available', ha='center', va='center', 
                           transform=ax.transAxes, fontsize=12)
                    ax.set_title(f'{model_name} - {data_type.upper()}')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'feature_importance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ 특성 중요도 비교 저장: feature_importance_comparison.png")
    
    def plot_normal_vs_smote_comparison(self, summary_df, viz_dir):
        """Normal vs SMOTE vs Undersampling 상세 비교"""
        print("⚖️ 샘플링 전략별 비교 생성...")
        
        # ensemble 모델 제외 (MIXED 데이터 타입)
        allowed_types = ['NORMAL', 'SMOTE', 'UNDERSAMPLING', 'COMBINED']
        df_filtered = summary_df[summary_df['Data_Type'].isin(allowed_types)]
        
        if len(df_filtered) == 0:
            print("  ⚠️ 샘플링 전략별 비교할 데이터 없음")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(18, 12))
        fig.suptitle('샘플링 전략별 데이터 성능 비교', fontsize=16, fontweight='bold')
        
        models = df_filtered['Model'].unique()
        metrics = ['Test_AUC', 'Test_F1', 'Test_Precision', 'Test_Recall']
        metric_names = ['Test AUC', 'Test F1', 'Test Precision', 'Test Recall']
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i//2, i%2]
            
            data_type_values = {dt: [] for dt in allowed_types}
            model_labels = []
            
            for model in models:
                model_has_data = False
                model_values = {}
                
                # 각 데이터 타입별 결과 확인
                for data_type in allowed_types:
                    mask = (df_filtered['Model'] == model) & (df_filtered['Data_Type'] == data_type)
                    result = df_filtered[mask][metric]
                    if len(result) > 0:
                        model_values[data_type] = result.iloc[0]
                        model_has_data = True
                    else:
                        model_values[data_type] = 0
                
                # 적어도 하나의 데이터 타입이 있는 경우 추가
                if model_has_data:
                    for data_type in allowed_types:
                        data_type_values[data_type].append(model_values[data_type])
                    model_labels.append(model)
            
            # 실제로 데이터가 있는 모델만 사용
            if len(model_labels) == 0:
                continue  # 이 메트릭에 대해 유효한 데이터가 없음
                
            x = np.arange(len(model_labels))
            width = 0.8 / len(allowed_types)  # 여러 데이터 타입에 맞게 조정
            
            colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']
            bars_list = []
            
            for idx, data_type in enumerate(allowed_types):
                values = data_type_values[data_type]
                if any(v > 0 for v in values):  # 실제 데이터가 있는 경우만
                    bars = ax.bar(x + (idx - len(allowed_types)/2 + 0.5) * width, 
                                 values, width, label=data_type, 
                                 alpha=0.8, color=colors[idx % len(colors)])
                    bars_list.append(bars)
                    
                    # 값 표시
                    for bar in bars:
                        height = bar.get_height()
                        if height > 0:
                            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)
            
            ax.set_title(f'{name} 비교', fontsize=12, fontweight='bold')
            ax.set_ylabel(name)
            ax.set_xlabel('모델')
            ax.set_xticks(x)
            ax.set_xticklabels(model_labels, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'sampling_strategy_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ 샘플링 전략별 비교 저장: sampling_strategy_comparison.png")
    
    def plot_cv_vs_test_comparison(self, summary_df, viz_dir):
        """CV vs Test 성능 비교 (과적합 확인)"""
        print("📊 CV vs Test 성능 비교 생성...")
        
        # 실제 사용 가능한 데이터 타입 확인
        available_data_types = summary_df['Data_Type'].unique()
        available_data_types = [dt for dt in available_data_types if dt != 'MIXED'][:2]  # MIXED 제외, 최대 2개
        
        if not available_data_types:
            print("  ⚠️ CV vs Test 비교를 위한 데이터 없음")
            return
        
        fig, axes = plt.subplots(1, len(available_data_types), figsize=(8*len(available_data_types), 6))
        if len(available_data_types) == 1:
            axes = [axes]  # 단일 subplot인 경우 리스트로 변환
        fig.suptitle('CV vs Test AUC 비교 (과적합 확인)', fontsize=16, fontweight='bold')
        
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']
        
        for idx, data_type in enumerate(available_data_types):
            ax = axes[idx]
            
            subset = summary_df[summary_df['Data_Type'] == data_type]
            models = subset['Model'].tolist()
            cv_scores = subset['CV_AUC'].tolist()
            test_scores = subset['Test_AUC'].tolist()
            
            x = np.arange(len(models))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, cv_scores, width, label='CV AUC', 
                          alpha=0.8, color='green')
            bars2 = ax.bar(x + width/2, test_scores, width, label='Test AUC', 
                          alpha=0.8, color=colors[idx])
            
            # 과적합 정도 표시
            for i, (cv, test) in enumerate(zip(cv_scores, test_scores)):
                overfitting = cv - test
                color = 'red' if overfitting > 0.05 else 'orange' if overfitting > 0.02 else 'green'
                ax.text(i, max(cv, test) + 0.01, f'{overfitting:.3f}', 
                       ha='center', va='bottom', fontsize=10, 
                       fontweight='bold', color=color)
            
            # 값 표시
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)
            
            ax.set_title(f'{data_type} 데이터', fontsize=12, fontweight='bold')
            ax.set_ylabel('AUC Score')
            ax.set_xlabel('모델')
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0.8, 1.0)
            
            # 과적합 기준선
            ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.5, 
                      label='High Performance')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'cv_vs_test_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ CV vs Test 비교 저장: cv_vs_test_comparison.png")
    
    def plot_threshold_optimization(self, viz_dir):
        """Threshold 최적화 결과 시각화"""
        print("🎯 Threshold 최적화 결과 시각화...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('모델별 최적 Threshold 분석', fontsize=16, fontweight='bold')
        
        model_names = []
        thresholds = []
        f1_scores = []
        precision_scores = []
        recall_scores = []
        
        # 각 모델의 threshold 최적화 결과 수집
        for model_key, result in self.results.items():
            if 'threshold_analysis' in result:
                analysis = result['threshold_analysis']
                model_name = model_key.split('_')[0]
                data_type = result['data_type']
                label = f"{model_name}\n({data_type.upper()})"
                
                model_names.append(label)
                thresholds.append(analysis.get('final_threshold', 0.5))
                
                # 최적 threshold에서의 성능
                valid_metrics = result.get('valid_metrics', {})
                f1_scores.append(valid_metrics.get('f1', 0))
                precision_scores.append(valid_metrics.get('precision', 0))
                recall_scores.append(valid_metrics.get('recall', 0))
        
        if not model_names:
            print("  ⚠️ Threshold 최적화 데이터 없음")
            return
        
        # 1. 모델별 최적 Threshold
        ax1 = axes[0, 0]
        bars = ax1.bar(model_names, thresholds, color=['blue', 'red', 'green', 'orange', 'purple', 'brown'][:len(model_names)])
        ax1.set_title('모델별 최적 Threshold', fontweight='bold')
        ax1.set_ylabel('Threshold')
        ax1.set_ylim(0, 1)
        
        # 값 표시
        for bar, thresh in zip(bars, thresholds):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{thresh:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # 2. Threshold vs F1 Score
        ax2 = axes[0, 1]
        scatter = ax2.scatter(thresholds, f1_scores, c=range(len(model_names)), 
                             s=100, alpha=0.7, cmap='viridis')
        ax2.set_title('Threshold vs F1 Score', fontweight='bold')
        ax2.set_xlabel('Threshold')
        ax2.set_ylabel('F1 Score')
        ax2.grid(True, alpha=0.3)
        
        # 모델명 표시
        for i, (thresh, f1, name) in enumerate(zip(thresholds, f1_scores, model_names)):
            ax2.annotate(name.split('\n')[0], (thresh, f1), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # 3. Precision vs Recall (최적 threshold에서)
        ax3 = axes[1, 0]
        scatter = ax3.scatter(recall_scores, precision_scores, c=range(len(model_names)),
                             s=100, alpha=0.7, cmap='viridis')
        ax3.set_title('Precision vs Recall (최적 Threshold)', fontweight='bold')
        ax3.set_xlabel('Recall')
        ax3.set_ylabel('Precision')
        ax3.grid(True, alpha=0.3)
        
        # 모델명 표시
        for i, (recall, precision, name) in enumerate(zip(recall_scores, precision_scores, model_names)):
            ax3.annotate(name.split('\n')[0], (recall, precision),
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # 4. 종합 성능 비교 (Radar Chart)
        ax4 = axes[1, 1]
        
        # 간단한 바 차트로 대체 (radar chart는 복잡함)
        x = np.arange(len(model_names))
        width = 0.25
        
        bars1 = ax4.bar(x - width, f1_scores, width, label='F1', alpha=0.8)
        bars2 = ax4.bar(x, precision_scores, width, label='Precision', alpha=0.8)
        bars3 = ax4.bar(x + width, recall_scores, width, label='Recall', alpha=0.8)
        
        ax4.set_title('최적 Threshold에서의 성능', fontweight='bold')
        ax4.set_ylabel('Score')
        ax4.set_xticks(x)
        ax4.set_xticklabels([name.split('\n')[0] for name in model_names], rotation=45)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'threshold_optimization_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ Threshold 최적화 분석 저장: threshold_optimization_analysis.png")
    
    def plot_precision_recall_curves(self, viz_dir):
        """Precision-Recall 곡선 시각화"""
        print("📈 Precision-Recall 곡선 생성...")
        
        # 실제 사용 가능한 데이터 타입만 가져오기
        available_data_types = list(self.data.keys())
        if not available_data_types:
            print("  ⚠️ Precision-Recall 곡선 생성을 위한 데이터 없음")
            return
            
        # 최대 2개까지만 표시 (subplot 구조상)
        data_types = available_data_types[:2]
        titles = [f'{dt.upper()} 데이터' for dt in data_types]
        colors = ['blue', 'red', 'green']
        
        fig, axes = plt.subplots(1, len(data_types), figsize=(8*len(data_types), 6))
        if len(data_types) == 1:
            axes = [axes]  # 단일 subplot인 경우 리스트로 변환
        fig.suptitle('Precision-Recall 곡선 비교', fontsize=16, fontweight='bold')
        
        for idx, (data_type, title) in enumerate(zip(data_types, titles)):
            ax = axes[idx]
            
            model_names = ['LogisticRegression', 'RandomForest', 'XGBoost']
            
            for i, model_name in enumerate(model_names):
                model_key = f'{model_name}_{data_type}'
                if model_key in self.results and 'threshold_analysis' in self.results[model_key]:
                    analysis = self.results[model_key]['threshold_analysis']
                    
                    if 'pr_curve' in analysis:
                        pr_data = analysis['pr_curve']
                        precision_vals = pr_data['precision']
                        recall_vals = pr_data['recall']
                        
                        # Average Precision 계산
                        valid_metrics = self.results[model_key].get('valid_metrics', {})
                        avg_precision = valid_metrics.get('average_precision', 0)
                        
                        ax.plot(recall_vals, precision_vals, color=colors[i], lw=2,
                               label=f'{model_name} (AP = {avg_precision:.3f})')
                        
                        # 최적 threshold 포인트 표시
                        optimal_threshold = analysis.get('final_threshold', 0.5)
                        
                        # 최적 threshold에서의 precision, recall 찾기
                        opt_precision = valid_metrics.get('precision', 0)
                        opt_recall = valid_metrics.get('recall', 0)
                        
                        ax.scatter([opt_recall], [opt_precision], color=colors[i], 
                                 s=100, marker='*', edgecolors='black', linewidth=1,
                                 label=f'{model_name} 최적점 (T={optimal_threshold:.3f})')
            
            # 기준선 (Random Classifier)
            y_true_ratio = self.data[data_type]['y_valid'].mean()
            ax.axhline(y=y_true_ratio, color='gray', linestyle='--', alpha=0.7,
                      label=f'Random (AP = {y_true_ratio:.3f})')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('Recall')
            ax.set_ylabel('Precision')
            ax.set_title(title)
            ax.legend(loc="lower left", fontsize=9)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'precision_recall_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ✅ Precision-Recall 곡선 저장: precision_recall_curves.png")

    def proper_cv_with_sampling(self, model, X, y, data_type, cv_folds=5):
        """
        샘플링 Data Leakage를 방지하는 올바른 Cross Validation
        각 CV fold마다 샘플링을 별도로 적용
        """
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.config['random_state'])
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            # 각 fold마다 별도로 분할
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # 훈련 fold에만 샘플링 적용 (Data Leakage 방지)
            try:
                X_fold_train_resampled, y_fold_train_resampled = self.apply_sampling_strategy(
                    X_fold_train, y_fold_train, data_type
                )
                
                # 모델 복사 및 훈련
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train_resampled, y_fold_train_resampled)
                
                # 검증 fold에서 평가 (원본 데이터만 사용)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
                
            except Exception as e:
                print(f"⚠️ Fold {fold+1} {data_type.upper()} 적용 실패: {e}")
                # 샘플링 실패 시 원본 데이터로 훈련
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train, y_fold_train)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
        
        return np.array(scores)

def load_config(config_path='master_config.json'):
    """설정 파일 로드"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def main():
    """메인 실행 함수"""
    print("🏢 한국 기업 부실예측 - 마스터 모델 러너")
    print("="*80)
    
    # 설정 로드
    config = load_config()
    
    # 러너 생성 및 실행
    runner = MasterModelRunner(config)
    runner.load_data()
    runner.run_all_models()
    runner.save_all_results()
    
    print(f"\n🎉 모든 모델 실행 완료!")
    print(f"📁 결과 저장 위치: {runner.output_dir}")
    print("="*80)

if __name__ == "__main__":
    main() 