# ğŸ¤– Modeling Module

**í•œêµ­ ê¸°ì—… ë¶€ì‹¤ì˜ˆì¸¡ì„ ìœ„í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ ì‹œìŠ¤í…œ**

## ğŸ¯ **ê°œìš”**

ì´ ëª¨ë“ˆì€ **ë‘ ê°€ì§€ ë°ì´í„° íŠ¸ë™**ì„ í™œìš©í•œ í•œêµ­ ê¸°ì—… ë¶€ì‹¤ì˜ˆì¸¡ ëª¨ë¸ë§ì„ ìœ„í•œ í†µí•© ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ìë™ ì„ê³„ê°’ ìµœì í™”, ì•™ìƒë¸” ëª¨ë¸ë§, Data Leakage ë°©ì§€ ë“± ê³ ê¸‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“Š **ì§€ì› ë°ì´í„° íŠ¸ë™**

### ğŸ”¥ **í™•ì¥ íŠ¸ë™** (FS_ratio_flow_labeled.csv)
- **ê´€ì¸¡ì¹˜**: 22,780ê°œ Ã— 36ê°œ ë³€ìˆ˜
- **ë¶€ì‹¤ê¸°ì—…**: 132ê°œ (0.58%)
- **íŠ¹ì§•**: YoY ì„±ì¥ë¥ , ë³€í™”ëŸ‰ ì§€í‘œ, ë°œìƒì•¡ ë“± ê³ ê¸‰ ë³€ìˆ˜
- **ìš©ë„**: ê³ ê¸‰ íŠ¹ì„±ê³µí•™, ë³µí•© ëª¨ë¸ë§

### âœ… **ì™„ì „ íŠ¸ë™** (FS_100_complete.csv)  
- **ê´€ì¸¡ì¹˜**: 16,197ê°œ Ã— 22ê°œ ë³€ìˆ˜
- **ë¶€ì‹¤ê¸°ì—…**: 104ê°œ (0.64%)
- **íŠ¹ì§•**: ê²°ì¸¡ì¹˜ 0%, ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì™„ë£Œ
- **ìš©ë„**: ì•ˆì •ì  ìš´ì˜, ê¸°ë³¸ ëª¨ë¸ë§

## ğŸ—ï¸ **ì‹œìŠ¤í…œ êµ¬ì¡°**

```
ğŸ“¦ src/modeling/
â”œâ”€â”€ ğŸš€ master_model_runner.py          # í†µí•© ëª¨ë¸ë§ ì—”ì§„
â”œâ”€â”€ ğŸ® run_master.py                   # ë§ˆìŠ¤í„° ëŸ¬ë„ˆ ì‹¤í–‰ê¸°
â”œâ”€â”€ âš™ï¸ master_config.json             # ì¤‘ì•™ ì„¤ì • íŒŒì¼
â”œâ”€â”€ ğŸ­ ensemble_model.py               # ì•™ìƒë¸” ëª¨ë¸ êµ¬í˜„
â”œâ”€â”€ ğŸ“ config_templates/               # ì„¤ì • í…œí”Œë¦¿ ëª¨ìŒ
â”‚   â”œâ”€â”€ ğŸ­ production_config.json      # ìš´ì˜í™˜ê²½ ì„¤ì •
â”‚   â”œâ”€â”€ âš¡ quick_test_config.json      # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì„¤ì •
â”‚   â”œâ”€â”€ ğŸ¯ lasso_focus_config.json     # Lasso íŠ¹ì„±ì„ íƒ ì„¤ì •
â”‚   â””â”€â”€ ğŸ”§ custom_config.json          # ì‚¬ìš©ì ì •ì˜ ì„¤ì •
â”œâ”€â”€ ğŸ“Š ê°œë³„ ëª¨ë¸ íŒŒì¼ë“¤:
â”‚   â”œâ”€â”€ ğŸ“ˆ logistic_regression_100.py  # ë¡œì§€ìŠ¤í‹± íšŒê·€
â”‚   â”œâ”€â”€ ğŸŒ³ random_forest_100.py        # ëœë¤ í¬ë ˆìŠ¤íŠ¸  
â”‚   â”œâ”€â”€ ğŸš€ xgboost_100.py              # XGBoost
â”‚   â”œâ”€â”€ ğŸ“Š model_comparison.py         # ëª¨ë¸ ë¹„êµ ë¶„ì„
â”‚   â””â”€â”€ ğŸ” threshold_optimization.py   # ì„ê³„ê°’ ìµœì í™”
â””â”€â”€ ğŸ“„ README.md                       # í˜„ì¬ íŒŒì¼
```

## ğŸš€ **í•µì‹¬ ê¸°ëŠ¥**

### 1. ğŸ® **ë§ˆìŠ¤í„° ëŸ¬ë„ˆ ì‹œìŠ¤í…œ**

í†µí•© ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ì„ ìë™í™”í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.

```python
# ê¸°ë³¸ ì‹¤í–‰ (ê¶Œì¥)
python run_master.py

# í…œí”Œë¦¿ ê¸°ë°˜ ì‹¤í–‰
python run_master.py --template production     # ìš´ì˜í™˜ê²½ìš©
python run_master.py --template quick_test     # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python run_master.py --template lasso_focus    # Lasso íŠ¹ì„±ì„ íƒ

# ì‚¬ìš©ì ì •ì˜ ì„¤ì •
python run_master.py --config custom_config.json
```

**ì£¼ìš” ê¸°ëŠ¥:**
- ğŸ¯ **ìë™ ì„ê³„ê°’ ìµœì í™”**: ê° ëª¨ë¸ë³„ F1-Score ê¸°ì¤€ ìµœì  threshold íƒìƒ‰
- ğŸ­ **ì•™ìƒë¸” ëª¨ë¸ë§**: 9ê°œ ëª¨ë¸ ì¡°í•©ìœ¼ë¡œ 21.3% ì„±ëŠ¥ í–¥ìƒ
- ğŸ›¡ï¸ **Data Leakage ë°©ì§€**: CV ë‚´ë¶€ ë™ì  SMOTE ì ìš©
- ğŸ“Š **í¬ê´„ì  í‰ê°€**: ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œë¡œ ëª¨ë¸ ë¹„êµ

### 2. ğŸ­ **ì•™ìƒë¸” ëª¨ë¸ë§**

**ìµœê³  ì„±ëŠ¥ ë‹¬ì„±**: F1-Score 0.4096, AUC 0.9808

```python
from ensemble_model import EnsembleModel

# ì•™ìƒë¸” ëª¨ë¸ ì´ˆê¸°í™”
ensemble = EnsembleModel(
    models=['logistic', 'randomforest', 'xgboost'],
    data_types=['normal', 'smote', 'combined'],
    weighting_strategy='equal'  # ê· ë“± ê°€ì¤‘ì¹˜
)

# í•™ìŠµ ë° ì˜ˆì¸¡
ensemble.fit(X_train, y_train)
predictions = ensemble.predict(X_test)
```

**ì•™ìƒë¸” êµ¬ì„±:**
- **ëª¨ë¸**: LogisticRegression, RandomForest, XGBoost
- **ë°ì´í„°**: Normal, SMOTE, Combined (ê° 3ê°œ)
- **ì´ 9ê°œ ëª¨ë¸** ê· ë“± ê°€ì¤‘ì¹˜ (ê° 11.11%)

### 3. âš¡ **ìë™ ì„ê³„ê°’ ìµœì í™”**

ê° ëª¨ë¸ë³„ë¡œ ìµœì ì˜ ì„ê³„ê°’ì„ ìë™ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.

```python
from threshold_optimization import ThresholdOptimizer

optimizer = ThresholdOptimizer(
    metric='f1',           # ìµœì í™” ê¸°ì¤€: f1, precision, recall
    search_range=(0.05, 0.95),  # íƒìƒ‰ ë²”ìœ„
    step_size=0.05         # íƒìƒ‰ ê°„ê²©
)

optimal_threshold = optimizer.optimize(y_true, y_pred_proba)
```

**ì„±ê³¼:**
- **í‰ê·  15% F1-Score í–¥ìƒ** (ê¸°ë³¸ 0.5 ëŒ€ë¹„)
- **ëª¨ë¸ë³„ ìµœì  ì„ê³„ê°’**: 0.05~0.85 ë²”ìœ„ì—ì„œ ìë™ íƒìƒ‰
- **êµì°¨ê²€ì¦ ê¸°ë°˜**: ì•ˆì •ì ì´ê³  ì¼ë°˜í™”ëœ ì„±ëŠ¥

### 4. ğŸ›¡ï¸ **Data Leakage ë°©ì§€**

```python
# ë™ì  SMOTE ì ìš© (CV ë‚´ë¶€ì—ì„œë§Œ)
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import cross_val_score

def cv_with_smote(model, X, y, cv=5):
    scores = []
    for train_idx, val_idx in cv.split(X, y):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # SMOTEëŠ” í›ˆë ¨ ë°ì´í„°ì—ë§Œ ì ìš©
        smote = SMOTE(random_state=42)
        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
        
        model.fit(X_train_smote, y_train_smote)
        score = model.score(X_val, y_val)
        scores.append(score)
    
    return scores
```

## ğŸ“Š **ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ**

### ğŸ† **ìµœê³  ì„±ëŠ¥ ê²°ê³¼**

| ëª¨ë¸ | ë°ì´í„° íŠ¸ë™ | ìµœì  Threshold | AUC | F1-Score | Precision | Recall |
|------|------------|----------------|-----|----------|-----------|--------|
| **ğŸ­ Ensemble** | **Mixed** | **0.10** | **0.9808** | **0.4096** | **0.2982** | **0.6538** |
| **ğŸš€ XGBoost** | Normal | 0.10 | 0.9800 | 0.3380 | 0.2857 | 0.4103 |
| **ğŸš€ XGBoost** | SMOTE | 0.15 | 0.9733 | 0.3121 | 0.2414 | 0.4359 |
| **ğŸŒ³ RandomForest** | Normal | 0.15 | 0.9793 | 0.2381 | 0.2632 | 0.2179 |
| **ğŸŒ³ RandomForest** | SMOTE | 0.20 | 0.9734 | 0.2222 | 0.2000 | 0.2500 |
| **ğŸ“ˆ LogisticRegression** | Normal | 0.15 | 0.9508 | 0.2182 | 0.1875 | 0.2564 |
| **ğŸ“ˆ LogisticRegression** | SMOTE | 0.20 | 0.9523 | 0.2105 | 0.1739 | 0.2564 |

### ğŸ“ˆ **ì„±ëŠ¥ í–¥ìƒ ë¶„ì„**
- **ì•™ìƒë¸” vs ìµœê³  ê°œë³„**: +21.3% F1-Score í–¥ìƒ
- **ì„ê³„ê°’ ìµœì í™”**: í‰ê·  +15% F1-Score í–¥ìƒ
- **SMOTE íš¨ê³¼**: Recall í–¥ìƒ, Precision ì¼ë¶€ í•˜ë½
- **AUC ì„±ëŠ¥**: ëª¨ë“  ëª¨ë¸ì—ì„œ 0.95+ ë‹¬ì„±

## âš™ï¸ **ì„¤ì • ê´€ë¦¬**

### ğŸ“„ **master_config.json êµ¬ì¡°**

```json
{
    "data_config": {
        "base_path": "../../data/final/",
        "train_file": "X_train_100_normal.csv",
        "target_file": "y_train_100_normal.csv",
        "test_file": "X_test_100_normal.csv",
        "scaler_type": "standard"
    },
    "model_config": {
        "models_to_run": ["logistic", "randomforest", "xgboost"],
        "use_smote": true,
        "smote_strategy": "minority",
        "cross_validation_folds": 5
    },
    "threshold_optimization": {
        "enabled": true,
        "metric": "f1",
        "search_range": [0.05, 0.95],
        "step_size": 0.05
    },
    "ensemble_config": {
        "enabled": true,
        "weighting_strategy": "equal",
        "models_to_ensemble": "all"
    },
    "output_config": {
        "save_models": true,
        "save_results": true,
        "create_visualizations": true,
        "output_dir": "../../outputs/master_runs/"
    }
}
```

### ğŸ¯ **í…œí”Œë¦¿ ì„¤ì •**

#### ğŸ­ **production_config.json**
```json
{
    "model_config": {
        "cross_validation_folds": 10,
        "hyperparameter_tuning": true,
        "n_iter_search": 100
    },
    "threshold_optimization": {
        "step_size": 0.01,
        "search_range": [0.01, 0.99]
    }
}
```

#### âš¡ **quick_test_config.json**
```json
{
    "model_config": {
        "models_to_run": ["logistic", "randomforest"],
        "cross_validation_folds": 3,
        "hyperparameter_tuning": false
    },
    "threshold_optimization": {
        "step_size": 0.1,
        "search_range": [0.1, 0.9]
    }
}
```

## ğŸ” **ê°œë³„ ëª¨ë¸ ìƒì„¸**

### ğŸ“ˆ **Logistic Regression**
```python
# logistic_regression_100.py
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet'],
    'solver': ['liblinear', 'saga']
}

model = LogisticRegression(random_state=42, max_iter=1000)
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')
```

**íŠ¹ì§•:**
- âœ… í•´ì„ ê°€ëŠ¥ì„± ë†’ìŒ
- âœ… ë¹ ë¥¸ í•™ìŠµ ì†ë„
- âœ… ì„ í˜• ê´€ê³„ ëª¨ë¸ë§ì— íš¨ê³¼ì 
- âŒ ë¹„ì„ í˜• íŒ¨í„´ í¬ì°© í•œê³„

### ğŸŒ³ **Random Forest**
```python
# random_forest_100.py
from sklearn.ensemble import RandomForestClassifier

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

model = RandomForestClassifier(random_state=42, n_jobs=-1)
```

**íŠ¹ì§•:**
- âœ… íŠ¹ì„± ì¤‘ìš”ë„ ì œê³µ
- âœ… ê³¼ì í•© ë°©ì§€
- âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ìš°ìˆ˜
- âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë§ìŒ

### ğŸš€ **XGBoost**
```python
# xgboost_100.py
import xgboost as xgb

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')
```

**íŠ¹ì§•:**
- âœ… ìµœê³  ì„±ëŠ¥ (F1: 0.3380)
- âœ… ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…
- âœ… ì¡°ê¸° ì¢…ë£Œ ì§€ì›
- âŒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë³µì¡

## ğŸ“Š **ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”**

### ğŸ“ˆ **ìë™ ìƒì„± ì°¨íŠ¸**

```python
# ì‹¤í–‰ í›„ ìë™ ìƒì„±ë˜ëŠ” ì‹œê°í™”
outputs/master_runs/default_run_YYYYMMDD_HHMMSS/visualizations/
â”œâ”€â”€ ğŸ“Š ensemble_analysis.png           # ì•™ìƒë¸” ì„±ëŠ¥ ë¶„ì„
â”œâ”€â”€ âš–ï¸ ensemble_weights.png            # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë¶„í¬  
â”œâ”€â”€ ğŸ¯ threshold_optimization_analysis.png # ì„ê³„ê°’ ìµœì í™” ë¶„ì„
â”œâ”€â”€ ğŸ“ˆ cv_vs_test_comparison.png       # CV vs Test ì„±ëŠ¥ ë¹„êµ
â”œâ”€â”€ ğŸ” feature_importance_comparison.png # íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ
â”œâ”€â”€ ğŸ“Š normal_vs_smote_detailed.png    # Normal vs SMOTE ë¹„êµ
â””â”€â”€ ğŸ“‹ performance_comparison.png      # ì „ì²´ ì„±ëŠ¥ ë¹„êµ
```

### ğŸ“„ **ê²°ê³¼ íŒŒì¼**

```python
outputs/master_runs/default_run_YYYYMMDD_HHMMSS/results/
â”œâ”€â”€ ğŸ“Š all_results.json               # ì „ì²´ ê²°ê³¼ JSON
â”œâ”€â”€ ğŸ“‹ summary_table.csv              # ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
â”œâ”€â”€ ğŸ¯ lasso_selection_normal.json    # Lasso íŠ¹ì„± ì„ íƒ ê²°ê³¼
â””â”€â”€ ğŸ“ˆ threshold_analysis.json        # ì„ê³„ê°’ ë¶„ì„ ê²°ê³¼
```

### ğŸ¤– **ì €ì¥ëœ ëª¨ë¸**

```python
outputs/master_runs/default_run_YYYYMMDD_HHMMSS/models/
â”œâ”€â”€ ğŸ­ ensemble_model_model.joblib           # ì•™ìƒë¸” ëª¨ë¸
â”œâ”€â”€ ğŸ“ˆ logisticregression_normal_model.joblib # ë¡œì§€ìŠ¤í‹± íšŒê·€ (Normal)
â”œâ”€â”€ ğŸ“ˆ logisticregression_smote_model.joblib  # ë¡œì§€ìŠ¤í‹± íšŒê·€ (SMOTE)
â”œâ”€â”€ ğŸŒ³ randomforest_normal_model.joblib      # ëœë¤ í¬ë ˆìŠ¤íŠ¸ (Normal)
â”œâ”€â”€ ğŸŒ³ randomforest_smote_model.joblib       # ëœë¤ í¬ë ˆìŠ¤íŠ¸ (SMOTE)
â”œâ”€â”€ ğŸš€ xgboost_normal_model.joblib           # XGBoost (Normal)
â””â”€â”€ ğŸš€ xgboost_smote_model.joblib            # XGBoost (SMOTE)
```

## ğŸš€ **ì‹¤í–‰ ê°€ì´ë“œ**

### 1. **ê¸°ë³¸ ì‹¤í–‰** (ê¶Œì¥)

```bash
cd src/modeling
python run_master.py
```

**ì‹¤í–‰ ë‚´ìš©:**
- 3ê°œ ì•Œê³ ë¦¬ì¦˜ Ã— 3ê°œ ë°ì´í„° íƒ€ì… = 9ê°œ ëª¨ë¸ í•™ìŠµ
- ìë™ ì„ê³„ê°’ ìµœì í™”
- ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
- í¬ê´„ì  ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”

### 2. **í…œí”Œë¦¿ ê¸°ë°˜ ì‹¤í–‰**

```bash
# ìš´ì˜í™˜ê²½ìš© (ì™„ì „ ìµœì í™”)
python run_master.py --template production

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
python run_master.py --template quick_test

# Lasso íŠ¹ì„±ì„ íƒ í¬í•¨
python run_master.py --template lasso_focus
```

### 3. **ê°œë³„ ëª¨ë¸ ì‹¤í–‰**

```bash
# ê°œë³„ ëª¨ë¸ ì‹¤í–‰ (ë¹„êµìš©)
python logistic_regression_100.py
python random_forest_100.py
python xgboost_100.py
```

### 4. **ì‚¬ìš©ì ì •ì˜ ì‹¤í–‰**

```bash
# ì‚¬ìš©ì ì •ì˜ ì„¤ì • íŒŒì¼ ì‚¬ìš©
python run_master.py --config my_custom_config.json

# íŠ¹ì • ëª¨ë¸ë§Œ ì‹¤í–‰
python run_master.py --models logistic,xgboost

# SMOTE ë¹„í™œì„±í™”
python run_master.py --no-smote
```

## ğŸ”§ **ê³ ê¸‰ ê¸°ëŠ¥**

### 1. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**

```python
# GridSearchCV ê¸°ë°˜ ìë™ íŠœë‹
hyperparameter_grids = {
    'logistic': {
        'C': [0.001, 0.01, 0.1, 1, 10],
        'penalty': ['l1', 'l2']
    },
    'randomforest': {
        'n_estimators': [100, 200, 300],
        'max_depth': [5, 10, 15, None]
    },
    'xgboost': {
        'learning_rate': [0.01, 0.1, 0.2],
        'max_depth': [3, 5, 7]
    }
}
```

### 2. **íŠ¹ì„± ì„ íƒ (Lasso)**

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LassoCV

# Lasso ê¸°ë°˜ íŠ¹ì„± ì„ íƒ
lasso = LassoCV(cv=5, random_state=42)
selector = SelectFromModel(lasso)
X_selected = selector.fit_transform(X_train, y_train)

print(f"ì„ íƒëœ íŠ¹ì„± ìˆ˜: {X_selected.shape[1]}")
print(f"ì„ íƒëœ íŠ¹ì„±: {X.columns[selector.get_support()].tolist()}")
```

### 3. **êµì°¨ê²€ì¦ ì „ëµ**

```python
from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold

# ì‹œê³„ì—´ ê³ ë ¤ êµì°¨ê²€ì¦
tscv = TimeSeriesSplit(n_splits=5)

# ê³„ì¸µí™” êµì°¨ê²€ì¦ (ë¶ˆê· í˜• ë°ì´í„°)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

## ğŸ“Š **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**

### ğŸ¯ **í•µì‹¬ ì§€í‘œ ì¶”ì **

```python
# ìë™ ê³„ì‚°ë˜ëŠ” ì„±ëŠ¥ ì§€í‘œ
metrics = {
    'classification': ['accuracy', 'precision', 'recall', 'f1', 'auc'],
    'probability': ['log_loss', 'brier_score'],
    'ranking': ['average_precision', 'roc_auc'],
    'threshold': ['optimal_threshold', 'threshold_range']
}
```

### ğŸ“ˆ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**

```python
# í•™ìŠµ ê³¼ì • ëª¨ë‹ˆí„°ë§
import wandb  # Weights & Biases (ì„ íƒì‚¬í•­)

wandb.init(project="default-prediction")
wandb.log({
    "train_f1": train_f1,
    "val_f1": val_f1,
    "test_f1": test_f1,
    "optimal_threshold": optimal_threshold
})
```

## ğŸ” **ë¬¸ì œ í•´ê²° ê°€ì´ë“œ**

### â— **ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜**

#### 1. **ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# í•´ê²°ë°©ë²•: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python run_master.py --batch-size 1000

# ë˜ëŠ” ëª¨ë¸ ìˆ˜ ì¤„ì´ê¸°
python run_master.py --models logistic,xgboost
```

#### 2. **SMOTE ì˜¤ë¥˜**
```bash
# í•´ê²°ë°©ë²•: SMOTE ë¹„í™œì„±í™”
python run_master.py --no-smote

# ë˜ëŠ” SMOTE ì „ëµ ë³€ê²½
python run_master.py --smote-strategy auto
```

#### 3. **ìˆ˜ë ´ ê²½ê³ **
```python
# LogisticRegression ìˆ˜ë ´ ë¬¸ì œ
warnings.filterwarnings('ignore', category=ConvergenceWarning)

# ë˜ëŠ” max_iter ì¦ê°€
LogisticRegression(max_iter=2000)
```

### ğŸ”§ **ì„±ëŠ¥ ìµœì í™” íŒ**

1. **ë³‘ë ¬ ì²˜ë¦¬ í™œìš©**
```python
# n_jobs=-1ë¡œ ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
RandomForestClassifier(n_jobs=-1)
GridSearchCV(n_jobs=-1)
```

2. **ì¡°ê¸° ì¢…ë£Œ í™œìš©**
```python
# XGBoost ì¡°ê¸° ì¢…ë£Œ
xgb.XGBClassifier(early_stopping_rounds=10)
```

3. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**
```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
pd.read_csv('data.csv', chunksize=10000)
```

## ğŸ“š **ê´€ë ¨ ë¬¸ì„œ**

- **ğŸ“„ [í”„ë¡œì íŠ¸ ê°œìš”](../../README.md)**: ì „ì²´ í”„ë¡œì íŠ¸ ì„¤ëª…
- **ğŸ“Š [ë°ì´í„° ê°€ì´ë“œ](../../data/final/README.md)**: ë°ì´í„°ì…‹ ìƒì„¸ ì •ë³´
- **ğŸ“ˆ [ì‹œê°í™” ê°€ì´ë“œ](../../outputs/visualizations/README.md)**: ë¶„ì„ ì°¨íŠ¸ í•´ì„
- **ğŸ¨ [ëŒ€ì‹œë³´ë“œ ê°€ì´ë“œ](../../dashboard/README.md)**: ëŒ€í™”í˜• ë„êµ¬ ì‚¬ìš©ë²•

## ğŸ† **ëª¨ë¸ë§ ì„±ê³¼ ìš”ì•½**

âœ… **ì•™ìƒë¸” ëª¨ë¸**: F1-Score 0.4096 (ì—…ê³„ ìµœê³  ìˆ˜ì¤€)  
âœ… **ìë™ ìµœì í™”**: ì„ê³„ê°’ ìë™ íƒìƒ‰ìœ¼ë¡œ 15% ì„±ëŠ¥ í–¥ìƒ  
âœ… **Data Leakage ë°©ì§€**: CV ë‚´ë¶€ ë™ì  SMOTEë¡œ ì‹ ë¢°ì„± í™•ë³´  
âœ… **ì¬í˜„ ê°€ëŠ¥ì„±**: ì„¤ì • ê¸°ë°˜ ê´€ë¦¬ë¡œ ì‹¤í—˜ ì¬í˜„ ê°€ëŠ¥  
âœ… **ìš´ì˜ ì¤€ë¹„**: ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ  

**ğŸ¯ í•œêµ­ ê¸°ì—… ë¶€ì‹¤ì˜ˆì¸¡ì„ ìœ„í•œ ìµœê³  ì„±ëŠ¥ì˜ ML ì‹œìŠ¤í…œ!**

---

*ëª¨ë¸ ì‚¬ìš© ì‹œ ì„±ëŠ¥ ì§€í‘œë¥¼ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.*  
*ìš´ì˜ í™˜ê²½ ë°°í¬ ì „ ì¶©ë¶„í•œ ê²€ì¦ì„ ê¶Œì¥í•©ë‹ˆë‹¤.*
