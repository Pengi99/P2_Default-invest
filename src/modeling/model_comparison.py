"""
ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

from sklearn.metrics import roc_curve, auc
import joblib

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic' if plt.matplotlib.get_backend() != 'Agg' else 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class ModelComparison:
    def __init__(self):
        self.models = {}
        self.results = {}
        self.test_data = None
        
    def load_models_and_results(self, models_path='outputs/models/'):
        """ì €ì¥ëœ ëª¨ë¸ë“¤ê³¼ ê²°ê³¼ë¥¼ ë¡œë“œ"""
        print("ğŸ“‚ ëª¨ë¸ ë° ê²°ê³¼ ë¡œë“œ")
        print("="*60)
        
        model_files = {
            'LogisticRegression': 'logistic_regression_best_model.joblib',
            'RandomForest': 'random_forest_best_model.joblib',
            'XGBoost': 'xgboost_best_model.joblib'
        }
        
        result_files = {
            'LogisticRegression': 'logistic_regression_results.json',
            'RandomForest': 'random_forest_results.json',
            'XGBoost': 'xgboost_results.json'
        }
        
        # ëª¨ë¸ ë¡œë“œ
        for model_name, model_file in model_files.items():
            model_path = os.path.join(models_path, model_file)
            if os.path.exists(model_path):
                self.models[model_name] = joblib.load(model_path)
                print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âš ï¸ {model_name} ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        # ê²°ê³¼ ë¡œë“œ
        for model_name, result_file in result_files.items():
            result_path = os.path.join(models_path, result_file)
            if os.path.exists(result_path):
                with open(result_path, 'r', encoding='utf-8') as f:
                    self.results[model_name] = json.load(f)
                print(f"âœ… {model_name} ê²°ê³¼ ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âš ï¸ {model_name} ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result_path}")
        
        print(f"\nğŸ“Š ë¡œë“œëœ ëª¨ë¸: {len(self.models)}ê°œ")
        print(f"ğŸ“Š ë¡œë“œëœ ê²°ê³¼: {len(self.results)}ê°œ")
    
    def load_test_data(self, data_path='data/final/'):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ")
        print("="*60)
        
        X_test = pd.read_csv(os.path.join(data_path, 'X_test_smote.csv'))
        y_test = pd.read_csv(os.path.join(data_path, 'y_test_smote.csv')).iloc[:, 0]
        
        self.test_data = {'X': X_test, 'y': y_test}
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        print(f"   í˜•íƒœ: {X_test.shape}")
        print(f"   ë¶€ì‹¤ ë¹„ìœ¨: {y_test.mean():.2%}")
    
    def create_performance_comparison(self):
        """ì„±ëŠ¥ ì§€í‘œ ë¹„êµí‘œ ìƒì„±"""
        print("\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë¹„êµ")
        print("="*60)
        
        metrics_df = []
        
        for model_name, result in self.results.items():
            if 'test_metrics' in result:
                metrics = result['test_metrics']
                metrics_df.append({
                    'Model': model_name,
                    'AUC': metrics.get('auc', 0),
                    'Precision': metrics.get('precision', 0),
                    'Recall': metrics.get('recall', 0),
                    'F1-Score': metrics.get('f1', 0),
                    'CV_Score': result.get('cv_best_score', 0)
                })
        
        self.performance_df = pd.DataFrame(metrics_df)
        
        if not self.performance_df.empty:
            print("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
            print(self.performance_df.round(4))
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
            best_auc_model = self.performance_df.loc[self.performance_df['AUC'].idxmax(), 'Model']
            best_f1_model = self.performance_df.loc[self.performance_df['F1-Score'].idxmax(), 'Model']
            
            print(f"\nğŸ¥‡ ìµœê³  AUC: {best_auc_model}")
            print(f"ğŸ¥‡ ìµœê³  F1-Score: {best_f1_model}")
        
        return self.performance_df
    
    def plot_roc_comparison(self, save_path='outputs/visualizations/'):
        """ROC ê³¡ì„  ë¹„êµ"""
        print("\nğŸ“ˆ ROC ê³¡ì„  ë¹„êµ")
        print("="*60)
        
        plt.figure(figsize=(10, 8))
        
        colors = ['blue', 'red', 'green', 'purple', 'orange']
        
        for i, (model_name, model) in enumerate(self.models.items()):
            if self.test_data is not None:
                # ì˜ˆì¸¡ í™•ë¥ 
                y_proba = model.predict_proba(self.test_data['X'])[:, 1]
                
                # ROC ê³¡ì„  ê³„ì‚°
                fpr, tpr, _ = roc_curve(self.test_data['y'], y_proba)
                roc_auc = auc(fpr, tpr)
                
                # í”Œë¡¯
                plt.plot(fpr, tpr, color=colors[i], lw=2, 
                        label=f'{model_name} (AUC = {roc_auc:.4f})')
        
        # ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°)
        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', 
                label='Random Classifier (AUC = 0.5000)')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('ROC ê³¡ì„  ë¹„êµ', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right", fontsize=10)
        plt.grid(True, alpha=0.3)
        
        # ì €ì¥
        os.makedirs(save_path, exist_ok=True)
        plt.savefig(f'{save_path}/model_roc_comparison.png', dpi=300, bbox_inches='tight')
        print(f"âœ… ROC ë¹„êµ ì €ì¥: {save_path}/model_roc_comparison.png")
        
        plt.show()
    
    def plot_metrics_comparison(self, save_path='outputs/visualizations/'):
        """ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸"""
        print("\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸")
        print("="*60)
        
        if hasattr(self, 'performance_df') and not self.performance_df.empty:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ë¹„êµ', fontsize=16, fontweight='bold')
            
            metrics = ['AUC', 'Precision', 'Recall', 'F1-Score']
            colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
            
            for i, metric in enumerate(metrics):
                ax = axes[i//2, i%2]
                bars = ax.bar(self.performance_df['Model'], self.performance_df[metric], 
                             color=colors[i], alpha=0.7, edgecolor='black')
                
                # ê°’ í‘œì‹œ
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=10)
                
                ax.set_title(f'{metric} ë¹„êµ', fontsize=12, fontweight='bold')
                ax.set_ylabel(metric, fontsize=10)
                ax.set_ylim(0, 1.1)
                ax.grid(True, alpha=0.3)
                
                # xì¶• ë ˆì´ë¸” íšŒì „
                ax.tick_params(axis='x', rotation=45)
            
            plt.tight_layout()
            
            # ì €ì¥
            os.makedirs(save_path, exist_ok=True)
            plt.savefig(f'{save_path}/model_metrics_comparison.png', dpi=300, bbox_inches='tight')
            print(f"âœ… ì§€í‘œ ë¹„êµ ì €ì¥: {save_path}/model_metrics_comparison.png")
            
            plt.show()
    
    def plot_feature_importance_comparison(self, save_path='outputs/visualizations/'):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ"""
        print("\nğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ")
        print("="*60)
        
        # Random Forestì™€ XGBoostì˜ íŠ¹ì„± ì¤‘ìš”ë„ë§Œ ë¹„êµ (ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ê³„ìˆ˜)
        tree_models = ['RandomForest', 'XGBoost']
        available_models = [model for model in tree_models if model in self.results]
        
        if len(available_models) >= 2:
            fig, axes = plt.subplots(1, len(available_models), figsize=(15, 8))
            if len(available_models) == 1:
                axes = [axes]
            
            fig.suptitle('íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ (Tree-based ëª¨ë¸)', fontsize=16, fontweight='bold')
            
            for i, model_name in enumerate(available_models):
                if 'feature_importances' in self.results[model_name]:
                    importances = self.results[model_name]['feature_importances']
                    
                    # ìƒìœ„ 10ê°œ íŠ¹ì„±
                    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]
                    features, values = zip(*sorted_features)
                    
                    # ìƒ‰ìƒ ì„¤ì •
                    color = 'green' if model_name == 'RandomForest' else 'purple'
                    
                    axes[i].barh(features, values, color=color, alpha=0.7)
                    axes[i].set_title(f'{model_name}\níŠ¹ì„± ì¤‘ìš”ë„ (Top 10)', fontsize=12, fontweight='bold')
                    axes[i].set_xlabel('ì¤‘ìš”ë„', fontsize=10)
                    
                    # ê°’ í‘œì‹œ
                    for j, v in enumerate(values):
                        axes[i].text(v + 0.001, j, f'{v:.3f}', va='center', fontsize=9)
            
            plt.tight_layout()
            
            # ì €ì¥
            os.makedirs(save_path, exist_ok=True)
            plt.savefig(f'{save_path}/feature_importance_comparison.png', dpi=300, bbox_inches='tight')
            print(f"âœ… íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ ì €ì¥: {save_path}/feature_importance_comparison.png")
            
            plt.show()
        else:
            print("âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµë¥¼ ìœ„í•œ ì¶©ë¶„í•œ Tree-based ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def generate_summary_report(self, save_path='outputs/reports/'):
        """ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±")
        print("="*60)
        
        os.makedirs(save_path, exist_ok=True)
        
        report = {
            'analysis_date': datetime.now().isoformat(),
            'models_compared': list(self.models.keys()),
            'performance_summary': self.performance_df.to_dict('records') if hasattr(self, 'performance_df') else [],
            'best_models': {},
            'recommendations': []
        }
        
        if hasattr(self, 'performance_df') and not self.performance_df.empty:
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤
            report['best_models'] = {
                'best_auc': {
                    'model': self.performance_df.loc[self.performance_df['AUC'].idxmax(), 'Model'],
                    'score': float(self.performance_df['AUC'].max())
                },
                'best_precision': {
                    'model': self.performance_df.loc[self.performance_df['Precision'].idxmax(), 'Model'],
                    'score': float(self.performance_df['Precision'].max())
                },
                'best_recall': {
                    'model': self.performance_df.loc[self.performance_df['Recall'].idxmax(), 'Model'],
                    'score': float(self.performance_df['Recall'].max())
                },
                'best_f1': {
                    'model': self.performance_df.loc[self.performance_df['F1-Score'].idxmax(), 'Model'],
                    'score': float(self.performance_df['F1-Score'].max())
                }
            }
            
            # ì¶”ì²œì‚¬í•­
            best_auc_model = report['best_models']['best_auc']['model']
            best_auc_score = report['best_models']['best_auc']['score']
            
            if best_auc_score > 0.8:
                report['recommendations'].append(f"{best_auc_model}ì´ ìš°ìˆ˜í•œ ì„±ëŠ¥(AUC: {best_auc_score:.4f})ì„ ë³´ì—¬ ìš´ì˜ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤.")
            elif best_auc_score > 0.7:
                report['recommendations'].append(f"{best_auc_model}ì´ ì–‘í˜¸í•œ ì„±ëŠ¥ì„ ë³´ì´ë‚˜ ì¶”ê°€ íŠœë‹ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                report['recommendations'].append("ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ê¸°ëŒ€ì— ë¯¸ì¹˜ì§€ ëª»í•©ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ ë˜ëŠ” íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì¬ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # ë¶ˆê· í˜• ë°ì´í„° ê´€ë ¨ ì¶”ì²œ
            avg_precision = self.performance_df['Precision'].mean()
            avg_recall = self.performance_df['Recall'].mean()
            
            if avg_precision > avg_recall:
                report['recommendations'].append("Precisionì´ Recallë³´ë‹¤ ë†’ìŠµë‹ˆë‹¤. ì‹¤ì œ ë¶€ì‹¤ ê¸°ì—… íƒì§€ìœ¨ í–¥ìƒì„ ìœ„í•œ ì„ê³„ê°’ ì¡°ì •ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
            else:
                report['recommendations'].append("Recallì´ ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŠµë‹ˆë‹¤. False Positive ê°ì†Œë¥¼ ìœ„í•œ ëª¨ë¸ ì •ë°€ë„ í–¥ìƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_path = os.path.join(save_path, 'model_comparison_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“Š ë¶„ì„ ìš”ì•½:")
        if 'best_models' in report and report['best_models']:
            print(f"   ğŸ¥‡ ìµœê³  AUC: {report['best_models']['best_auc']['model']} ({report['best_models']['best_auc']['score']:.4f})")
            print(f"   ğŸ¯ ìµœê³  F1: {report['best_models']['best_f1']['model']} ({report['best_models']['best_f1']['score']:.4f})")
        
        if report['recommendations']:
            print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"   {i}. {rec}")
        
        return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¢ í•œêµ­ ê¸°ì—… ë¶€ì‹¤ì˜ˆì¸¡ - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print("="*60)
    
    # ë¹„êµ ê°ì²´ ìƒì„±
    comparison = ModelComparison()
    
    # ëª¨ë¸ ë° ê²°ê³¼ ë¡œë“œ
    comparison.load_models_and_results()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    comparison.load_test_data()
    
    # ì„±ëŠ¥ ë¹„êµí‘œ ìƒì„±
    performance_df = comparison.create_performance_comparison()
    
    # ì‹œê°í™”
    comparison.plot_roc_comparison()
    comparison.plot_metrics_comparison()
    comparison.plot_feature_importance_comparison()
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    report = comparison.generate_summary_report()
    
    print("\nğŸ‰ ëª¨ë¸ ë¹„êµ ë¶„ì„ ì™„ë£Œ!")
    print("ğŸ“Š ëª¨ë“  ê²°ê³¼ëŠ” outputs/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 