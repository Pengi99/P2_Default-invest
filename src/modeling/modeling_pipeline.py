"""
ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸
===============================

ê¸°ëŠ¥:
1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
2. ë‹¤ì–‘í•œ ìƒ˜í”Œë§ ì „ëµ ì ìš© (Normal, SMOTE, Undersampling, Combined)
3. ëª¨ë¸ ìµœì í™” (LogisticRegression, RandomForest, XGBoost)
4. Threshold ìµœì í™”
5. ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
6. ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”

Configë¥¼ í†µí•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›
"""

import pandas as pd
import numpy as np
import yaml
import json
import pickle
import logging
import os
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Union
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic' if plt.matplotlib.get_backend() != 'Agg' else 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ëª¨ë¸ë§ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.linear_model import LogisticRegression, LassoCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score, balanced_accuracy_score
)
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler

# ìƒ˜í”Œë§ ë¼ì´ë¸ŒëŸ¬ë¦¬
from imblearn.over_sampling import BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks


class ModelingPipeline:
    """
    ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
    
    Config íŒŒì¼ì„ í†µí•´ ëª¨ë“  ì„¤ì •ì„ ê´€ë¦¬í•˜ë©°,
    ë°ì´í„° ë¡œë“œë¶€í„° ëª¨ë¸ í›ˆë ¨, í‰ê°€ê¹Œì§€ ì „ì²´ ê³¼ì •ì„ ìˆ˜í–‰
    """
    
    def __init__(self, config_path: str):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config_path: config YAML/JSON íŒŒì¼ ê²½ë¡œ
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.logger = self._setup_logging()
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.project_root = Path(__file__).parent.parent.parent
        
        # ì‹¤í–‰ ì •ë³´ ì„¤ì •
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"{self.config['experiment']['name']}_{self.timestamp}"
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = self.project_root / self.config['output']['base_dir'] / self.run_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'results').mkdir(exist_ok=True)
        (self.output_dir / 'visualizations').mkdir(exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        self.results = {
            'experiment_info': {},
            'data_info': {},
            'modeling_steps': {},
            'model_performance': {},
            'ensemble_results': {}
        }
        
        # ë°ì´í„° ë° ëª¨ë¸ ì €ì¥ìš©
        self.data = {}
        self.models = {}
        self.model_results = {}
        
        self.logger.info("ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        self.logger.info(f"ì‹¤í–‰ ì´ë¦„: {self.run_name}")
        self.logger.info(f"ì¶œë ¥ ê²½ë¡œ: {self.output_dir}")
    
    def _load_config(self) -> Dict:
        """Config íŒŒì¼ ë¡œë“œ"""
        try:
            # YAML ìš°ì„  ì‹œë„
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        except yaml.YAMLError:
            # YAML ì‹¤íŒ¨ ì‹œ JSON ì‹œë„
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
        return config
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger('ModelingPipeline')
        logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        if self.config['logging']['save_to_file']:
            log_dir = self.output_dir / 'logs'
            log_dir.mkdir(parents=True, exist_ok=True)
            
            log_file = log_dir / f"modeling_{self.timestamp}.log"
            file_handler = logging.FileHandler(log_file)
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def load_data(self) -> Dict[str, pd.DataFrame]:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        self.logger.info("ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        data_path = self.project_root / self.config['data']['input_path']
        
        # ë°ì´í„° ë¡œë“œ
        self.data['normal'] = {
            'X_train': pd.read_csv(data_path / self.config['data']['files']['X_train']),
            'X_val': pd.read_csv(data_path / self.config['data']['files']['X_val']),
            'X_test': pd.read_csv(data_path / self.config['data']['files']['X_test']),
            'y_train': pd.read_csv(data_path / self.config['data']['files']['y_train']).iloc[:, 0],
            'y_val': pd.read_csv(data_path / self.config['data']['files']['y_val']).iloc[:, 0],
            'y_test': pd.read_csv(data_path / self.config['data']['files']['y_test']).iloc[:, 0]
        }
        
        # í™œì„±í™”ëœ ë°ì´í„° íƒ€ì…ë³„ë¡œ ë³µì‚¬ (ë™ì  ìƒ˜í”Œë§ì„ ìœ„í•´)
        enabled_data_types = [dt for dt, config in self.config['sampling']['data_types'].items() if config['enabled']]
        for data_type in enabled_data_types:
            if data_type != 'normal':
                self.data[data_type] = {k: v.copy() for k, v in self.data['normal'].items()}
        
        # ë°ì´í„° ì •ë³´ ì¶œë ¥
        data = self.data['normal']
        self.logger.info(f"Train: {data['X_train'].shape}, ë¶€ì‹¤ë¹„ìœ¨: {data['y_train'].mean():.2%}")
        self.logger.info(f"Validation: {data['X_val'].shape}, ë¶€ì‹¤ë¹„ìœ¨: {data['y_val'].mean():.2%}")
        self.logger.info(f"Test: {data['X_test'].shape}, ë¶€ì‹¤ë¹„ìœ¨: {data['y_test'].mean():.2%}")
        
        # ë°ì´í„° ì •ë³´ ì €ì¥
        self.results['data_info'] = {
            'shapes': {
                'train': data['X_train'].shape,
                'val': data['X_val'].shape,
                'test': data['X_test'].shape
            },
            'target_distribution': {
                'train': data['y_train'].value_counts().to_dict(),
                'val': data['y_val'].value_counts().to_dict(),
                'test': data['y_test'].value_counts().to_dict()
            },
            'feature_count': data['X_train'].shape[1],
            'feature_names': list(data['X_train'].columns)
        }
        
        return self.data
    
    def apply_sampling_strategy(self, X: pd.DataFrame, y: pd.Series, data_type: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ìƒ˜í”Œë§ ì „ëµ ì ìš©
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            data_type: ë°ì´í„° íƒ€ì… ('normal', 'smote', 'undersampling', 'combined')
            
        Returns:
            tuple: (X_resampled, y_resampled)
        """
        if data_type == 'normal':
            return X, y
        
        config = self.config['sampling']['data_types'][data_type]
        
        if data_type == 'smote':
            smote = BorderlineSMOTE(
                sampling_strategy=config['sampling_strategy'],
                random_state=self.config['random_state'],
                k_neighbors=config.get('k_neighbors', 5),
                m_neighbors=config.get('m_neighbors', 10)
            )
            return smote.fit_resample(X, y)
        
        elif data_type == 'undersampling':
            method = config['method']
            
            if method == 'random':
                undersampler = RandomUnderSampler(
                    sampling_strategy=config['sampling_strategy'],
                    random_state=self.config['random_state']
                )
            elif method == 'edited_nearest_neighbours':
                undersampler = EditedNearestNeighbours(
                    sampling_strategy=config['sampling_strategy']
                )
            elif method == 'tomek':
                undersampler = TomekLinks(
                    sampling_strategy=config['sampling_strategy']
                )
            else:
                self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ë”ìƒ˜í”Œë§ ë°©ë²•: {method}, Random ì‚¬ìš©")
                undersampler = RandomUnderSampler(
                    sampling_strategy=config['sampling_strategy'],
                    random_state=self.config['random_state']
                )
            
            return undersampler.fit_resample(X, y)
        
        elif data_type == 'combined':
            # 1ë‹¨ê³„: SMOTE ì ìš©
            smote = BorderlineSMOTE(
                sampling_strategy=config['smote_ratio'],
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            X_smote, y_smote = smote.fit_resample(X, y)
            
            # 2ë‹¨ê³„: ì–¸ë”ìƒ˜í”Œë§ ì ìš©
            undersampler = RandomUnderSampler(
                sampling_strategy=config['undersampling_ratio'],
                random_state=self.config['random_state']
            )
            X_combined, y_combined = undersampler.fit_resample(X_smote, y_smote)
            
            return X_combined, y_combined
        
        else:
            self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° íƒ€ì…: {data_type}, ì›ë³¸ ë°ì´í„° ë°˜í™˜")
            return X, y
    
    def apply_scaling(self, X_train: pd.DataFrame, X_val: pd.DataFrame, X_test: pd.DataFrame, data_type: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict]:
        """
        ì»¬ëŸ¼ë³„ ìŠ¤ì¼€ì¼ë§ ì ìš©
        
        Args:
            X_train: í›ˆë ¨ ë°ì´í„°
            X_val: ê²€ì¦ ë°ì´í„°
            X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            data_type: ë°ì´í„° íƒ€ì…
            
        Returns:
            tuple: (X_train_scaled, X_val_scaled, X_test_scaled, scalers)
        """
        if not self.config.get('scaling', {}).get('enabled', False):
            return X_train, X_val, X_test, {}
        
        self.logger.info(f"ìŠ¤ì¼€ì¼ë§ ì ìš© ì‹œì‘ ({data_type.upper()})")
        
        scaling_config = self.config['scaling']
        scalers = {}
        
        # ê° ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë³„ë¡œ ì»¬ëŸ¼ ì²˜ë¦¬
        for scaler_type, columns in scaling_config.get('column_groups', {}).items():
            if not columns:
                continue
                
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
            existing_columns = [col for col in columns if col in X_train.columns]
            if not existing_columns:
                self.logger.warning(f"{scaler_type} ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {columns}")
                continue
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
            if scaler_type.lower() == 'standard':
                scaler = StandardScaler()
            elif scaler_type.lower() == 'robust':
                scaler = RobustScaler()
            elif scaler_type.lower() == 'minmax':
                scaler = MinMaxScaler()
            else:
                self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìŠ¤ì¼€ì¼ë§ ë°©ë²•: {scaler_type}, Standard ì‚¬ìš©")
                scaler = StandardScaler()
            
            # í›ˆë ¨ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ…
            scaler.fit(X_train[existing_columns])
            
            # ìŠ¤ì¼€ì¼ë§ ì ìš©
            X_train.loc[:, existing_columns] = scaler.transform(X_train[existing_columns])
            X_val.loc[:, existing_columns] = scaler.transform(X_val[existing_columns])
            X_test.loc[:, existing_columns] = scaler.transform(X_test[existing_columns])
            
            scalers[scaler_type] = {
                'scaler': scaler,
                'columns': existing_columns
            }
            
            self.logger.info(f"{scaler_type} ìŠ¤ì¼€ì¼ë§ ì ìš©: {len(existing_columns)}ê°œ ì»¬ëŸ¼")
        
        # ìŠ¤ì¼€ì¼ë§ ì •ë³´ ì €ì¥
        scaling_info = {
            'enabled': True,
            'scalers_used': list(scalers.keys()),
            'total_scaled_columns': sum(len(info['columns']) for info in scalers.values()),
            'scaling_details': {
                scaler_type: {
                    'method': scaler_type,
                    'columns': info['columns'],
                    'column_count': len(info['columns'])
                }
                for scaler_type, info in scalers.items()
            }
        }
        
        self.logger.info(f"ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ: {scaling_info['total_scaled_columns']}ê°œ ì»¬ëŸ¼")
        
        return X_train, X_val, X_test, scalers
    
    def apply_lasso_feature_selection(self, data_type: str = 'normal'):
        """Lasso íŠ¹ì„± ì„ íƒ ì ìš©"""
        if not self.config['feature_selection']['enabled']:
            return
        
        self.logger.info(f"Lasso íŠ¹ì„± ì„ íƒ ì ìš© ({data_type.upper()})")
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Lasso CV
        lasso_config = self.config['feature_selection']['lasso']
        lasso_cv = LassoCV(
            alphas=lasso_config['alphas'],
            cv=lasso_config['cv_folds'],
            random_state=self.config['random_state'],
            n_jobs=self.config.get('performance', {}).get('n_jobs', 1)
        )
        lasso_cv.fit(X_train_scaled, y_train)
        
        # íŠ¹ì„± ì„ íƒ
        threshold = lasso_config['threshold']
        if threshold == 'median':
            threshold_value = np.median(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        elif threshold == 'mean':
            threshold_value = np.mean(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        else:
            threshold_value = float(threshold)
        
        selected_mask = np.abs(lasso_cv.coef_) >= threshold_value
        selected_features = X_train.columns[selected_mask].tolist()
        
        self.logger.info(f"ìµœì  alpha: {lasso_cv.alpha_:.6f}")
        self.logger.info(f"ì„ íƒëœ íŠ¹ì„±: {len(selected_features)}/{len(X_train.columns)}")
        
        # ëª¨ë“  ë°ì´í„° íƒ€ì…ì— ë™ì¼í•œ íŠ¹ì„± ì ìš©
        enabled_data_types = [dt for dt, config in self.config['sampling']['data_types'].items() if config['enabled']]
        for dt in enabled_data_types:
            for split in ['X_train', 'X_val', 'X_test']:
                self.data[dt][split] = self.data[dt][split][selected_features]
        
        # ê²°ê³¼ ì €ì¥
        lasso_results = {
            'optimal_alpha': float(lasso_cv.alpha_),
            'threshold_value': float(threshold_value),
            'original_features': len(X_train.columns),
            'selected_features': len(selected_features),
            'selected_feature_names': selected_features,
            'feature_coefficients': dict(zip(X_train.columns, lasso_cv.coef_))
        }
        
        self.results['modeling_steps']['feature_selection'] = lasso_results
        
        # íŒŒì¼ë¡œ ì €ì¥
        with open(self.output_dir / 'results' / 'lasso_selection.json', 'w', encoding='utf-8') as f:
            json.dump(lasso_results, f, indent=2, ensure_ascii=False)
    
    def optimize_model(self, model_name: str, data_type: str):
        """ëª¨ë¸ ìµœì í™”"""
        self.logger.info(f"{model_name} ìµœì í™” ì‹œì‘ ({data_type.upper()})")
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        if model_name == 'logistic_regression':
            return self._optimize_logistic_regression(X_train, y_train, data_type)
        elif model_name == 'random_forest':
            return self._optimize_random_forest(X_train, y_train, data_type)
        elif model_name == 'xgboost':
            return self._optimize_xgboost(X_train, y_train, data_type)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
    
    def _optimize_logistic_regression(self, X_train: pd.DataFrame, y_train: pd.Series, data_type: str):
        """ë¡œì§€ìŠ¤í‹± íšŒê·€ ìµœì í™”"""
        config = self.config['models']['logistic_regression']
        
        def objective(trial):
            # penaltyì™€ solver ì¡°í•© ì„ íƒ
            penalty_solver_combinations = []
            
            for penalty in config['penalty']:
                if penalty == 'l1':
                    for solver in ['liblinear', 'saga']:
                        penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'l2':
                    for solver in config['l2_solvers']:
                        if solver in ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']:
                            penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'elasticnet':
                    penalty_solver_combinations.append(f"{penalty}_saga")
            
            combination = trial.suggest_categorical('penalty_solver', penalty_solver_combinations)
            penalty, solver = combination.split('_', 1)
            
            C = trial.suggest_float('C', *config['C_range'], log=True)
            max_iter = trial.suggest_int('max_iter', *config['max_iter_range'])
            
            params = {
                'penalty': penalty,
                'C': C,
                'solver': solver,
                'max_iter': max_iter,
                'random_state': self.config['random_state']
            }
            
            if penalty == 'elasticnet':
                params['l1_ratio'] = trial.suggest_float('l1_ratio', *config['l1_ratio_range'])
            
            model = LogisticRegression(**params)
            
            # Cross validation (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
            scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=config['n_trials'])
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params.copy()
        penalty_solver = best_params.pop('penalty_solver')
        penalty, solver = penalty_solver.split('_', 1)
        best_params['penalty'] = penalty
        best_params['solver'] = solver
        
        model = LogisticRegression(**best_params)
        
        # ëª¨ë¸ í›ˆë ¨ (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
        model.fit(X_train, y_train)
        
        self.logger.info(f"ìµœì  AUC: {study.best_value:.4f}")
        self.logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return model, best_params, study.best_value
    
    def _optimize_random_forest(self, X_train: pd.DataFrame, y_train: pd.Series, data_type: str):
        """ëœë¤ í¬ë ˆìŠ¤íŠ¸ ìµœì í™”"""
        config = self.config['models']['random_forest']
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *config['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *config['max_depth_range']),
                'min_samples_split': trial.suggest_int('min_samples_split', *config['min_samples_split_range']),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', *config['min_samples_leaf_range']),
                'max_features': trial.suggest_float('max_features', *config['max_features_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1
            }
            
            model = RandomForestClassifier(**params)
            
            # Cross validation (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
            scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=config['n_trials'])
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params
        model = RandomForestClassifier(**best_params)
        
        # ëª¨ë¸ í›ˆë ¨ (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
        model.fit(X_train, y_train)
        
        self.logger.info(f"ìµœì  AUC: {study.best_value:.4f}")
        self.logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return model, best_params, study.best_value
    
    def _optimize_xgboost(self, X_train: pd.DataFrame, y_train: pd.Series, data_type: str):
        """XGBoost ìµœì í™”"""
        config = self.config['models']['xgboost']
        
        def objective(trial):
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'n_estimators': trial.suggest_int('n_estimators', *config['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *config['max_depth_range']),
                'learning_rate': trial.suggest_float('learning_rate', *config['learning_rate_range']),
                'subsample': trial.suggest_float('subsample', *config['subsample_range']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *config['colsample_bytree_range']),
                'reg_alpha': trial.suggest_float('reg_alpha', *config['reg_alpha_range']),
                'reg_lambda': trial.suggest_float('reg_lambda', *config['reg_lambda_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1,
                'verbosity': 0
            }
            
            model = xgb.XGBClassifier(**params)
            
            # Cross validation (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
            scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=config['n_trials'])
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params
        model = xgb.XGBClassifier(**best_params)
        
        # ëª¨ë¸ í›ˆë ¨ (ì´ë¯¸ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš©)
        model.fit(X_train, y_train)
        
        self.logger.info(f"ìµœì  AUC: {study.best_value:.4f}")
        self.logger.info(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return model, best_params, study.best_value
    
    def _proper_cv_with_sampling(self, model, X: pd.DataFrame, y: pd.Series, data_type: str, cv_folds: int = 5):
        """
        ìƒ˜í”Œë§ Data Leakageë¥¼ ë°©ì§€í•˜ëŠ” ì˜¬ë°”ë¥¸ Cross Validation
        ê° CV foldë§ˆë‹¤ ìƒ˜í”Œë§ì„ ë³„ë„ë¡œ ì ìš©
        """
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.config['random_state'])
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            # ê° foldë§ˆë‹¤ ë³„ë„ë¡œ ë¶„í• 
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # í›ˆë ¨ foldì—ë§Œ ìƒ˜í”Œë§ ì ìš© (Data Leakage ë°©ì§€)
            try:
                X_fold_train_resampled, y_fold_train_resampled = self.apply_sampling_strategy(
                    X_fold_train, y_fold_train, data_type
                )
                
                # ëª¨ë¸ ë³µì‚¬ ë° í›ˆë ¨
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train_resampled, y_fold_train_resampled)
                
                # ê²€ì¦ foldì—ì„œ í‰ê°€ (ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
                
            except Exception as e:
                self.logger.warning(f"Fold {fold+1} {data_type.upper()} ì ìš© ì‹¤íŒ¨: {e}")
                # ìƒ˜í”Œë§ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„°ë¡œ í›ˆë ¨
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train, y_fold_train)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
        
        return np.array(scores)
    
    def find_optimal_threshold(self, model_key: str):
        """ê° ëª¨ë¸ë³„ ìµœì  threshold ì°¾ê¸°"""
        self.logger.info(f"{model_key} ìµœì  Threshold íƒìƒ‰")
        
        model = self.models[model_key]
        data_type = self.model_results[model_key]['data_type']
        
        X_val = self.data[data_type]['X_val']
        y_val = self.data[data_type]['y_val']
        
        # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
        y_val_proba = model.predict_proba(X_val)[:, 1]
        
        # ë‹¤ì–‘í•œ thresholdì—ì„œì˜ ì„±ëŠ¥ ê³„ì‚°
        thresholds = np.arange(0.05, 0.5, 0.05)
        threshold_results = []
        
        for threshold in thresholds:
            y_val_pred = (y_val_proba >= threshold).astype(int)
            
            if len(np.unique(y_val_pred)) == 1:
                continue
            
            try:
                metrics = {
                    'threshold': threshold,
                    'precision': precision_score(y_val, y_val_pred, zero_division=0),
                    'recall': recall_score(y_val, y_val_pred, zero_division=0),
                    'f1': f1_score(y_val, y_val_pred, zero_division=0),
                    'balanced_accuracy': balanced_accuracy_score(y_val, y_val_pred)
                }
                threshold_results.append(metrics)
            except:
                continue
        
        if not threshold_results:
            self.logger.warning("ìµœì  threshold ì°¾ê¸° ì‹¤íŒ¨, ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©")
            return 0.5, {}
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        threshold_df = pd.DataFrame(threshold_results)
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ë³„ ìµœì  threshold ì°¾ê¸°
        metric_priority = self.config.get('threshold_optimization', {}).get('metric_priority', 'f1')
        
        optimal_thresholds = {}
        for metric in ['f1', 'precision', 'recall', 'balanced_accuracy']:
            if metric in threshold_df.columns:
                best_idx = threshold_df[metric].idxmax()
                optimal_thresholds[metric] = {
                    'threshold': threshold_df.loc[best_idx, 'threshold'],
                    'value': threshold_df.loc[best_idx, metric]
                }
        
        # ìš°ì„ ìˆœìœ„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ìµœì¢… threshold ì„ íƒ
        if metric_priority in optimal_thresholds:
            final_threshold = optimal_thresholds[metric_priority]['threshold']
            final_value = optimal_thresholds[metric_priority]['value']
        else:
            final_threshold = optimal_thresholds['f1']['threshold']
            final_value = optimal_thresholds['f1']['value']
        
        self.logger.info(f"ìµœì¢… ì„ íƒ: {final_threshold:.3f} ({metric_priority.upper()}: {final_value:.4f})")
        
        # Precision-Recall ê³¡ì„  ë°ì´í„° ì €ì¥
        precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_val, y_val_proba)
        
        threshold_analysis = {
            'all_thresholds': threshold_results,
            'optimal_by_metric': optimal_thresholds,
            'final_threshold': final_threshold,
            'final_metric': metric_priority,
            'final_value': final_value,
            'pr_curve': {
                'precision': precision_vals.tolist(),
                'recall': recall_vals.tolist(),
                'thresholds': pr_thresholds.tolist()
            }
        }
        
        return final_threshold, threshold_analysis
    
    def evaluate_model(self, model_key: str):
        """ëª¨ë¸ í‰ê°€ (ìµœì  threshold ì‚¬ìš©)"""
        # ìµœì  threshold ì°¾ê¸°
        optimal_threshold, threshold_analysis = self.find_optimal_threshold(model_key)
        
        model = self.models[model_key]
        data_type = self.model_results[model_key]['data_type']
        
        X_val = self.data[data_type]['X_val']
        y_val = self.data[data_type]['y_val']
        X_test = self.data[data_type]['X_test']
        y_test = self.data[data_type]['y_test']
        
        # ì˜ˆì¸¡ (ìµœì  threshold ì‚¬ìš©)
        y_val_proba = model.predict_proba(X_val)[:, 1]
        y_val_pred = (y_val_proba >= optimal_threshold).astype(int)
        
        y_test_proba = model.predict_proba(X_test)[:, 1]
        y_test_pred = (y_test_proba >= optimal_threshold).astype(int)
        
        # ê²€ì¦ ì„±ëŠ¥
        val_metrics = {
            'auc': roc_auc_score(y_val, y_val_proba),
            'precision': precision_score(y_val, y_val_pred, zero_division=0),
            'recall': recall_score(y_val, y_val_pred, zero_division=0),
            'f1': f1_score(y_val, y_val_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_val, y_val_pred),
            'average_precision': average_precision_score(y_val, y_val_proba)
        }
        
        # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
        test_metrics = {
            'auc': roc_auc_score(y_test, y_test_proba),
            'precision': precision_score(y_test, y_test_pred, zero_division=0),
            'recall': recall_score(y_test, y_test_pred, zero_division=0),
            'f1': f1_score(y_test, y_test_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),
            'average_precision': average_precision_score(y_test, y_test_proba)
        }
        
        # ê²°ê³¼ ì—…ë°ì´íŠ¸
        self.model_results[model_key].update({
            'optimal_threshold': optimal_threshold,
            'threshold_analysis': threshold_analysis,
            'val_metrics': val_metrics,
            'test_metrics': test_metrics,
            'predictions': {
                'y_val_proba': y_val_proba.tolist(),
                'y_test_proba': y_test_proba.tolist()
            }
        })
        
        self.logger.info(f"{model_key} ìµœì¢… í‰ê°€ (Threshold: {optimal_threshold:.3f}):")
        self.logger.info(f"ê²€ì¦ - AUC: {val_metrics['auc']:.4f}, F1: {val_metrics['f1']:.4f}")
        self.logger.info(f"í…ŒìŠ¤íŠ¸ - AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1']:.4f}")
    
    def run_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ì‹¤í–‰"""
        self.logger.info("ëª¨ë“  ëª¨ë¸ ì‹¤í–‰ ì‹œì‘")
        
        # Lasso íŠ¹ì„± ì„ íƒ
        if self.config['feature_selection']['enabled']:
            self.apply_lasso_feature_selection('normal')
        
        # í™œì„±í™”ëœ ë°ì´í„° íƒ€ì… í™•ì¸
        enabled_data_types = [dt for dt, config in self.config['sampling']['data_types'].items() if config['enabled']]
        enabled_models = [model for model, config in self.config['models'].items() if config['enabled']]
        
        self.logger.info(f"í™œì„±í™”ëœ ë°ì´í„° íƒ€ì…: {enabled_data_types}")
        self.logger.info(f"í™œì„±í™”ëœ ëª¨ë¸: {enabled_models}")
        
        # ë°ì´í„° íƒ€ì…ë³„ë¡œ ëª¨ë¸ ì‹¤í–‰
        for data_type in enabled_data_types:
            self.logger.info(f"{data_type.upper()} ë°ì´í„° ì²˜ë¦¬")
            
            # ìƒ˜í”Œë§ ì ìš© (SMOTE ë“±)
            if data_type != 'normal':
                X_train_resampled, y_train_resampled = self.apply_sampling_strategy(
                    self.data[data_type]['X_train'], 
                    self.data[data_type]['y_train'], 
                    data_type
                )
                self.data[data_type]['X_train'] = X_train_resampled
                self.data[data_type]['y_train'] = y_train_resampled
                self.logger.info(f"{data_type.upper()} ìƒ˜í”Œë§ ì ìš© ì™„ë£Œ: {len(X_train_resampled):,}ê°œ ìƒ˜í”Œ")
            
            # ìŠ¤ì¼€ì¼ë§ ì ìš© (SMOTE í›„)
            X_train_scaled, X_val_scaled, X_test_scaled, scalers = self.apply_scaling(
                self.data[data_type]['X_train'].copy(),
                self.data[data_type]['X_val'].copy(), 
                self.data[data_type]['X_test'].copy(),
                data_type
            )
            
            # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            self.data[data_type]['X_train'] = X_train_scaled
            self.data[data_type]['X_val'] = X_val_scaled  
            self.data[data_type]['X_test'] = X_test_scaled
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì •ë³´ ì €ì¥
            if scalers:
                self.results['modeling_steps'][f'scaling_{data_type}'] = {
                    'scalers_info': {k: {'columns': v['columns']} for k, v in scalers.items()}
                }
            
            for model_name in enabled_models:
                model_key = f"{model_name}_{data_type}"
                
                # ëª¨ë¸ ìµœì í™” (ì´ë¯¸ ìƒ˜í”Œë§ê³¼ ìŠ¤ì¼€ì¼ë§ì´ ì ìš©ëœ ë°ì´í„° ì‚¬ìš©)
                model, best_params, cv_score = self.optimize_model(model_name, data_type)
                
                # ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
                self.models[model_key] = model
                self.model_results[model_key] = {
                    'model_name': model_name,
                    'data_type': data_type,
                    'best_params': best_params,
                    'cv_score': cv_score
                }
                
                # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥ (tree ê¸°ë°˜ ëª¨ë¸ì˜ ê²½ìš°)
                if hasattr(model, 'feature_importances_'):
                    feature_names = self.data[data_type]['X_train'].columns
                    self.model_results[model_key]['feature_importances'] = dict(
                        zip(feature_names, model.feature_importances_)
                    )
        
        # ëª¨ë“  ëª¨ë¸ í‰ê°€
        self.logger.info("ëª¨ë“  ëª¨ë¸ í‰ê°€ ë° Threshold ìµœì í™”")
        for model_key in self.models.keys():
            self.evaluate_model(model_key)
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        self.logger.info("ê²°ê³¼ ì €ì¥ ì‹œì‘")
        
        # ì‹¤í—˜ ì •ë³´ ì €ì¥
        self.results['experiment_info'] = {
            'name': self.config['experiment']['name'],
            'config_path': str(self.config_path),
            'timestamp': datetime.now().isoformat(),
            'version': self.config['experiment']['version'],
            'description': self.config['experiment']['description']
        }
        
        # ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ ì €ì¥
        self.results['model_performance'] = self.model_results
        
        # ëª¨ë¸ ì €ì¥
        models_dir = self.output_dir / 'models'
        for model_key, model in self.models.items():
            model_path = models_dir / f'{model_key.lower()}_model.joblib'
            joblib.dump(model, model_path)
            self.logger.info(f"{model_key} ëª¨ë¸ ì €ì¥: {model_path}")
        
        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        def convert_to_serializable(obj):
            if isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(v) for v in obj]
            elif isinstance(obj, (np.float32, np.float64)):
                return float(obj)
            elif isinstance(obj, (np.int32, np.int64)):
                return int(obj)
            else:
                return obj
        
        serializable_results = convert_to_serializable(self.results)
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.output_dir / 'results' / 'modeling_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ì „ì²´ ê²°ê³¼ ì €ì¥: {results_path}")
        
        # ì„¤ì • ì €ì¥
        config_path = self.output_dir / 'modeling_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        self.logger.info(f"ì„¤ì • ì €ì¥: {config_path}")
        
        # ìš”ì•½ í…Œì´ë¸” ìƒì„±
        self.create_summary_table()
    
    def create_summary_table(self):
        """ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        summary_data = []
        
        for model_key, result in self.model_results.items():
            model_name = result['model_name']
            data_type = result['data_type']
            
            summary_data.append({
                'Model': model_name,
                'Data_Type': data_type.upper(),
                'Optimal_Threshold': result.get('optimal_threshold', 0.5),
                'CV_AUC': result['cv_score'],
                'Val_AUC': result['val_metrics']['auc'],
                'Val_F1': result['val_metrics']['f1'],
                'Test_AUC': result['test_metrics']['auc'],
                'Test_Precision': result['test_metrics']['precision'],
                'Test_Recall': result['test_metrics']['recall'],
                'Test_F1': result['test_metrics']['f1'],
                'Test_Balanced_Acc': result['test_metrics'].get('balanced_accuracy', 0),
                'Average_Precision': result['test_metrics'].get('average_precision', 0)
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # ì €ì¥
        summary_path = self.output_dir / 'results' / 'summary_table.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        self.logger.info("ì‹¤í–‰ ê²°ê³¼ ìš”ì•½:")
        self.logger.info(f"\n{summary_df.round(4).to_string()}")
        self.logger.info(f"ìš”ì•½ í…Œì´ë¸” ì €ì¥: {summary_path}")
        
        return summary_df
    
    def run_pipeline(self) -> str:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("=== ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            self.load_data()
            
            # 2. ëª¨ë“  ëª¨ë¸ ì‹¤í–‰
            self.run_all_models()
            
            # 3. ê²°ê³¼ ì €ì¥
            self.save_results()
            
            self.logger.info("=== ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            
            return str(self.output_dir)
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸')
    parser.add_argument('--config', '-c', type=str, 
                       default='config/modeling_config.yaml',
                       help='Config íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = ModelingPipeline(args.config)
    experiment_dir = pipeline.run_pipeline()
    
    print(f"\nâœ… ëª¨ë¸ë§ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {experiment_dir}")


if __name__ == "__main__":
    main()