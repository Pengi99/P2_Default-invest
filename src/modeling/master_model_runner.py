"""
ë§ˆìŠ¤í„° ëª¨ë¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Normal/SMOTE ë°ì´í„°ì— ëŒ€í•´ 3ê°œ ëª¨ë¸(LogisticRegression, RandomForest, XGBoost)ì„ ìë™ ì‹¤í–‰
"""

import pandas as pd
import numpy as np
import json
import os
import warnings
from datetime import datetime
from pathlib import Path
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic' if plt.matplotlib.get_backend() != 'Agg' else 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score, balanced_accuracy_score
)
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
from imblearn.over_sampling import BorderlineSMOTE
from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, TomekLinks
from ensemble_model import EnsembleModel

class MasterModelRunner:
    def __init__(self, config):
        """
        ë§ˆìŠ¤í„° ëª¨ë¸ ëŸ¬ë„ˆ ì´ˆê¸°í™”
        
        Args:
            config (dict): ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"{config['run_name']}_{self.timestamp}"
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
        self.output_dir = Path(config['output_base_dir']) / self.run_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'results').mkdir(exist_ok=True)
        (self.output_dir / 'visualizations').mkdir(exist_ok=True)
        
        # ë°ì´í„° ì €ì¥ ë³€ìˆ˜
        self.data = {}
        self.models = {}
        self.results = {}
        self.selected_features = None
        
        print(f"ğŸš€ ë§ˆìŠ¤í„° ëª¨ë¸ ëŸ¬ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“ ì‹¤í–‰ ì´ë¦„: {self.run_name}")
        print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {self.output_dir}")
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ")
        print("="*60)
        
        data_path = Path(self.config['data_path'])
        
        # ì›ë³¸ ë°ì´í„° (SMOTEëŠ” ë™ì ìœ¼ë¡œ ì ìš©)
        # self.data['normal'] = {
        #     'X_train': pd.read_csv(data_path / 'X_train_100_normal.csv'),
        #     'X_valid': pd.read_csv(data_path / 'X_valid_100_normal.csv'),
        #     'X_test': pd.read_csv(data_path / 'X_test_100_normal.csv'),
        #     'y_train': pd.read_csv(data_path / 'y_train_100_normal.csv').iloc[:, 0],
        #     'y_valid': pd.read_csv(data_path / 'y_valid_100_normal.csv').iloc[:, 0],
        #     'y_test': pd.read_csv(data_path / 'y_test_100_normal.csv').iloc[:, 0]
        # }
        self.data['normal'] = {
            'X_train': pd.read_csv(data_path / 'X_train.csv'),
            'X_valid': pd.read_csv(data_path / 'X_val.csv'),
            'X_test': pd.read_csv(data_path / 'X_test.csv'),
            'y_train': pd.read_csv(data_path / 'y_train.csv').iloc[:, 0],
            'y_valid': pd.read_csv(data_path / 'y_val.csv').iloc[:, 0],
            'y_test': pd.read_csv(data_path / 'y_test.csv').iloc[:, 0]
        }
        
        # í™œì„±í™”ëœ ë°ì´í„° íƒ€ì…ë³„ë¡œ ë³µì‚¬ (ë™ì  ìƒ˜í”Œë§ ì ìš©)
        enabled_data_types = [dt for dt, config in self.config['data_types'].items() if config['enabled']]
        for data_type in enabled_data_types:
            if data_type != 'normal':
                self.data[data_type] = self.data['normal'].copy()
        
        # Normal ë°ì´í„° ì •ë³´ ì¶œë ¥
        data = self.data['normal']
        print(f"âœ… NORMAL ë°ì´í„°:")
        print(f"   Train: {data['X_train'].shape}, ë¶€ì‹¤ë¹„ìœ¨: {data['y_train'].mean():.2%}")
        print(f"   Valid: {data['X_valid'].shape}, ë¶€ì‹¤ë¹„ìœ¨: {data['y_valid'].mean():.2%}")
        print(f"   Test: {data['X_test'].shape}, ë¶€ì‹¤ë¹„ìœ¨: {data['y_test'].mean():.2%}")
        
        # í™œì„±í™”ëœ ë°ì´í„° íƒ€ì…ë³„ ì •ë³´ ì¶œë ¥
        for data_type in enabled_data_types:
            if data_type == 'normal':
                continue
            elif data_type == 'smote':
                config = self.config['data_types']['smote']
                print(f"âœ… SMOTE ë°ì´í„°:")
                print(f"   ì›ë³¸ê³¼ ë™ì¼í•œ í¬ê¸° (ë™ì  ì ìš©): {data['X_train'].shape}")
                print(f"   ğŸ”„ SMOTEëŠ” CV ë° ìµœì¢… í›ˆë ¨ ì‹œ ë™ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤")
                print(f"   ğŸ¯ ëª©í‘œ ë¶€ì‹¤ë¹„ìœ¨: {config['sampling_strategy']*100:.0f}% (BorderlineSMOTE)")
                print(f"   ğŸš« Data Leakage ë°©ì§€: CV ë‚´ë¶€ì—ì„œë§Œ ì ìš©")
            elif data_type == 'undersampling':
                config = self.config['data_types']['undersampling']
                print(f"âœ… UNDERSAMPLING ë°ì´í„°:")
                print(f"   ì›ë³¸ê³¼ ë™ì¼í•œ í¬ê¸° (ë™ì  ì ìš©): {data['X_train'].shape}")
                print(f"   ğŸ”„ ì–¸ë”ìƒ˜í”Œë§ì€ CV ë° ìµœì¢… í›ˆë ¨ ì‹œ ë™ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤")
                print(f"   ğŸ¯ ë°©ë²•: {config['method']} (sampling_strategy: {config['sampling_strategy']})")
                print(f"   ğŸš« Data Leakage ë°©ì§€: CV ë‚´ë¶€ì—ì„œë§Œ ì ìš©")
            elif data_type == 'combined':
                config = self.config['data_types']['combined']
                print(f"âœ… COMBINED ë°ì´í„°:")
                print(f"   ì›ë³¸ê³¼ ë™ì¼í•œ í¬ê¸° (ë™ì  ì ìš©): {data['X_train'].shape}")
                print(f"   ğŸ”„ SMOTE + ì–¸ë”ìƒ˜í”Œë§ ì¡°í•©ì´ ë™ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤")
                print(f"   ğŸ¯ SMOTE ë¹„ìœ¨: {config['smote_ratio']*100:.0f}%, ì–¸ë”ìƒ˜í”Œë§ ë¹„ìœ¨: {config['undersampling_ratio']*100:.0f}%")
                print(f"   ğŸš« Data Leakage ë°©ì§€: CV ë‚´ë¶€ì—ì„œë§Œ ì ìš©")
    
    def apply_sampling_strategy(self, X, y, data_type):
        """
        ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ìƒ˜í”Œë§ ì „ëµ ì ìš©
        
        Args:
            X: íŠ¹ì„± ë°ì´í„°
            y: íƒ€ê²Ÿ ë°ì´í„°
            data_type: ë°ì´í„° íƒ€ì… ('normal', 'smote', 'undersampling', 'combined')
            
        Returns:
            tuple: (X_resampled, y_resampled)
        """
        if data_type == 'normal':
            return X, y
        
        elif data_type == 'smote':
            config = self.config['data_types']['smote']
            smote = BorderlineSMOTE(
                sampling_strategy=config['sampling_strategy'],
                random_state=self.config['random_state'],
                k_neighbors=config['k_neighbors'],
                m_neighbors=config['m_neighbors']
            )
            return smote.fit_resample(X, y)
        
        elif data_type == 'undersampling':
            config = self.config['data_types']['undersampling']
            
            if config['method'] == 'random':
                undersampler = RandomUnderSampler(
                    sampling_strategy=config['sampling_strategy'],
                    random_state=config['random_state']
                )
            elif config['method'] == 'edited_nearest_neighbours':
                undersampler = EditedNearestNeighbours(
                    sampling_strategy=config['sampling_strategy']
                )
            elif config['method'] == 'tomek':
                undersampler = TomekLinks(
                    sampling_strategy=config['sampling_strategy']
                )
            else:
                print(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ë”ìƒ˜í”Œë§ ë°©ë²•: {config['method']}, Random ì‚¬ìš©")
                undersampler = RandomUnderSampler(
                    sampling_strategy=config['sampling_strategy'],
                    random_state=config['random_state']
                )
            
            return undersampler.fit_resample(X, y)
        
        elif data_type == 'combined':
            config = self.config['data_types']['combined']
            
            # 1ë‹¨ê³„: SMOTE ì ìš©
            smote = BorderlineSMOTE(
                sampling_strategy=config['smote_ratio'],
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            X_smote, y_smote = smote.fit_resample(X, y)
            
            # 2ë‹¨ê³„: ì–¸ë”ìƒ˜í”Œë§ ì ìš©
            undersampler = RandomUnderSampler(
                sampling_strategy=config['undersampling_ratio'],
                random_state=self.config['random_state']
            )
            X_combined, y_combined = undersampler.fit_resample(X_smote, y_smote)
            
            return X_combined, y_combined
        
        else:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° íƒ€ì…: {data_type}, ì›ë³¸ ë°ì´í„° ë°˜í™˜")
            return X, y

    def apply_lasso_feature_selection(self, data_type):
        """Lasso íŠ¹ì„± ì„ íƒ ì ìš©"""
        if not self.config['lasso']['enabled']:
            return
            
        print(f"\nğŸ” Lasso íŠ¹ì„± ì„ íƒ ì ìš© ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Lasso CV
        lasso_cv = LassoCV(
            alphas=self.config['lasso']['alphas'],
            cv=self.config['lasso']['cv_folds'],
            random_state=self.config['random_state'],
            n_jobs=-1
        )
        lasso_cv.fit(X_train_scaled, y_train)
        
        # íŠ¹ì„± ì„ íƒ
        threshold = self.config['lasso']['threshold']
        if threshold == 'median':
            threshold_value = np.median(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        elif threshold == 'mean':
            threshold_value = np.mean(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        else:
            threshold_value = float(threshold)
        
        selected_mask = np.abs(lasso_cv.coef_) >= threshold_value
        selected_features = X_train.columns[selected_mask].tolist()
        
        print(f"âœ… ìµœì  alpha: {lasso_cv.alpha_:.6f}")
        print(f"ğŸ“Š ì„ íƒëœ íŠ¹ì„±: {len(selected_features)}/{len(X_train.columns)}")
        print(f"ğŸ¯ ì„ íƒëœ íŠ¹ì„±: {selected_features}")
        
        # ë°ì´í„° ì—…ë°ì´íŠ¸
        for split in ['X_train', 'X_valid', 'X_test']:
            self.data[data_type][split] = self.data[data_type][split][selected_features]
        
        # ê²°ê³¼ ì €ì¥
        lasso_results = {
            'optimal_alpha': float(lasso_cv.alpha_),
            'threshold_value': float(threshold_value),
            'original_features': len(X_train.columns),
            'selected_features': len(selected_features),
            'selected_feature_names': selected_features,
            'feature_coefficients': dict(zip(X_train.columns, lasso_cv.coef_))
        }
        
        # results ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(parents=True, exist_ok=True)
        
        with open(results_dir / f'lasso_selection_{data_type}.json', 'w') as f:
            json.dump(lasso_results, f, indent=2, ensure_ascii=False)
    
    def optimize_logistic_regression(self, data_type):
        """ë¡œì§€ìŠ¤í‹± íšŒê·€ ìµœì í™”"""
        print(f"\nğŸ” ë¡œì§€ìŠ¤í‹± íšŒê·€ ìµœì í™” ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            # penaltyì™€ solverë¥¼ í•˜ë‚˜ì˜ ì¡°í•©ìœ¼ë¡œ ì„ íƒ (ë™ì  ì„ íƒì§€ ë¬¸ì œ í•´ê²°)
            penalty_solver_combinations = []
            
            # ê°€ëŠ¥í•œ ëª¨ë“  penalty-solver ì¡°í•© ìƒì„±
            for penalty in self.config['models']['logistic']['penalty']:
                if penalty == 'l1':
                    for solver in ['liblinear', 'saga']:
                        penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'l2':
                    for solver in self.config['models']['logistic']['l2_solvers']:
                        if solver in ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']:
                            penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'elasticnet':
                    penalty_solver_combinations.append(f"{penalty}_saga")
            
            # ì¡°í•© ì„ íƒ
            combination = trial.suggest_categorical('penalty_solver', penalty_solver_combinations)
            penalty, solver = combination.split('_', 1)
            
            C = trial.suggest_float('C', *self.config['models']['logistic']['C_range'], log=True)
            max_iter = trial.suggest_int('max_iter', *self.config['models']['logistic']['max_iter_range'])
            
            params = {
                'penalty': penalty,
                'C': C,
                'solver': solver,
                'max_iter': max_iter,
                'random_state': self.config['random_state']
            }
            
            if penalty == 'elasticnet':
                params['l1_ratio'] = trial.suggest_float('l1_ratio', *self.config['models']['logistic']['l1_ratio_range'])
            
            model = LogisticRegression(**params)
            
            # Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ CV (ìƒ˜í”Œë§ ë°ì´í„° íƒ€ì…ì¸ ê²½ìš°)
            if data_type != 'normal':
                # ì›ë³¸ ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§ ì ìš© ì „)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_sampling(model, X_train_original, y_train_original, data_type, cv_folds=5)
            else:
                # Normal ë°ì´í„°ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['logistic']['n_trials'])
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params.copy()
        
        # penalty_solver ì¡°í•©ì„ ë¶„ë¦¬
        penalty_solver = best_params.pop('penalty_solver')
        penalty, solver = penalty_solver.split('_', 1)
        
        # ë¶„ë¦¬ëœ penaltyì™€ solver ì¶”ê°€
        best_params['penalty'] = penalty
        best_params['solver'] = solver
        
        model = LogisticRegression(**best_params)
        
        # ìƒ˜í”Œë§ ë°ì´í„° íƒ€ì…ì¸ ê²½ìš° ìµœì¢… í›ˆë ¨ì—ë„ ìƒ˜í”Œë§ ì ìš©
        if data_type != 'normal':
            X_train_resampled, y_train_resampled = self.apply_sampling_strategy(X_train, y_train, data_type)
            model.fit(X_train_resampled, y_train_resampled)
            print(f"âœ… {data_type.upper()} ì ìš© í›„ í›ˆë ¨: {len(X_train_resampled):,}ê°œ ìƒ˜í”Œ")
            print(f"   ë¶€ì‹¤ë¹„ìœ¨: {y_train_resampled.mean():.2%}")
        else:
            model.fit(X_train, y_train)
        
        # ì €ì¥
        model_key = f'LogisticRegression_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type
        }
        
        print(f"âœ… ìµœì  AUC: {study.best_value:.4f}")
        print(f"ğŸ“Š ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return model
    
    def optimize_random_forest(self, data_type):
        """ëœë¤ í¬ë ˆìŠ¤íŠ¸ ìµœì í™”"""
        print(f"\nğŸŒ² ëœë¤ í¬ë ˆìŠ¤íŠ¸ ìµœì í™” ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *self.config['models']['random_forest']['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *self.config['models']['random_forest']['max_depth_range']),
                'min_samples_split': trial.suggest_int('min_samples_split', *self.config['models']['random_forest']['min_samples_split_range']),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', *self.config['models']['random_forest']['min_samples_leaf_range']),
                'max_features': trial.suggest_float('max_features', *self.config['models']['random_forest']['max_features_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1
            }
            
            model = RandomForestClassifier(**params)
            
            # Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ CV (ìƒ˜í”Œë§ ë°ì´í„° íƒ€ì…ì¸ ê²½ìš°)
            if data_type != 'normal':
                # ì›ë³¸ ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§ ì ìš© ì „)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_sampling(model, X_train_original, y_train_original, data_type, cv_folds=5)
            else:
                # Normal ë°ì´í„°ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['random_forest']['n_trials'])
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params
        model = RandomForestClassifier(**best_params)
        
        # ìƒ˜í”Œë§ ë°ì´í„° íƒ€ì…ì¸ ê²½ìš° ìµœì¢… í›ˆë ¨ì—ë„ ìƒ˜í”Œë§ ì ìš©
        if data_type != 'normal':
            X_train_resampled, y_train_resampled = self.apply_sampling_strategy(X_train, y_train, data_type)
            model.fit(X_train_resampled, y_train_resampled)
            print(f"âœ… {data_type.upper()} ì ìš© í›„ í›ˆë ¨: {len(X_train_resampled):,}ê°œ ìƒ˜í”Œ")
            print(f"   ë¶€ì‹¤ë¹„ìœ¨: {y_train_resampled.mean():.2%}")
        else:
            model.fit(X_train, y_train)
        
        # ì €ì¥
        model_key = f'RandomForest_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type,
            'feature_importances': dict(zip(X_train.columns, model.feature_importances_))
        }
        
        print(f"âœ… ìµœì  AUC: {study.best_value:.4f}")
        print(f"ğŸ“Š ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return model
    
    def optimize_xgboost(self, data_type):
        """XGBoost ìµœì í™”"""
        print(f"\nğŸš€ XGBoost ìµœì í™” ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'n_estimators': trial.suggest_int('n_estimators', *self.config['models']['xgboost']['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *self.config['models']['xgboost']['max_depth_range']),
                'learning_rate': trial.suggest_float('learning_rate', *self.config['models']['xgboost']['learning_rate_range']),
                'subsample': trial.suggest_float('subsample', *self.config['models']['xgboost']['subsample_range']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *self.config['models']['xgboost']['colsample_bytree_range']),
                'reg_alpha': trial.suggest_float('reg_alpha', *self.config['models']['xgboost']['reg_alpha_range']),
                'reg_lambda': trial.suggest_float('reg_lambda', *self.config['models']['xgboost']['reg_lambda_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1,
                'verbosity': 0
            }
            
            model = xgb.XGBClassifier(**params)
            
            # Data Leakage ë°©ì§€ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ CV (ìƒ˜í”Œë§ ë°ì´í„° íƒ€ì…ì¸ ê²½ìš°)
            if data_type != 'normal':
                # ì›ë³¸ ë°ì´í„° ë¡œë“œ (ìƒ˜í”Œë§ ì ìš© ì „)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_sampling(model, X_train_original, y_train_original, data_type, cv_folds=5)
            else:
                # Normal ë°ì´í„°ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['xgboost']['n_trials'])
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params
        model = xgb.XGBClassifier(**best_params)
        
        # ìƒ˜í”Œë§ ë°ì´í„° íƒ€ì…ì¸ ê²½ìš° ìµœì¢… í›ˆë ¨ì—ë„ ìƒ˜í”Œë§ ì ìš©
        if data_type != 'normal':
            X_train_resampled, y_train_resampled = self.apply_sampling_strategy(X_train, y_train, data_type)
            model.fit(X_train_resampled, y_train_resampled)
            print(f"âœ… {data_type.upper()} ì ìš© í›„ í›ˆë ¨: {len(X_train_resampled):,}ê°œ ìƒ˜í”Œ")
            print(f"   ë¶€ì‹¤ë¹„ìœ¨: {y_train_resampled.mean():.2%}")
        else:
            model.fit(X_train, y_train)
        
        # ì €ì¥
        model_key = f'XGBoost_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type,
            'feature_importances': dict(zip(X_train.columns, model.feature_importances_))
        }
        
        print(f"âœ… ìµœì  AUC: {study.best_value:.4f}")
        print(f"ğŸ“Š ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        
        return model
    
    def find_optimal_threshold(self, model_key):
        """ê° ëª¨ë¸ë³„ ìµœì  threshold ì°¾ê¸°"""
        print(f"\nğŸ¯ {model_key} ìµœì  Threshold íƒìƒ‰")
        print("="*60)
        
        model = self.models[model_key]
        data_type = self.results[model_key]['data_type']
        
        X_valid = self.data[data_type]['X_valid']
        y_valid = self.data[data_type]['y_valid']
        
        # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
        
        # Precision-Recall ê³¡ì„ ê³¼ ë‹¤ì–‘í•œ thresholdì—ì„œì˜ ì„±ëŠ¥ ê³„ì‚°
        thresholds = np.arange(0.05, 0.5, 0.05)  # 0.05ë¶€í„° 0.5ê¹Œì§€ 0.05 ê°„ê²©
        
        threshold_results = []
        
        for threshold in thresholds:
            y_valid_pred = (y_valid_proba >= threshold).astype(int)
            
            # ì˜ˆì¸¡ê°’ì´ ëª¨ë‘ 0ì´ê±°ë‚˜ 1ì¸ ê²½ìš° ìŠ¤í‚µ
            if len(np.unique(y_valid_pred)) == 1:
                continue
            
            try:
                metrics = {
                    'threshold': threshold,
                    'precision': precision_score(y_valid, y_valid_pred, zero_division=0),
                    'recall': recall_score(y_valid, y_valid_pred, zero_division=0),
                    'f1': f1_score(y_valid, y_valid_pred, zero_division=0),
                    'balanced_accuracy': balanced_accuracy_score(y_valid, y_valid_pred)
                }
                threshold_results.append(metrics)
            except:
                continue
        
        if not threshold_results:
            print("âš ï¸ ìµœì  threshold ì°¾ê¸° ì‹¤íŒ¨, ê¸°ë³¸ê°’ 0.5 ì‚¬ìš©")
            return 0.5, {}
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        threshold_df = pd.DataFrame(threshold_results)
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ë³„ ìµœì  threshold ì°¾ê¸°
        metric_priority = self.config.get('threshold_optimization', {}).get('metric_priority', 'f1')
        
        optimal_thresholds = {}
        for metric in ['f1', 'precision', 'recall', 'balanced_accuracy']:
            if metric in threshold_df.columns:
                best_idx = threshold_df[metric].idxmax()
                optimal_thresholds[metric] = {
                    'threshold': threshold_df.loc[best_idx, 'threshold'],
                    'value': threshold_df.loc[best_idx, metric]
                }
        
        # ìš°ì„ ìˆœìœ„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ìµœì¢… threshold ì„ íƒ
        if metric_priority in optimal_thresholds:
            final_threshold = optimal_thresholds[metric_priority]['threshold']
            final_value = optimal_thresholds[metric_priority]['value']
        else:
            final_threshold = optimal_thresholds['f1']['threshold']
            final_value = optimal_thresholds['f1']['value']
        
        print(f"ğŸ“ˆ Threshold ìµœì í™” ê²°ê³¼:")
        for metric, result in optimal_thresholds.items():
            marker = "ğŸ¯" if metric == metric_priority else "  "
            print(f"{marker} {metric.upper()}: {result['threshold']:.3f} (ê°’: {result['value']:.4f})")
        
        print(f"\nâœ… ìµœì¢… ì„ íƒ: {final_threshold:.3f} ({metric_priority.upper()}: {final_value:.4f})")
        
        # Precision-Recall ê³¡ì„  ë°ì´í„° ì €ì¥
        precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_valid, y_valid_proba)
        
        threshold_analysis = {
            'all_thresholds': threshold_results,
            'optimal_by_metric': optimal_thresholds,
            'final_threshold': final_threshold,
            'final_metric': metric_priority,
            'final_value': final_value,
            'pr_curve': {
                'precision': precision_vals.tolist(),
                'recall': recall_vals.tolist(),
                'thresholds': pr_thresholds.tolist()
            }
        }
        
        return final_threshold, threshold_analysis
    
    def evaluate_model(self, model_key):
        """ëª¨ë¸ í‰ê°€ (ìµœì  threshold ì‚¬ìš©)"""
        # ìµœì  threshold ì°¾ê¸°
        optimal_threshold, threshold_analysis = self.find_optimal_threshold(model_key)
        
        model = self.models[model_key]
        data_type = self.results[model_key]['data_type']
        
        X_valid = self.data[data_type]['X_valid']
        y_valid = self.data[data_type]['y_valid']
        X_test = self.data[data_type]['X_test']
        y_test = self.data[data_type]['y_test']
        
        # ì˜ˆì¸¡ (ìµœì  threshold ì‚¬ìš©)
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
        y_valid_pred = (y_valid_proba >= optimal_threshold).astype(int)
        
        y_test_proba = model.predict_proba(X_test)[:, 1]
        y_test_pred = (y_test_proba >= optimal_threshold).astype(int)
        
        # ê²€ì¦ ì„±ëŠ¥
        valid_metrics = {
            'auc': roc_auc_score(y_valid, y_valid_proba),
            'precision': precision_score(y_valid, y_valid_pred, zero_division=0),
            'recall': recall_score(y_valid, y_valid_pred, zero_division=0),
            'f1': f1_score(y_valid, y_valid_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_valid, y_valid_pred),
            'average_precision': average_precision_score(y_valid, y_valid_proba)
        }
        
        # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
        test_metrics = {
            'auc': roc_auc_score(y_test, y_test_proba),
            'precision': precision_score(y_test, y_test_pred, zero_division=0),
            'recall': recall_score(y_test, y_test_pred, zero_division=0),
            'f1': f1_score(y_test, y_test_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),
            'average_precision': average_precision_score(y_test, y_test_proba)
        }
        
        # ê²°ê³¼ ì—…ë°ì´íŠ¸
        self.results[model_key].update({
            'optimal_threshold': optimal_threshold,
            'threshold_analysis': threshold_analysis,
            'valid_metrics': valid_metrics,
            'test_metrics': test_metrics,
            'predictions': {
                'y_valid_proba': y_valid_proba.tolist(),
                'y_test_proba': y_test_proba.tolist()
            }
        })
        
        print(f"\nğŸ“Š {model_key} ìµœì¢… í‰ê°€ (Threshold: {optimal_threshold:.3f}):")
        print(f"   ê²€ì¦ - AUC: {valid_metrics['auc']:.4f}, F1: {valid_metrics['f1']:.4f}, Precision: {valid_metrics['precision']:.4f}, Recall: {valid_metrics['recall']:.4f}")
        print(f"   í…ŒìŠ¤íŠ¸ - AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1']:.4f}, Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}")
    
    def run_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ì‹¤í–‰"""
        print("\nğŸš€ ëª¨ë“  ëª¨ë¸ ì‹¤í–‰ ì‹œì‘")
        print("="*80)
        
        # í™œì„±í™”ëœ ë°ì´í„° íƒ€ì… í™•ì¸
        enabled_data_types = [dt for dt, config in self.config['data_types'].items() if config['enabled']]
        print(f"ğŸ¯ í™œì„±í™”ëœ ë°ì´í„° íƒ€ì…: {enabled_data_types}")
        
        # Lasso íŠ¹ì„± ì„ íƒ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        if self.config['lasso']['enabled']:
            self.apply_lasso_feature_selection('normal')  # normal ë°ì´í„°ë¡œ íŠ¹ì„± ì„ íƒ
            
            # ì„ íƒëœ íŠ¹ì„±ì„ ë‹¤ë¥¸ ë°ì´í„° íƒ€ì…ì—ë„ ì ìš©
            selected_features = self.data['normal']['X_train'].columns.tolist()
            self.selected_features = selected_features
            
            # ë‹¤ë¥¸ ë°ì´í„° íƒ€ì…ì—ë„ ë™ì¼í•œ íŠ¹ì„± ì ìš©
            for data_type in enabled_data_types:
                if data_type != 'normal':
                    for split in ['X_train', 'X_valid', 'X_test']:
                        self.data[data_type][split] = self.data[data_type][split][selected_features]
        
        # í™œì„±í™”ëœ ë°ì´í„° íƒ€ì…ë³„ë¡œ ì‹¤í–‰
        for data_type in enabled_data_types:
            print(f"\nğŸ“Š {data_type.upper()} ë°ì´í„° ì²˜ë¦¬")
            print("="*60)
            
            # ëª¨ë¸ë³„ ìµœì í™”
            models_to_run = []
            if self.config['models']['logistic']['enabled']:
                models_to_run.append(('logistic', self.optimize_logistic_regression))
            if self.config['models']['random_forest']['enabled']:
                models_to_run.append(('random_forest', self.optimize_random_forest))
            if self.config['models']['xgboost']['enabled']:
                models_to_run.append(('xgboost', self.optimize_xgboost))
            
            for model_name, optimize_func in models_to_run:
                optimize_func(data_type)
        
        # ëª¨ë“  ëª¨ë¸ í‰ê°€ (ìµœì  threshold ìë™ íƒìƒ‰)
        print(f"\nğŸ“Š ëª¨ë“  ëª¨ë¸ í‰ê°€ ë° Threshold ìµœì í™”")
        print("="*60)
        
        for model_key in self.models.keys():
            self.evaluate_model(model_key)
        
        # ì•™ìƒë¸” ëª¨ë¸ ì‹¤í–‰
        if self.config.get('ensemble', {}).get('enabled', False):
            self.run_ensemble_model()
    
    def run_ensemble_model(self):
        """ì•™ìƒë¸” ëª¨ë¸ ì‹¤í–‰"""
        print(f"\nğŸ­ ì•™ìƒë¸” ëª¨ë¸ ì‹¤í–‰")
        print("="*60)
        
        ensemble_config = self.config['ensemble']
        
        # ì•™ìƒë¸”ì— í¬í•¨í•  ëª¨ë¸ë“¤ í•„í„°ë§
        ensemble_models = {}
        enabled_models = ensemble_config.get('models', [])
        enabled_data_types = ensemble_config.get('data_types', ['normal', 'smote'])
        
        # ëª¨ë¸ ì´ë¦„ ë§¤í•‘ (ì„¤ì •ëª… -> ì‹¤ì œ ëª¨ë¸ í‚¤ëª…)
        model_name_mapping = {
            'logistic': 'LogisticRegression',
            'random_forest': 'RandomForest', 
            'xgboost': 'XGBoost'
        }
        
        print(f"ğŸ” í˜„ì¬ í›ˆë ¨ëœ ëª¨ë¸ë“¤: {list(self.models.keys())}")
        print(f"ğŸ¯ ì•™ìƒë¸” ì„¤ì • - ëª¨ë¸: {enabled_models}, ë°ì´í„° íƒ€ì…: {enabled_data_types}")
        
        for model_key, model_obj in self.models.items():
            # ëª¨ë¸ í‚¤ì—ì„œ ì •ë³´ ì¶”ì¶œ (ì˜ˆ: LogisticRegression_normal)
            model_parts = model_key.split('_')
            if len(model_parts) >= 2:
                model_name = model_parts[0]  # LogisticRegression, RandomForest, XGBoost
                data_type = model_parts[1].lower()  # normal, smote
            else:
                continue  # ì˜¬ë°”ë¥´ì§€ ì•Šì€ í‚¤ í˜•ì‹ì€ ê±´ë„ˆë›°ê¸°
            
            # ì„¤ì •ì˜ ëª¨ë¸ëª…ì„ ì‹¤ì œ ëª¨ë¸ëª…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
            enabled_model_names = [model_name_mapping.get(em, em) for em in enabled_models]
            
            # ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ
            if model_name in enabled_model_names and data_type in enabled_data_types:
                ensemble_models[model_key] = model_obj
                print(f"âœ… ì•™ìƒë¸”ì— í¬í•¨: {model_key} (ëª¨ë¸: {model_name}, ë°ì´í„°: {data_type})")
        
        if not ensemble_models:
            print("âš ï¸ ì•™ìƒë¸”ì— í¬í•¨í•  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ğŸ’¡ ë””ë²„ê¹… ì •ë³´:")
            print(f"   - ì„¤ì •ëœ ëª¨ë¸: {enabled_models}")
            print(f"   - ë§¤í•‘ëœ ëª¨ë¸ëª…: {[model_name_mapping.get(em, em) for em in enabled_models]}")
            print(f"   - ì„¤ì •ëœ ë°ì´í„° íƒ€ì…: {enabled_data_types}")
            print(f"   - ì‹¤ì œ ëª¨ë¸ í‚¤ë“¤: {list(self.models.keys())}")
            return
        
        # ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
        ensemble = EnsembleModel(self.config, ensemble_models)
        
        # ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° (normal ë°ì´í„° ì‚¬ìš©)
        X_valid = self.data['normal']['X_valid']
        y_valid = self.data['normal']['y_valid']
        X_test = self.data['normal']['X_test']
        y_test = self.data['normal']['y_test']
        
        print(f"\nğŸ¯ ì•™ìƒë¸” ì˜ˆì¸¡ ìˆ˜í–‰")
        print("="*40)
        
        # ê²€ì¦ ë°ì´í„°ë¡œ ì•™ìƒë¸” ì˜ˆì¸¡ (ìë™ ê°€ì¤‘ì¹˜ ê³„ì‚° í¬í•¨)
        ensemble_valid_proba = ensemble.ensemble_predict_proba(
            X_valid, X_valid, y_valid
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
        ensemble_test_proba = ensemble.ensemble_predict_proba(X_test)
        
        # ìµœì  threshold ì°¾ê¸°
        if ensemble_config.get('threshold_optimization', {}).get('enabled', True):
            metric = ensemble_config.get('threshold_optimization', {}).get('metric_priority', 'f1')
            optimal_threshold, threshold_metrics = ensemble.find_optimal_threshold(
                X_valid, y_valid, metric=metric
            )
        else:
            optimal_threshold = 0.5
            threshold_metrics = {}
        
        # ìµœì¢… ì„±ëŠ¥ í‰ê°€
        ensemble_metrics = ensemble.evaluate_ensemble(X_test, y_test, optimal_threshold)
        
        # ì•™ìƒë¸” ëª¨ë¸ ì €ì¥
        ensemble_key = 'ensemble_model'
        self.models[ensemble_key] = ensemble
        
        # ê²°ê³¼ ì €ì¥
        self.results[ensemble_key] = {
            'model_type': 'ensemble',
            'data_type': 'mixed',
            'method': ensemble_config.get('method', 'weighted_average'),
            'auto_weight': ensemble_config.get('auto_weight', False),
            'included_models': list(ensemble_models.keys()),
            'weights': ensemble.weights,
            'optimal_threshold': optimal_threshold,
            'threshold_metrics': threshold_metrics,
            'cv_score': np.mean([self.results[mk]['cv_score'] for mk in ensemble_models.keys()]),
            'valid_metrics': {
                'auc': roc_auc_score(y_valid, ensemble_valid_proba),
                'precision': precision_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'recall': recall_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'f1': f1_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'balanced_accuracy': balanced_accuracy_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int)),
                'average_precision': average_precision_score(y_valid, ensemble_valid_proba)
            },
            'test_metrics': ensemble_metrics,
            'predictions': {
                'y_valid_proba': ensemble_valid_proba.tolist(),
                'y_test_proba': ensemble_test_proba.tolist()
            }
        }
        
        print(f"\nğŸ† ì•™ìƒë¸” ëª¨ë¸ ìµœì¢… ì„±ëŠ¥:")
        print(f"   ë°©ë²•: {ensemble_config.get('method', 'weighted_average')}")
        print(f"   í¬í•¨ ëª¨ë¸: {len(ensemble_models)}ê°œ")
        print(f"   ìµœì  Threshold: {optimal_threshold:.3f}")
        print(f"   í…ŒìŠ¤íŠ¸ AUC: {ensemble_metrics['auc']:.4f}")
        print(f"   í…ŒìŠ¤íŠ¸ F1: {ensemble_metrics['f1']:.4f}")
        print(f"   í…ŒìŠ¤íŠ¸ Precision: {ensemble_metrics['precision']:.4f}")
        print(f"   í…ŒìŠ¤íŠ¸ Recall: {ensemble_metrics['recall']:.4f}")
        
        # ì•™ìƒë¸” ì‹œê°í™” ìƒì„±
        viz_dir = self.output_dir / 'visualizations'
        ensemble.create_ensemble_report(viz_dir)
    
    def save_all_results(self):
        """ëª¨ë“  ê²°ê³¼ ì €ì¥"""
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥")
        print("="*60)
        
        # ëª¨ë¸ ì €ì¥
        models_dir = self.output_dir / 'models'
        models_dir.mkdir(exist_ok=True)
        
        for model_key, model in self.models.items():
            model_path = models_dir / f'{model_key.lower()}_model.joblib'
            joblib.dump(model, model_path)
            print(f"âœ… {model_key} ëª¨ë¸ ì €ì¥: {model_path}")
        
        # ê²°ê³¼ ì €ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜)
        def convert_to_serializable(obj):
            """JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
            if isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(v) for v in obj]
            elif isinstance(obj, (np.float32, np.float64)):
                return float(obj)
            elif isinstance(obj, (np.int32, np.int64)):
                return int(obj)
            else:
                return obj
        
        serializable_results = convert_to_serializable(self.results)
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(exist_ok=True)
        
        results_path = results_dir / 'all_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì „ì²´ ê²°ê³¼ ì €ì¥: {results_path}")
        
        # ì„¤ì • ì €ì¥
        config_path = self.output_dir / 'config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì„¤ì • ì €ì¥: {config_path}")
        
        # ìš”ì•½ í…Œì´ë¸” ìƒì„±
        self.create_summary_table()
        
        # ì‹œê°í™” ìƒì„±
        self.create_visualizations()
    
    def create_summary_table(self):
        """ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        summary_data = []
        
        for model_key, result in self.results.items():
            model_name = model_key.split('_')[0]
            data_type = result['data_type']
            
            summary_data.append({
                'Model': model_name,
                'Data_Type': data_type.upper(),
                'Optimal_Threshold': result.get('optimal_threshold', 0.5),
                'CV_AUC': result['cv_score'],
                'Valid_AUC': result['valid_metrics']['auc'],
                'Valid_F1': result['valid_metrics']['f1'],
                'Test_AUC': result['test_metrics']['auc'],
                'Test_Precision': result['test_metrics']['precision'],
                'Test_Recall': result['test_metrics']['recall'],
                'Test_F1': result['test_metrics']['f1'],
                'Test_Balanced_Acc': result['test_metrics'].get('balanced_accuracy', 0),
                'Average_Precision': result['test_metrics'].get('average_precision', 0)
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # ì €ì¥
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(exist_ok=True)
        summary_path = results_dir / 'summary_table.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ† ì‹¤í–‰ ê²°ê³¼ ìš”ì•½:")
        print(summary_df.round(4))
        print(f"âœ… ìš”ì•½ í…Œì´ë¸” ì €ì¥: {summary_path}")
        
        return summary_df
    
    def create_visualizations(self):
        """ëª¨ë“  ì‹œê°í™” ìƒì„±"""
        print(f"\nğŸ“ˆ ì‹œê°í™” ìƒì„±")
        print("="*60)
        
        viz_dir = self.output_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        # ìš”ì•½ í…Œì´ë¸” ìƒì„± (ì‹œê°í™”ì—ì„œ ì‚¬ìš©)
        summary_df = self.create_summary_table()
        
        # 1. ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸
        self.plot_performance_comparison(summary_df, viz_dir)
        
        # 2. ROC ê³¡ì„  ë¹„êµ
        self.plot_roc_curves(viz_dir)
        
        # 3. íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ (RF, XGBoost)
        self.plot_feature_importance_comparison(viz_dir)
        
        # 4. Normal vs SMOTE ë¹„êµ
        self.plot_normal_vs_smote_comparison(summary_df, viz_dir)
        
        # 5. CV vs Test ì„±ëŠ¥ ë¹„êµ
        self.plot_cv_vs_test_comparison(summary_df, viz_dir)
        
        # 6. Threshold ìµœì í™” ê²°ê³¼ ì‹œê°í™”
        self.plot_threshold_optimization(viz_dir)
        
        # 7. Precision-Recall ê³¡ì„ 
        self.plot_precision_recall_curves(viz_dir)
        
        print(f"âœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ: {viz_dir}")
    
    def plot_performance_comparison(self, summary_df, viz_dir):
        """ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸"""
        print("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì°¨íŠ¸ ìƒì„±...")
        
        # ì§€í‘œë³„ ë¹„êµ
        metrics = ['CV_AUC', 'Test_AUC', 'Test_Precision', 'Test_Recall', 'Test_F1']
        metric_names = ['CV AUC', 'Test AUC', 'Test Precision', 'Test Recall', 'Test F1']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ë¹„êµ (ì•™ìƒë¸” í¬í•¨)', fontsize=16, fontweight='bold')
        
        # ìƒ‰ìƒ ì„¤ì •
        colors = {'NORMAL': 'skyblue', 'SMOTE': 'lightcoral', 'MIXED': 'gold', 'ENSEMBLE': 'purple'}
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            row, col = i // 3, i % 3
            ax = axes[row, col]
            
            # ì•™ìƒë¸”ê³¼ ì¼ë°˜ ëª¨ë¸ ë¶„ë¦¬
            ensemble_data = summary_df[summary_df['Model'] == 'ensemble']
            regular_data = summary_df[summary_df['Model'] != 'ensemble']
            
            if not regular_data.empty:
                # ì¼ë°˜ ëª¨ë¸ë“¤ - ê¸°ì¡´ ë°©ì‹
                pivot_data = regular_data.pivot(index='Model', columns='Data_Type', values=metric)
                
                # ë°” ì°¨íŠ¸
                x = np.arange(len(pivot_data.index))
                width = 0.35
                
                if 'NORMAL' in pivot_data.columns:
                    bars1 = ax.bar(x - width/2, pivot_data['NORMAL'], width, 
                                  label='Normal', alpha=0.8, color=colors['NORMAL'])
                if 'SMOTE' in pivot_data.columns:
                    bars2 = ax.bar(x + width/2, pivot_data['SMOTE'], width, 
                                  label='SMOTE', alpha=0.8, color=colors['SMOTE'])
                
                # ê°’ í‘œì‹œ
                container_idx = 0
                for col_name in pivot_data.columns:
                    if col_name in ['NORMAL', 'SMOTE']:
                        if container_idx < len(ax.containers):
                            bars = ax.containers[container_idx]
                            for bar in bars:
                                height = bar.get_height()
                                if not np.isnan(height):
                                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)
                            container_idx += 1
                
                ax.set_xticks(x)
                ax.set_xticklabels(pivot_data.index, rotation=45)
                
                # ì•™ìƒë¸” ì¶”ê°€
                if not ensemble_data.empty:
                    ensemble_x = len(pivot_data.index)
                    ensemble_value = ensemble_data[metric].iloc[0]
                    bars3 = ax.bar(ensemble_x, ensemble_value, width*2, 
                                  label='Ensemble', alpha=0.9, color=colors['ENSEMBLE'])
                    
                    # ì•™ìƒë¸” ê°’ í‘œì‹œ
                    ax.text(ensemble_x, ensemble_value + 0.01,
                           f'{ensemble_value:.3f}', ha='center', va='bottom', fontsize=9)
                    
                    # xì¶• ë¼ë²¨ ì—…ë°ì´íŠ¸
                    all_labels = list(pivot_data.index) + ['Ensemble']
                    ax.set_xticks(list(range(len(all_labels))))
                    ax.set_xticklabels(all_labels, rotation=45)
            
            elif not ensemble_data.empty:
                # ì•™ìƒë¸”ë§Œ ìˆëŠ” ê²½ìš°
                ensemble_value = ensemble_data[metric].iloc[0]
                ax.bar(0, ensemble_value, width=0.5, 
                      label='Ensemble', alpha=0.9, color=colors['ENSEMBLE'])
                ax.text(0, ensemble_value + 0.01,
                       f'{ensemble_value:.3f}', ha='center', va='bottom', fontsize=9)
                ax.set_xticks([0])
                ax.set_xticklabels(['Ensemble'])
            
            ax.set_title(f'{name}', fontsize=12, fontweight='bold')
            ax.set_ylabel(name)
            ax.set_xlabel('ëª¨ë¸')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1.1 if metric != 'Test_F1' else max(summary_df[metric].max() * 1.2, 0.5))
        
        # ë§ˆì§€ë§‰ subplot ì œê±°
        axes[1, 2].remove()
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì €ì¥: performance_comparison.png")
    
    def plot_roc_curves(self, viz_dir):
        """ROC ê³¡ì„  ë¹„êµ"""
        print("ğŸ“ˆ ROC ê³¡ì„  ìƒì„±...")
        
        from sklearn.metrics import roc_curve, auc
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íƒ€ì…ë§Œ ê°€ì ¸ì˜¤ê¸°
        available_data_types = list(self.data.keys())
        if not available_data_types:
            print("  âš ï¸ ROC ê³¡ì„  ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì—†ìŒ")
            return
            
        # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í‘œì‹œ (subplot êµ¬ì¡°ìƒ)
        data_types = available_data_types[:2]
        titles = [f'{dt.upper()} ë°ì´í„°' for dt in data_types]
        colors = ['blue', 'red', 'green']
        
        fig, axes = plt.subplots(1, len(data_types), figsize=(8*len(data_types), 6))
        if len(data_types) == 1:
            axes = [axes]  # ë‹¨ì¼ subplotì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        fig.suptitle('ROC ê³¡ì„  ë¹„êµ', fontsize=16, fontweight='bold')
        
        for idx, (data_type, title) in enumerate(zip(data_types, titles)):
            ax = axes[idx]
            
            X_test = self.data[data_type]['X_test']
            y_test = self.data[data_type]['y_test']
            
            model_names = ['LogisticRegression', 'RandomForest', 'XGBoost']
            
            for i, model_name in enumerate(model_names):
                model_key = f'{model_name}_{data_type}'
                if model_key in self.models:
                    model = self.models[model_key]
                    y_proba = model.predict_proba(X_test)[:, 1]
                    
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    roc_auc = auc(fpr, tpr)
                    
                    ax.plot(fpr, tpr, color=colors[i], lw=2, 
                           label=f'{model_name} (AUC = {roc_auc:.4f})')
            
            # ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°)
            ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', 
                   label='Random (AUC = 0.5000)')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(title)
            ax.legend(loc="lower right")
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'roc_curves_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… ROC ê³¡ì„  ì €ì¥: roc_curves_comparison.png")
    
    def plot_feature_importance_comparison(self, viz_dir):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ (RF, XGBoostë§Œ)"""
        print("ğŸ” íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ ìƒì„±...")
        
        tree_models = ['RandomForest', 'XGBoost']
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íƒ€ì…ë§Œ ê°€ì ¸ì˜¤ê¸°
        available_data_types = list(self.data.keys())[:2]  # ìµœëŒ€ 2ê°œ
        
        if not available_data_types:
            print("  âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„° ì—†ìŒ")
            return
        
        fig, axes = plt.subplots(2, len(available_data_types), figsize=(8*len(available_data_types), 12))
        if len(available_data_types) == 1:
            axes = axes.reshape(-1, 1)  # 2D ë°°ì—´ë¡œ ìœ ì§€
        fig.suptitle('íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ (Tree-based ëª¨ë¸)', fontsize=16, fontweight='bold')
        
        for i, model_name in enumerate(tree_models):
            for j, data_type in enumerate(available_data_types):
                ax = axes[i, j]
                model_key = f'{model_name}_{data_type}'
                
                if model_key in self.results and 'feature_importances' in self.results[model_key]:
                    importances = self.results[model_key]['feature_importances']
                    
                    # ìƒìœ„ 10ê°œ íŠ¹ì„±
                    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]
                    features, values = zip(*sorted_features)
                    
                    # ìƒ‰ìƒ ì„¤ì •
                    color = 'green' if model_name == 'RandomForest' else 'purple'
                    
                    bars = ax.barh(features, values, color=color, alpha=0.7)
                    
                    # ê°’ í‘œì‹œ
                    for k, v in enumerate(values):
                        ax.text(v + 0.001, k, f'{v:.3f}', va='center', fontsize=9)
                    
                    ax.set_title(f'{model_name} - {data_type.upper()}\níŠ¹ì„± ì¤‘ìš”ë„ (Top 10)', 
                               fontsize=12, fontweight='bold')
                    ax.set_xlabel('ì¤‘ìš”ë„')
                    ax.grid(True, alpha=0.3)
                else:
                    ax.text(0.5, 0.5, 'No data available', ha='center', va='center', 
                           transform=ax.transAxes, fontsize=12)
                    ax.set_title(f'{model_name} - {data_type.upper()}')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'feature_importance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ ì €ì¥: feature_importance_comparison.png")
    
    def plot_normal_vs_smote_comparison(self, summary_df, viz_dir):
        """Normal vs SMOTE vs Undersampling ìƒì„¸ ë¹„êµ"""
        print("âš–ï¸ ìƒ˜í”Œë§ ì „ëµë³„ ë¹„êµ ìƒì„±...")
        
        # ensemble ëª¨ë¸ ì œì™¸ (MIXED ë°ì´í„° íƒ€ì…)
        allowed_types = ['NORMAL', 'SMOTE', 'UNDERSAMPLING', 'COMBINED']
        df_filtered = summary_df[summary_df['Data_Type'].isin(allowed_types)]
        
        if len(df_filtered) == 0:
            print("  âš ï¸ ìƒ˜í”Œë§ ì „ëµë³„ ë¹„êµí•  ë°ì´í„° ì—†ìŒ")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(18, 12))
        fig.suptitle('ìƒ˜í”Œë§ ì „ëµë³„ ë°ì´í„° ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
        
        models = df_filtered['Model'].unique()
        metrics = ['Test_AUC', 'Test_F1', 'Test_Precision', 'Test_Recall']
        metric_names = ['Test AUC', 'Test F1', 'Test Precision', 'Test Recall']
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i//2, i%2]
            
            data_type_values = {dt: [] for dt in allowed_types}
            model_labels = []
            
            for model in models:
                model_has_data = False
                model_values = {}
                
                # ê° ë°ì´í„° íƒ€ì…ë³„ ê²°ê³¼ í™•ì¸
                for data_type in allowed_types:
                    mask = (df_filtered['Model'] == model) & (df_filtered['Data_Type'] == data_type)
                    result = df_filtered[mask][metric]
                    if len(result) > 0:
                        model_values[data_type] = result.iloc[0]
                        model_has_data = True
                    else:
                        model_values[data_type] = 0
                
                # ì ì–´ë„ í•˜ë‚˜ì˜ ë°ì´í„° íƒ€ì…ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
                if model_has_data:
                    for data_type in allowed_types:
                        data_type_values[data_type].append(model_values[data_type])
                    model_labels.append(model)
            
            # ì‹¤ì œë¡œ ë°ì´í„°ê°€ ìˆëŠ” ëª¨ë¸ë§Œ ì‚¬ìš©
            if len(model_labels) == 0:
                continue  # ì´ ë©”íŠ¸ë¦­ì— ëŒ€í•´ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŒ
                
            x = np.arange(len(model_labels))
            width = 0.8 / len(allowed_types)  # ì—¬ëŸ¬ ë°ì´í„° íƒ€ì…ì— ë§ê²Œ ì¡°ì •
            
            colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']
            bars_list = []
            
            for idx, data_type in enumerate(allowed_types):
                values = data_type_values[data_type]
                if any(v > 0 for v in values):  # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    bars = ax.bar(x + (idx - len(allowed_types)/2 + 0.5) * width, 
                                 values, width, label=data_type, 
                                 alpha=0.8, color=colors[idx % len(colors)])
                    bars_list.append(bars)
                    
                    # ê°’ í‘œì‹œ
                    for bar in bars:
                        height = bar.get_height()
                        if height > 0:
                            ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)
            
            ax.set_title(f'{name} ë¹„êµ', fontsize=12, fontweight='bold')
            ax.set_ylabel(name)
            ax.set_xlabel('ëª¨ë¸')
            ax.set_xticks(x)
            ax.set_xticklabels(model_labels, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'sampling_strategy_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… ìƒ˜í”Œë§ ì „ëµë³„ ë¹„êµ ì €ì¥: sampling_strategy_comparison.png")
    
    def plot_cv_vs_test_comparison(self, summary_df, viz_dir):
        """CV vs Test ì„±ëŠ¥ ë¹„êµ (ê³¼ì í•© í™•ì¸)"""
        print("ğŸ“Š CV vs Test ì„±ëŠ¥ ë¹„êµ ìƒì„±...")
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íƒ€ì… í™•ì¸
        available_data_types = summary_df['Data_Type'].unique()
        available_data_types = [dt for dt in available_data_types if dt != 'MIXED'][:2]  # MIXED ì œì™¸, ìµœëŒ€ 2ê°œ
        
        if not available_data_types:
            print("  âš ï¸ CV vs Test ë¹„êµë¥¼ ìœ„í•œ ë°ì´í„° ì—†ìŒ")
            return
        
        fig, axes = plt.subplots(1, len(available_data_types), figsize=(8*len(available_data_types), 6))
        if len(available_data_types) == 1:
            axes = [axes]  # ë‹¨ì¼ subplotì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        fig.suptitle('CV vs Test AUC ë¹„êµ (ê³¼ì í•© í™•ì¸)', fontsize=16, fontweight='bold')
        
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']
        
        for idx, data_type in enumerate(available_data_types):
            ax = axes[idx]
            
            subset = summary_df[summary_df['Data_Type'] == data_type]
            models = subset['Model'].tolist()
            cv_scores = subset['CV_AUC'].tolist()
            test_scores = subset['Test_AUC'].tolist()
            
            x = np.arange(len(models))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, cv_scores, width, label='CV AUC', 
                          alpha=0.8, color='green')
            bars2 = ax.bar(x + width/2, test_scores, width, label='Test AUC', 
                          alpha=0.8, color=colors[idx])
            
            # ê³¼ì í•© ì •ë„ í‘œì‹œ
            for i, (cv, test) in enumerate(zip(cv_scores, test_scores)):
                overfitting = cv - test
                color = 'red' if overfitting > 0.05 else 'orange' if overfitting > 0.02 else 'green'
                ax.text(i, max(cv, test) + 0.01, f'{overfitting:.3f}', 
                       ha='center', va='bottom', fontsize=10, 
                       fontweight='bold', color=color)
            
            # ê°’ í‘œì‹œ
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)
            
            ax.set_title(f'{data_type} ë°ì´í„°', fontsize=12, fontweight='bold')
            ax.set_ylabel('AUC Score')
            ax.set_xlabel('ëª¨ë¸')
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0.8, 1.0)
            
            # ê³¼ì í•© ê¸°ì¤€ì„ 
            ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.5, 
                      label='High Performance')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'cv_vs_test_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… CV vs Test ë¹„êµ ì €ì¥: cv_vs_test_comparison.png")
    
    def plot_threshold_optimization(self, viz_dir):
        """Threshold ìµœì í™” ê²°ê³¼ ì‹œê°í™”"""
        print("ğŸ¯ Threshold ìµœì í™” ê²°ê³¼ ì‹œê°í™”...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ëª¨ë¸ë³„ ìµœì  Threshold ë¶„ì„', fontsize=16, fontweight='bold')
        
        model_names = []
        thresholds = []
        f1_scores = []
        precision_scores = []
        recall_scores = []
        
        # ê° ëª¨ë¸ì˜ threshold ìµœì í™” ê²°ê³¼ ìˆ˜ì§‘
        for model_key, result in self.results.items():
            if 'threshold_analysis' in result:
                analysis = result['threshold_analysis']
                model_name = model_key.split('_')[0]
                data_type = result['data_type']
                label = f"{model_name}\n({data_type.upper()})"
                
                model_names.append(label)
                thresholds.append(analysis.get('final_threshold', 0.5))
                
                # ìµœì  thresholdì—ì„œì˜ ì„±ëŠ¥
                valid_metrics = result.get('valid_metrics', {})
                f1_scores.append(valid_metrics.get('f1', 0))
                precision_scores.append(valid_metrics.get('precision', 0))
                recall_scores.append(valid_metrics.get('recall', 0))
        
        if not model_names:
            print("  âš ï¸ Threshold ìµœì í™” ë°ì´í„° ì—†ìŒ")
            return
        
        # 1. ëª¨ë¸ë³„ ìµœì  Threshold
        ax1 = axes[0, 0]
        bars = ax1.bar(model_names, thresholds, color=['blue', 'red', 'green', 'orange', 'purple', 'brown'][:len(model_names)])
        ax1.set_title('ëª¨ë¸ë³„ ìµœì  Threshold', fontweight='bold')
        ax1.set_ylabel('Threshold')
        ax1.set_ylim(0, 1)
        
        # ê°’ í‘œì‹œ
        for bar, thresh in zip(bars, thresholds):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{thresh:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # 2. Threshold vs F1 Score
        ax2 = axes[0, 1]
        scatter = ax2.scatter(thresholds, f1_scores, c=range(len(model_names)), 
                             s=100, alpha=0.7, cmap='viridis')
        ax2.set_title('Threshold vs F1 Score', fontweight='bold')
        ax2.set_xlabel('Threshold')
        ax2.set_ylabel('F1 Score')
        ax2.grid(True, alpha=0.3)
        
        # ëª¨ë¸ëª… í‘œì‹œ
        for i, (thresh, f1, name) in enumerate(zip(thresholds, f1_scores, model_names)):
            ax2.annotate(name.split('\n')[0], (thresh, f1), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # 3. Precision vs Recall (ìµœì  thresholdì—ì„œ)
        ax3 = axes[1, 0]
        scatter = ax3.scatter(recall_scores, precision_scores, c=range(len(model_names)),
                             s=100, alpha=0.7, cmap='viridis')
        ax3.set_title('Precision vs Recall (ìµœì  Threshold)', fontweight='bold')
        ax3.set_xlabel('Recall')
        ax3.set_ylabel('Precision')
        ax3.grid(True, alpha=0.3)
        
        # ëª¨ë¸ëª… í‘œì‹œ
        for i, (recall, precision, name) in enumerate(zip(recall_scores, precision_scores, model_names)):
            ax3.annotate(name.split('\n')[0], (recall, precision),
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # 4. ì¢…í•© ì„±ëŠ¥ ë¹„êµ (Radar Chart)
        ax4 = axes[1, 1]
        
        # ê°„ë‹¨í•œ ë°” ì°¨íŠ¸ë¡œ ëŒ€ì²´ (radar chartëŠ” ë³µì¡í•¨)
        x = np.arange(len(model_names))
        width = 0.25
        
        bars1 = ax4.bar(x - width, f1_scores, width, label='F1', alpha=0.8)
        bars2 = ax4.bar(x, precision_scores, width, label='Precision', alpha=0.8)
        bars3 = ax4.bar(x + width, recall_scores, width, label='Recall', alpha=0.8)
        
        ax4.set_title('ìµœì  Thresholdì—ì„œì˜ ì„±ëŠ¥', fontweight='bold')
        ax4.set_ylabel('Score')
        ax4.set_xticks(x)
        ax4.set_xticklabels([name.split('\n')[0] for name in model_names], rotation=45)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'threshold_optimization_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… Threshold ìµœì í™” ë¶„ì„ ì €ì¥: threshold_optimization_analysis.png")
    
    def plot_precision_recall_curves(self, viz_dir):
        """Precision-Recall ê³¡ì„  ì‹œê°í™”"""
        print("ğŸ“ˆ Precision-Recall ê³¡ì„  ìƒì„±...")
        
        # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íƒ€ì…ë§Œ ê°€ì ¸ì˜¤ê¸°
        available_data_types = list(self.data.keys())
        if not available_data_types:
            print("  âš ï¸ Precision-Recall ê³¡ì„  ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì—†ìŒ")
            return
            
        # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ í‘œì‹œ (subplot êµ¬ì¡°ìƒ)
        data_types = available_data_types[:2]
        titles = [f'{dt.upper()} ë°ì´í„°' for dt in data_types]
        colors = ['blue', 'red', 'green']
        
        fig, axes = plt.subplots(1, len(data_types), figsize=(8*len(data_types), 6))
        if len(data_types) == 1:
            axes = [axes]  # ë‹¨ì¼ subplotì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        fig.suptitle('Precision-Recall ê³¡ì„  ë¹„êµ', fontsize=16, fontweight='bold')
        
        for idx, (data_type, title) in enumerate(zip(data_types, titles)):
            ax = axes[idx]
            
            model_names = ['LogisticRegression', 'RandomForest', 'XGBoost']
            
            for i, model_name in enumerate(model_names):
                model_key = f'{model_name}_{data_type}'
                if model_key in self.results and 'threshold_analysis' in self.results[model_key]:
                    analysis = self.results[model_key]['threshold_analysis']
                    
                    if 'pr_curve' in analysis:
                        pr_data = analysis['pr_curve']
                        precision_vals = pr_data['precision']
                        recall_vals = pr_data['recall']
                        
                        # Average Precision ê³„ì‚°
                        valid_metrics = self.results[model_key].get('valid_metrics', {})
                        avg_precision = valid_metrics.get('average_precision', 0)
                        
                        ax.plot(recall_vals, precision_vals, color=colors[i], lw=2,
                               label=f'{model_name} (AP = {avg_precision:.3f})')
                        
                        # ìµœì  threshold í¬ì¸íŠ¸ í‘œì‹œ
                        optimal_threshold = analysis.get('final_threshold', 0.5)
                        
                        # ìµœì  thresholdì—ì„œì˜ precision, recall ì°¾ê¸°
                        opt_precision = valid_metrics.get('precision', 0)
                        opt_recall = valid_metrics.get('recall', 0)
                        
                        ax.scatter([opt_recall], [opt_precision], color=colors[i], 
                                 s=100, marker='*', edgecolors='black', linewidth=1,
                                 label=f'{model_name} ìµœì ì  (T={optimal_threshold:.3f})')
            
            # ê¸°ì¤€ì„  (Random Classifier)
            y_true_ratio = self.data[data_type]['y_valid'].mean()
            ax.axhline(y=y_true_ratio, color='gray', linestyle='--', alpha=0.7,
                      label=f'Random (AP = {y_true_ratio:.3f})')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('Recall')
            ax.set_ylabel('Precision')
            ax.set_title(title)
            ax.legend(loc="lower left", fontsize=9)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'precision_recall_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… Precision-Recall ê³¡ì„  ì €ì¥: precision_recall_curves.png")

    def proper_cv_with_sampling(self, model, X, y, data_type, cv_folds=5):
        """
        ìƒ˜í”Œë§ Data Leakageë¥¼ ë°©ì§€í•˜ëŠ” ì˜¬ë°”ë¥¸ Cross Validation
        ê° CV foldë§ˆë‹¤ ìƒ˜í”Œë§ì„ ë³„ë„ë¡œ ì ìš©
        """
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.config['random_state'])
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            # ê° foldë§ˆë‹¤ ë³„ë„ë¡œ ë¶„í• 
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # í›ˆë ¨ foldì—ë§Œ ìƒ˜í”Œë§ ì ìš© (Data Leakage ë°©ì§€)
            try:
                X_fold_train_resampled, y_fold_train_resampled = self.apply_sampling_strategy(
                    X_fold_train, y_fold_train, data_type
                )
                
                # ëª¨ë¸ ë³µì‚¬ ë° í›ˆë ¨
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train_resampled, y_fold_train_resampled)
                
                # ê²€ì¦ foldì—ì„œ í‰ê°€ (ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
                
            except Exception as e:
                print(f"âš ï¸ Fold {fold+1} {data_type.upper()} ì ìš© ì‹¤íŒ¨: {e}")
                # ìƒ˜í”Œë§ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°ì´í„°ë¡œ í›ˆë ¨
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train, y_fold_train)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
        
        return np.array(scores)

def load_config(config_path='master_config.json'):
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¢ í•œêµ­ ê¸°ì—… ë¶€ì‹¤ì˜ˆì¸¡ - ë§ˆìŠ¤í„° ëª¨ë¸ ëŸ¬ë„ˆ")
    print("="*80)
    
    # ì„¤ì • ë¡œë“œ
    config = load_config()
    
    # ëŸ¬ë„ˆ ìƒì„± ë° ì‹¤í–‰
    runner = MasterModelRunner(config)
    runner.load_data()
    runner.run_all_models()
    runner.save_all_results()
    
    print(f"\nğŸ‰ ëª¨ë“  ëª¨ë¸ ì‹¤í–‰ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {runner.output_dir}")
    print("="*80)

if __name__ == "__main__":
    main() 