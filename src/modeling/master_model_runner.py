"""
ÎßàÏä§ÌÑ∞ Î™®Îç∏ Ïã§Ìñâ Ïä§ÌÅ¨Î¶ΩÌä∏
Normal/SMOTE Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌï¥ 3Í∞ú Î™®Îç∏(LogisticRegression, RandomForest, XGBoost)ÏùÑ ÏûêÎèô Ïã§Ìñâ
"""

import pandas as pd
import numpy as np
import json
import os
import warnings
from datetime import datetime
from pathlib import Path
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings('ignore')

# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï
plt.rcParams['font.family'] = 'AppleGothic' if plt.matplotlib.get_backend() != 'Agg' else 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve, auc,
    precision_recall_curve, average_precision_score, balanced_accuracy_score
)
import xgboost as xgb
import optuna
from optuna.samplers import TPESampler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV
from imblearn.over_sampling import BorderlineSMOTE
from ensemble_model import EnsembleModel

class MasterModelRunner:
    def __init__(self, config):
        """
        ÎßàÏä§ÌÑ∞ Î™®Îç∏ Îü¨ÎÑà Ï¥àÍ∏∞Ìôî
        
        Args:
            config (dict): ÏÑ§Ï†ï ÎîïÏÖîÎÑàÎ¶¨
        """
        self.config = config
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.run_name = f"{config['run_name']}_{self.timestamp}"
        
        # Í≤∞Í≥º Ï†ÄÏû• Í≤ΩÎ°ú ÏÑ§Ï†ï
        self.output_dir = Path(config['output_base_dir']) / self.run_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ÌïòÏúÑ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'results').mkdir(exist_ok=True)
        (self.output_dir / 'visualizations').mkdir(exist_ok=True)
        
        # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Î≥ÄÏàò
        self.data = {}
        self.models = {}
        self.results = {}
        self.selected_features = None
        
        print(f"üöÄ ÎßàÏä§ÌÑ∞ Î™®Îç∏ Îü¨ÎÑà Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        print(f"üìÅ Ïã§Ìñâ Ïù¥Î¶Ñ: {self.run_name}")
        print(f"üìÅ Ï∂úÎ†• Í≤ΩÎ°ú: {self.output_dir}")
        
    def load_data(self):
        """Îç∞Ïù¥ÌÑ∞ Î°úÎìú"""
        print("\nüìÇ Îç∞Ïù¥ÌÑ∞ Î°úÎìú")
        print("="*60)
        
        data_path = Path(self.config['data_path'])
        
        # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ (SMOTEÎäî ÎèôÏ†ÅÏúºÎ°ú Ï†ÅÏö©)
        self.data['normal'] = {
            'X_train': pd.read_csv(data_path / 'X_train_100_normal.csv'),
            'X_valid': pd.read_csv(data_path / 'X_valid_100_normal.csv'),
            'X_test': pd.read_csv(data_path / 'X_test_100_normal.csv'),
            'y_train': pd.read_csv(data_path / 'y_train_100_normal.csv').iloc[:, 0],
            'y_valid': pd.read_csv(data_path / 'y_valid_100_normal.csv').iloc[:, 0],
            'y_test': pd.read_csv(data_path / 'y_test_100_normal.csv').iloc[:, 0]
        }
        
        # SMOTE Îç∞Ïù¥ÌÑ∞Îäî ÎèôÏ†ÅÏúºÎ°ú ÏÉùÏÑ± (Data Leakage Î∞©ÏßÄ)
        self.data['smote'] = self.data['normal'].copy()  # ÎèôÏùºÌïú ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©
        
        # Normal Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥ Ï∂úÎ†•
        data = self.data['normal']
        print(f"‚úÖ NORMAL Îç∞Ïù¥ÌÑ∞:")
        print(f"   Train: {data['X_train'].shape}, Î∂ÄÏã§ÎπÑÏú®: {data['y_train'].mean():.2%}")
        print(f"   Valid: {data['X_valid'].shape}, Î∂ÄÏã§ÎπÑÏú®: {data['y_valid'].mean():.2%}")
        print(f"   Test: {data['X_test'].shape}, Î∂ÄÏã§ÎπÑÏú®: {data['y_test'].mean():.2%}")
        
        # SMOTE Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥ Ï∂úÎ†• (ÎèôÏ†Å Ï†ÅÏö© ÏÑ§Î™Ö)
        print(f"‚úÖ SMOTE Îç∞Ïù¥ÌÑ∞:")
        print(f"   ÏõêÎ≥∏Í≥º ÎèôÏùºÌïú ÌÅ¨Í∏∞ (ÎèôÏ†Å Ï†ÅÏö©): {data['X_train'].shape}")
        print(f"   üîÑ SMOTEÎäî CV Î∞è ÏµúÏ¢Ö ÌõàÎ†® Ïãú ÎèôÏ†ÅÏúºÎ°ú Ï†ÅÏö©Îê©ÎãàÎã§")
        print(f"   üéØ Î™©Ìëú Î∂ÄÏã§ÎπÑÏú®: 10% (BorderlineSMOTE)")
        print(f"   üö´ Data Leakage Î∞©ÏßÄ: CV ÎÇ¥Î∂ÄÏóêÏÑúÎßå Ï†ÅÏö©")
    
    def apply_lasso_feature_selection(self, data_type):
        """Lasso ÌäπÏÑ± ÏÑ†ÌÉù Ï†ÅÏö©"""
        if not self.config['lasso']['enabled']:
            return
            
        print(f"\nüîç Lasso ÌäπÏÑ± ÏÑ†ÌÉù Ï†ÅÏö© ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        # Ïä§ÏºÄÏùºÎßÅ
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        
        # Lasso CV
        lasso_cv = LassoCV(
            alphas=self.config['lasso']['alphas'],
            cv=self.config['lasso']['cv_folds'],
            random_state=self.config['random_state'],
            n_jobs=-1
        )
        lasso_cv.fit(X_train_scaled, y_train)
        
        # ÌäπÏÑ± ÏÑ†ÌÉù
        threshold = self.config['lasso']['threshold']
        if threshold == 'median':
            threshold_value = np.median(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        elif threshold == 'mean':
            threshold_value = np.mean(np.abs(lasso_cv.coef_[lasso_cv.coef_ != 0]))
        else:
            threshold_value = float(threshold)
        
        selected_mask = np.abs(lasso_cv.coef_) >= threshold_value
        selected_features = X_train.columns[selected_mask].tolist()
        
        print(f"‚úÖ ÏµúÏ†Å alpha: {lasso_cv.alpha_:.6f}")
        print(f"üìä ÏÑ†ÌÉùÎêú ÌäπÏÑ±: {len(selected_features)}/{len(X_train.columns)}")
        print(f"üéØ ÏÑ†ÌÉùÎêú ÌäπÏÑ±: {selected_features}")
        
        # Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏
        for split in ['X_train', 'X_valid', 'X_test']:
            self.data[data_type][split] = self.data[data_type][split][selected_features]
        
        # Í≤∞Í≥º Ï†ÄÏû•
        lasso_results = {
            'optimal_alpha': float(lasso_cv.alpha_),
            'threshold_value': float(threshold_value),
            'original_features': len(X_train.columns),
            'selected_features': len(selected_features),
            'selected_feature_names': selected_features,
            'feature_coefficients': dict(zip(X_train.columns, lasso_cv.coef_))
        }
        
        # results ÎîîÎ†âÌÜ†Î¶¨ Ï°¥Ïû¨ ÌôïÏù∏ Î∞è ÏÉùÏÑ±
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(parents=True, exist_ok=True)
        
        with open(results_dir / f'lasso_selection_{data_type}.json', 'w') as f:
            json.dump(lasso_results, f, indent=2, ensure_ascii=False)
    
    def optimize_logistic_regression(self, data_type):
        """Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä ÏµúÏ†ÅÌôî"""
        print(f"\nüîç Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä ÏµúÏ†ÅÌôî ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            # penaltyÏôÄ solverÎ•º ÌïòÎÇòÏùò Ï°∞Ìï©ÏúºÎ°ú ÏÑ†ÌÉù (ÎèôÏ†Å ÏÑ†ÌÉùÏßÄ Î¨∏Ï†ú Ìï¥Í≤∞)
            penalty_solver_combinations = []
            
            # Í∞ÄÎä•Ìïú Î™®Îì† penalty-solver Ï°∞Ìï© ÏÉùÏÑ±
            for penalty in self.config['models']['logistic']['penalty']:
                if penalty == 'l1':
                    for solver in ['liblinear', 'saga']:
                        penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'l2':
                    for solver in self.config['models']['logistic']['l2_solvers']:
                        if solver in ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']:
                            penalty_solver_combinations.append(f"{penalty}_{solver}")
                elif penalty == 'elasticnet':
                    penalty_solver_combinations.append(f"{penalty}_saga")
            
            # Ï°∞Ìï© ÏÑ†ÌÉù
            combination = trial.suggest_categorical('penalty_solver', penalty_solver_combinations)
            penalty, solver = combination.split('_', 1)
            
            C = trial.suggest_float('C', *self.config['models']['logistic']['C_range'], log=True)
            max_iter = trial.suggest_int('max_iter', *self.config['models']['logistic']['max_iter_range'])
            
            params = {
                'penalty': penalty,
                'C': C,
                'solver': solver,
                'max_iter': max_iter,
                'random_state': self.config['random_state']
            }
            
            if penalty == 'elasticnet':
                params['l1_ratio'] = trial.suggest_float('l1_ratio', *self.config['models']['logistic']['l1_ratio_range'])
            
            model = LogisticRegression(**params)
            
            # Data Leakage Î∞©ÏßÄÎ•º ÏúÑÌïú Ïò¨Î∞îÎ•∏ CV (SMOTE Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞)
            if data_type == 'smote':
                # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú (SMOTE Ï†ÅÏö© Ï†Ñ)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_smote(model, X_train_original, y_train_original, cv_folds=5)
            else:
                # Normal Îç∞Ïù¥ÌÑ∞Îäî Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['logistic']['n_trials'])
        
        # ÏµúÏ†Å Î™®Îç∏ ÌõàÎ†®
        best_params = study.best_params.copy()
        
        # penalty_solver Ï°∞Ìï©ÏùÑ Î∂ÑÎ¶¨
        penalty_solver = best_params.pop('penalty_solver')
        penalty, solver = penalty_solver.split('_', 1)
        
        # Î∂ÑÎ¶¨Îêú penaltyÏôÄ solver Ï∂îÍ∞Ä
        best_params['penalty'] = penalty
        best_params['solver'] = solver
        
        model = LogisticRegression(**best_params)
        
        # SMOTE Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞ ÏµúÏ¢Ö ÌõàÎ†®ÏóêÎèÑ SMOTE Ï†ÅÏö©
        if data_type == 'smote':
            smote = BorderlineSMOTE(
                sampling_strategy=0.1, 
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
            model.fit(X_train_smote, y_train_smote)
            print(f"‚úÖ SMOTE Ï†ÅÏö© ÌõÑ ÌõàÎ†®: {len(X_train_smote):,}Í∞ú ÏÉòÌîå")
        else:
            model.fit(X_train, y_train)
        
        # Ï†ÄÏû•
        model_key = f'LogisticRegression_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type
        }
        
        print(f"‚úÖ ÏµúÏ†Å AUC: {study.best_value:.4f}")
        print(f"üìä ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞: {best_params}")
        
        return model
    
    def optimize_random_forest(self, data_type):
        """ÎûúÎç§ Ìè¨Î†àÏä§Ìä∏ ÏµúÏ†ÅÌôî"""
        print(f"\nüå≤ ÎûúÎç§ Ìè¨Î†àÏä§Ìä∏ ÏµúÏ†ÅÌôî ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', *self.config['models']['random_forest']['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *self.config['models']['random_forest']['max_depth_range']),
                'min_samples_split': trial.suggest_int('min_samples_split', *self.config['models']['random_forest']['min_samples_split_range']),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', *self.config['models']['random_forest']['min_samples_leaf_range']),
                'max_features': trial.suggest_float('max_features', *self.config['models']['random_forest']['max_features_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1
            }
            
            model = RandomForestClassifier(**params)
            
            # Data Leakage Î∞©ÏßÄÎ•º ÏúÑÌïú Ïò¨Î∞îÎ•∏ CV (SMOTE Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞)
            if data_type == 'smote':
                # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú (SMOTE Ï†ÅÏö© Ï†Ñ)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_smote(model, X_train_original, y_train_original, cv_folds=5)
            else:
                # Normal Îç∞Ïù¥ÌÑ∞Îäî Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['random_forest']['n_trials'])
        
        # ÏµúÏ†Å Î™®Îç∏ ÌõàÎ†®
        best_params = study.best_params
        model = RandomForestClassifier(**best_params)
        
        # SMOTE Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞ ÏµúÏ¢Ö ÌõàÎ†®ÏóêÎèÑ SMOTE Ï†ÅÏö©
        if data_type == 'smote':
            smote = BorderlineSMOTE(
                sampling_strategy=0.1, 
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
            model.fit(X_train_smote, y_train_smote)
            print(f"‚úÖ SMOTE Ï†ÅÏö© ÌõÑ ÌõàÎ†®: {len(X_train_smote):,}Í∞ú ÏÉòÌîå")
        else:
            model.fit(X_train, y_train)
        
        # Ï†ÄÏû•
        model_key = f'RandomForest_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type,
            'feature_importances': dict(zip(X_train.columns, model.feature_importances_))
        }
        
        print(f"‚úÖ ÏµúÏ†Å AUC: {study.best_value:.4f}")
        print(f"üìä ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞: {best_params}")
        
        return model
    
    def optimize_xgboost(self, data_type):
        """XGBoost ÏµúÏ†ÅÌôî"""
        print(f"\nüöÄ XGBoost ÏµúÏ†ÅÌôî ({data_type.upper()})")
        print("="*60)
        
        X_train = self.data[data_type]['X_train']
        y_train = self.data[data_type]['y_train']
        
        def objective(trial):
            params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'n_estimators': trial.suggest_int('n_estimators', *self.config['models']['xgboost']['n_estimators_range']),
                'max_depth': trial.suggest_int('max_depth', *self.config['models']['xgboost']['max_depth_range']),
                'learning_rate': trial.suggest_float('learning_rate', *self.config['models']['xgboost']['learning_rate_range']),
                'subsample': trial.suggest_float('subsample', *self.config['models']['xgboost']['subsample_range']),
                'colsample_bytree': trial.suggest_float('colsample_bytree', *self.config['models']['xgboost']['colsample_bytree_range']),
                'reg_alpha': trial.suggest_float('reg_alpha', *self.config['models']['xgboost']['reg_alpha_range']),
                'reg_lambda': trial.suggest_float('reg_lambda', *self.config['models']['xgboost']['reg_lambda_range']),
                'random_state': self.config['random_state'],
                'n_jobs': -1,
                'verbosity': 0
            }
            
            model = xgb.XGBClassifier(**params)
            
            # Data Leakage Î∞©ÏßÄÎ•º ÏúÑÌïú Ïò¨Î∞îÎ•∏ CV (SMOTE Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞)
            if data_type == 'smote':
                # ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú (SMOTE Ï†ÅÏö© Ï†Ñ)
                X_train_original = self.data['normal']['X_train']
                y_train_original = self.data['normal']['y_train']
                scores = self.proper_cv_with_smote(model, X_train_original, y_train_original, cv_folds=5)
            else:
                # Normal Îç∞Ïù¥ÌÑ∞Îäî Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©
                skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.config['random_state'])
                scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='roc_auc', n_jobs=-1)
            
            return scores.mean()
        
        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=self.config['random_state']))
        study.optimize(objective, n_trials=self.config['models']['xgboost']['n_trials'])
        
        # ÏµúÏ†Å Î™®Îç∏ ÌõàÎ†®
        best_params = study.best_params
        model = xgb.XGBClassifier(**best_params)
        
        # SMOTE Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÏù∏ Í≤ΩÏö∞ ÏµúÏ¢Ö ÌõàÎ†®ÏóêÎèÑ SMOTE Ï†ÅÏö©
        if data_type == 'smote':
            smote = BorderlineSMOTE(
                sampling_strategy=0.1, 
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
            model.fit(X_train_smote, y_train_smote)
            print(f"‚úÖ SMOTE Ï†ÅÏö© ÌõÑ ÌõàÎ†®: {len(X_train_smote):,}Í∞ú ÏÉòÌîå")
        else:
            model.fit(X_train, y_train)
        
        # Ï†ÄÏû•
        model_key = f'XGBoost_{data_type}'
        self.models[model_key] = model
        self.results[model_key] = {
            'best_params': best_params,
            'cv_score': study.best_value,
            'data_type': data_type,
            'feature_importances': dict(zip(X_train.columns, model.feature_importances_))
        }
        
        print(f"‚úÖ ÏµúÏ†Å AUC: {study.best_value:.4f}")
        print(f"üìä ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞: {best_params}")
        
        return model
    
    def find_optimal_threshold(self, model_key):
        """Í∞Å Î™®Îç∏Î≥Ñ ÏµúÏ†Å threshold Ï∞æÍ∏∞"""
        print(f"\nüéØ {model_key} ÏµúÏ†Å Threshold ÌÉêÏÉâ")
        print("="*60)
        
        model = self.models[model_key]
        data_type = self.results[model_key]['data_type']
        
        X_valid = self.data[data_type]['X_valid']
        y_valid = self.data[data_type]['y_valid']
        
        # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏° ÌôïÎ•†
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
        
        # Precision-Recall Í≥°ÏÑ†Í≥º Îã§ÏñëÌïú thresholdÏóêÏÑúÏùò ÏÑ±Îä• Í≥ÑÏÇ∞
        thresholds = np.arange(0.05, 0.5, 0.05)  # 0.05Î∂ÄÌÑ∞ 0.5ÍπåÏßÄ 0.05 Í∞ÑÍ≤©
        
        threshold_results = []
        
        for threshold in thresholds:
            y_valid_pred = (y_valid_proba >= threshold).astype(int)
            
            # ÏòàÏ∏°Í∞íÏù¥ Î™®Îëê 0Ïù¥Í±∞ÎÇò 1Ïù∏ Í≤ΩÏö∞ Ïä§ÌÇµ
            if len(np.unique(y_valid_pred)) == 1:
                continue
            
            try:
                metrics = {
                    'threshold': threshold,
                    'precision': precision_score(y_valid, y_valid_pred, zero_division=0),
                    'recall': recall_score(y_valid, y_valid_pred, zero_division=0),
                    'f1': f1_score(y_valid, y_valid_pred, zero_division=0),
                    'balanced_accuracy': balanced_accuracy_score(y_valid, y_valid_pred)
                }
                threshold_results.append(metrics)
            except:
                continue
        
        if not threshold_results:
            print("‚ö†Ô∏è ÏµúÏ†Å threshold Ï∞æÍ∏∞ Ïã§Ìå®, Í∏∞Î≥∏Í∞í 0.5 ÏÇ¨Ïö©")
            return 0.5, {}
        
        # Í≤∞Í≥ºÎ•º DataFrameÏúºÎ°ú Î≥ÄÌôò
        threshold_df = pd.DataFrame(threshold_results)
        
        # Ï£ºÏöî Î©îÌä∏Î¶≠Î≥Ñ ÏµúÏ†Å threshold Ï∞æÍ∏∞
        metric_priority = self.config.get('threshold_optimization', {}).get('metric_priority', 'f1')
        
        optimal_thresholds = {}
        for metric in ['f1', 'precision', 'recall', 'balanced_accuracy']:
            if metric in threshold_df.columns:
                best_idx = threshold_df[metric].idxmax()
                optimal_thresholds[metric] = {
                    'threshold': threshold_df.loc[best_idx, 'threshold'],
                    'value': threshold_df.loc[best_idx, metric]
                }
        
        # Ïö∞ÏÑ†ÏàúÏúÑ Î©îÌä∏Î¶≠ÏúºÎ°ú ÏµúÏ¢Ö threshold ÏÑ†ÌÉù
        if metric_priority in optimal_thresholds:
            final_threshold = optimal_thresholds[metric_priority]['threshold']
            final_value = optimal_thresholds[metric_priority]['value']
        else:
            final_threshold = optimal_thresholds['f1']['threshold']
            final_value = optimal_thresholds['f1']['value']
        
        print(f"üìà Threshold ÏµúÏ†ÅÌôî Í≤∞Í≥º:")
        for metric, result in optimal_thresholds.items():
            marker = "üéØ" if metric == metric_priority else "  "
            print(f"{marker} {metric.upper()}: {result['threshold']:.3f} (Í∞í: {result['value']:.4f})")
        
        print(f"\n‚úÖ ÏµúÏ¢Ö ÏÑ†ÌÉù: {final_threshold:.3f} ({metric_priority.upper()}: {final_value:.4f})")
        
        # Precision-Recall Í≥°ÏÑ† Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
        precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_valid, y_valid_proba)
        
        threshold_analysis = {
            'all_thresholds': threshold_results,
            'optimal_by_metric': optimal_thresholds,
            'final_threshold': final_threshold,
            'final_metric': metric_priority,
            'final_value': final_value,
            'pr_curve': {
                'precision': precision_vals.tolist(),
                'recall': recall_vals.tolist(),
                'thresholds': pr_thresholds.tolist()
            }
        }
        
        return final_threshold, threshold_analysis
    
    def evaluate_model(self, model_key):
        """Î™®Îç∏ ÌèâÍ∞Ä (ÏµúÏ†Å threshold ÏÇ¨Ïö©)"""
        # ÏµúÏ†Å threshold Ï∞æÍ∏∞
        optimal_threshold, threshold_analysis = self.find_optimal_threshold(model_key)
        
        model = self.models[model_key]
        data_type = self.results[model_key]['data_type']
        
        X_valid = self.data[data_type]['X_valid']
        y_valid = self.data[data_type]['y_valid']
        X_test = self.data[data_type]['X_test']
        y_test = self.data[data_type]['y_test']
        
        # ÏòàÏ∏° (ÏµúÏ†Å threshold ÏÇ¨Ïö©)
        y_valid_proba = model.predict_proba(X_valid)[:, 1]
        y_valid_pred = (y_valid_proba >= optimal_threshold).astype(int)
        
        y_test_proba = model.predict_proba(X_test)[:, 1]
        y_test_pred = (y_test_proba >= optimal_threshold).astype(int)
        
        # Í≤ÄÏ¶ù ÏÑ±Îä•
        valid_metrics = {
            'auc': roc_auc_score(y_valid, y_valid_proba),
            'precision': precision_score(y_valid, y_valid_pred, zero_division=0),
            'recall': recall_score(y_valid, y_valid_pred, zero_division=0),
            'f1': f1_score(y_valid, y_valid_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_valid, y_valid_pred),
            'average_precision': average_precision_score(y_valid, y_valid_proba)
        }
        
        # ÌÖåÏä§Ìä∏ ÏÑ±Îä•
        test_metrics = {
            'auc': roc_auc_score(y_test, y_test_proba),
            'precision': precision_score(y_test, y_test_pred, zero_division=0),
            'recall': recall_score(y_test, y_test_pred, zero_division=0),
            'f1': f1_score(y_test, y_test_pred, zero_division=0),
            'balanced_accuracy': balanced_accuracy_score(y_test, y_test_pred),
            'average_precision': average_precision_score(y_test, y_test_proba)
        }
        
        # Í≤∞Í≥º ÏóÖÎç∞Ïù¥Ìä∏
        self.results[model_key].update({
            'optimal_threshold': optimal_threshold,
            'threshold_analysis': threshold_analysis,
            'valid_metrics': valid_metrics,
            'test_metrics': test_metrics,
            'predictions': {
                'y_valid_proba': y_valid_proba.tolist(),
                'y_test_proba': y_test_proba.tolist()
            }
        })
        
        print(f"\nüìä {model_key} ÏµúÏ¢Ö ÌèâÍ∞Ä (Threshold: {optimal_threshold:.3f}):")
        print(f"   Í≤ÄÏ¶ù - AUC: {valid_metrics['auc']:.4f}, F1: {valid_metrics['f1']:.4f}, Precision: {valid_metrics['precision']:.4f}, Recall: {valid_metrics['recall']:.4f}")
        print(f"   ÌÖåÏä§Ìä∏ - AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1']:.4f}, Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}")
    
    def run_all_models(self):
        """Î™®Îì† Î™®Îç∏ Ïã§Ìñâ"""
        print("\nüöÄ Î™®Îì† Î™®Îç∏ Ïã§Ìñâ ÏãúÏûë")
        print("="*80)
        
        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖÎ≥ÑÎ°ú Ïã§Ìñâ
        for data_type in ['normal', 'smote']:
            print(f"\nüìä {data_type.upper()} Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨")
            print("="*60)
            
            # Lasso ÌäπÏÑ± ÏÑ†ÌÉù
            if self.config['lasso']['enabled']:
                self.apply_lasso_feature_selection(data_type)
            
            # Î™®Îç∏Î≥Ñ ÏµúÏ†ÅÌôî
            models_to_run = []
            if self.config['models']['logistic']['enabled']:
                models_to_run.append(('logistic', self.optimize_logistic_regression))
            if self.config['models']['random_forest']['enabled']:
                models_to_run.append(('random_forest', self.optimize_random_forest))
            if self.config['models']['xgboost']['enabled']:
                models_to_run.append(('xgboost', self.optimize_xgboost))
            
            for model_name, optimize_func in models_to_run:
                optimize_func(data_type)
        
        # Î™®Îì† Î™®Îç∏ ÌèâÍ∞Ä (ÏµúÏ†Å threshold ÏûêÎèô ÌÉêÏÉâ)
        print(f"\nüìä Î™®Îì† Î™®Îç∏ ÌèâÍ∞Ä Î∞è Threshold ÏµúÏ†ÅÌôî")
        print("="*60)
        
        for model_key in self.models.keys():
            self.evaluate_model(model_key)
        
        # ÏïôÏÉÅÎ∏î Î™®Îç∏ Ïã§Ìñâ
        if self.config.get('ensemble', {}).get('enabled', False):
            self.run_ensemble_model()
    
    def run_ensemble_model(self):
        """ÏïôÏÉÅÎ∏î Î™®Îç∏ Ïã§Ìñâ"""
        print(f"\nüé≠ ÏïôÏÉÅÎ∏î Î™®Îç∏ Ïã§Ìñâ")
        print("="*60)
        
        ensemble_config = self.config['ensemble']
        
        # ÏïôÏÉÅÎ∏îÏóê Ìè¨Ìï®Ìï† Î™®Îç∏Îì§ ÌïÑÌÑ∞ÎßÅ
        ensemble_models = {}
        enabled_models = ensemble_config.get('models', [])
        enabled_data_types = ensemble_config.get('data_types', ['normal', 'smote'])
        
        # Î™®Îç∏ Ïù¥Î¶Ñ Îß§Ìïë (ÏÑ§Ï†ïÎ™Ö -> Ïã§Ï†ú Î™®Îç∏ ÌÇ§Î™Ö)
        model_name_mapping = {
            'logistic': 'LogisticRegression',
            'random_forest': 'RandomForest', 
            'xgboost': 'XGBoost'
        }
        
        print(f"üîç ÌòÑÏû¨ ÌõàÎ†®Îêú Î™®Îç∏Îì§: {list(self.models.keys())}")
        print(f"üéØ ÏïôÏÉÅÎ∏î ÏÑ§Ï†ï - Î™®Îç∏: {enabled_models}, Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: {enabled_data_types}")
        
        for model_key, model_obj in self.models.items():
            # Î™®Îç∏ ÌÇ§ÏóêÏÑú Ï†ïÎ≥¥ Ï∂îÏ∂ú (Ïòà: LogisticRegression_normal)
            model_parts = model_key.split('_')
            if len(model_parts) >= 2:
                model_name = model_parts[0]  # LogisticRegression, RandomForest, XGBoost
                data_type = model_parts[1].lower()  # normal, smote
            else:
                continue  # Ïò¨Î∞îÎ•¥ÏßÄ ÏïäÏùÄ ÌÇ§ ÌòïÏãùÏùÄ Í±¥ÎÑàÎõ∞Í∏∞
            
            # ÏÑ§Ï†ïÏùò Î™®Îç∏Î™ÖÏùÑ Ïã§Ï†ú Î™®Îç∏Î™ÖÏúºÎ°ú Î≥ÄÌôòÌïòÏó¨ ÎπÑÍµê
            enabled_model_names = [model_name_mapping.get(em, em) for em in enabled_models]
            
            # ÏÑ§Ï†ïÏóê Îî∞Îùº Î™®Îç∏ ÏÑ†ÌÉù
            if model_name in enabled_model_names and data_type in enabled_data_types:
                ensemble_models[model_key] = model_obj
                print(f"‚úÖ ÏïôÏÉÅÎ∏îÏóê Ìè¨Ìï®: {model_key} (Î™®Îç∏: {model_name}, Îç∞Ïù¥ÌÑ∞: {data_type})")
        
        if not ensemble_models:
            print("‚ö†Ô∏è ÏïôÏÉÅÎ∏îÏóê Ìè¨Ìï®Ìï† Î™®Îç∏Ïù¥ ÏóÜÏäµÎãàÎã§.")
            print(f"üí° ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥:")
            print(f"   - ÏÑ§Ï†ïÎêú Î™®Îç∏: {enabled_models}")
            print(f"   - Îß§ÌïëÎêú Î™®Îç∏Î™Ö: {[model_name_mapping.get(em, em) for em in enabled_models]}")
            print(f"   - ÏÑ§Ï†ïÎêú Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: {enabled_data_types}")
            print(f"   - Ïã§Ï†ú Î™®Îç∏ ÌÇ§Îì§: {list(self.models.keys())}")
            return
        
        # ÏïôÏÉÅÎ∏î Î™®Îç∏ ÏÉùÏÑ±
        ensemble = EnsembleModel(self.config, ensemble_models)
        
        # Í≤ÄÏ¶ù Î∞è ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ (normal Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö©)
        X_valid = self.data['normal']['X_valid']
        y_valid = self.data['normal']['y_valid']
        X_test = self.data['normal']['X_test']
        y_test = self.data['normal']['y_test']
        
        print(f"\nüéØ ÏïôÏÉÅÎ∏î ÏòàÏ∏° ÏàòÌñâ")
        print("="*40)
        
        # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Î°ú ÏïôÏÉÅÎ∏î ÏòàÏ∏° (ÏûêÎèô Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ Ìè¨Ìï®)
        ensemble_valid_proba = ensemble.ensemble_predict_proba(
            X_valid, X_valid, y_valid
        )
        
        # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏòàÏ∏°
        ensemble_test_proba = ensemble.ensemble_predict_proba(X_test)
        
        # ÏµúÏ†Å threshold Ï∞æÍ∏∞
        if ensemble_config.get('threshold_optimization', {}).get('enabled', True):
            metric = ensemble_config.get('threshold_optimization', {}).get('metric_priority', 'f1')
            optimal_threshold, threshold_metrics = ensemble.find_optimal_threshold(
                X_valid, y_valid, metric=metric
            )
        else:
            optimal_threshold = 0.5
            threshold_metrics = {}
        
        # ÏµúÏ¢Ö ÏÑ±Îä• ÌèâÍ∞Ä
        ensemble_metrics = ensemble.evaluate_ensemble(X_test, y_test, optimal_threshold)
        
        # ÏïôÏÉÅÎ∏î Î™®Îç∏ Ï†ÄÏû•
        ensemble_key = 'ensemble_model'
        self.models[ensemble_key] = ensemble
        
        # Í≤∞Í≥º Ï†ÄÏû•
        self.results[ensemble_key] = {
            'model_type': 'ensemble',
            'data_type': 'mixed',
            'method': ensemble_config.get('method', 'weighted_average'),
            'auto_weight': ensemble_config.get('auto_weight', False),
            'included_models': list(ensemble_models.keys()),
            'weights': ensemble.weights,
            'optimal_threshold': optimal_threshold,
            'threshold_metrics': threshold_metrics,
            'cv_score': np.mean([self.results[mk]['cv_score'] for mk in ensemble_models.keys()]),
            'valid_metrics': {
                'auc': roc_auc_score(y_valid, ensemble_valid_proba),
                'precision': precision_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'recall': recall_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'f1': f1_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int), zero_division=0),
                'balanced_accuracy': balanced_accuracy_score(y_valid, (ensemble_valid_proba >= optimal_threshold).astype(int)),
                'average_precision': average_precision_score(y_valid, ensemble_valid_proba)
            },
            'test_metrics': ensemble_metrics,
            'predictions': {
                'y_valid_proba': ensemble_valid_proba.tolist(),
                'y_test_proba': ensemble_test_proba.tolist()
            }
        }
        
        print(f"\nüèÜ ÏïôÏÉÅÎ∏î Î™®Îç∏ ÏµúÏ¢Ö ÏÑ±Îä•:")
        print(f"   Î∞©Î≤ï: {ensemble_config.get('method', 'weighted_average')}")
        print(f"   Ìè¨Ìï® Î™®Îç∏: {len(ensemble_models)}Í∞ú")
        print(f"   ÏµúÏ†Å Threshold: {optimal_threshold:.3f}")
        print(f"   ÌÖåÏä§Ìä∏ AUC: {ensemble_metrics['auc']:.4f}")
        print(f"   ÌÖåÏä§Ìä∏ F1: {ensemble_metrics['f1']:.4f}")
        print(f"   ÌÖåÏä§Ìä∏ Precision: {ensemble_metrics['precision']:.4f}")
        print(f"   ÌÖåÏä§Ìä∏ Recall: {ensemble_metrics['recall']:.4f}")
        
        # ÏïôÏÉÅÎ∏î ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
        viz_dir = self.output_dir / 'visualizations'
        ensemble.create_ensemble_report(viz_dir)
    
    def save_all_results(self):
        """Î™®Îì† Í≤∞Í≥º Ï†ÄÏû•"""
        print(f"\nüíæ Í≤∞Í≥º Ï†ÄÏû•")
        print("="*60)
        
        # Î™®Îç∏ Ï†ÄÏû•
        models_dir = self.output_dir / 'models'
        models_dir.mkdir(exist_ok=True)
        
        for model_key, model in self.models.items():
            model_path = models_dir / f'{model_key.lower()}_model.joblib'
            joblib.dump(model, model_path)
            print(f"‚úÖ {model_key} Î™®Îç∏ Ï†ÄÏû•: {model_path}")
        
        # Í≤∞Í≥º Ï†ÄÏû• (JSON ÏßÅÎ†¨Ìôî Í∞ÄÎä•ÌïòÎèÑÎ°ù Î≥ÄÌôò)
        def convert_to_serializable(obj):
            """JSON ÏßÅÎ†¨Ìôî Í∞ÄÎä•Ìïú ÌòïÌÉúÎ°ú Î≥ÄÌôò"""
            if isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(v) for v in obj]
            elif isinstance(obj, (np.float32, np.float64)):
                return float(obj)
            elif isinstance(obj, (np.int32, np.int64)):
                return int(obj)
            else:
                return obj
        
        serializable_results = convert_to_serializable(self.results)
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(exist_ok=True)
        
        results_path = results_dir / 'all_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ Ï†ÑÏ≤¥ Í≤∞Í≥º Ï†ÄÏû•: {results_path}")
        
        # ÏÑ§Ï†ï Ï†ÄÏû•
        config_path = self.output_dir / 'config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ÏÑ§Ï†ï Ï†ÄÏû•: {config_path}")
        
        # ÏöîÏïΩ ÌÖåÏù¥Î∏î ÏÉùÏÑ±
        self.create_summary_table()
        
        # ÏãúÍ∞ÅÌôî ÏÉùÏÑ±
        self.create_visualizations()
    
    def create_summary_table(self):
        """ÏöîÏïΩ ÌÖåÏù¥Î∏î ÏÉùÏÑ±"""
        summary_data = []
        
        for model_key, result in self.results.items():
            model_name = model_key.split('_')[0]
            data_type = result['data_type']
            
            summary_data.append({
                'Model': model_name,
                'Data_Type': data_type.upper(),
                'Optimal_Threshold': result.get('optimal_threshold', 0.5),
                'CV_AUC': result['cv_score'],
                'Valid_AUC': result['valid_metrics']['auc'],
                'Valid_F1': result['valid_metrics']['f1'],
                'Test_AUC': result['test_metrics']['auc'],
                'Test_Precision': result['test_metrics']['precision'],
                'Test_Recall': result['test_metrics']['recall'],
                'Test_F1': result['test_metrics']['f1'],
                'Test_Balanced_Acc': result['test_metrics'].get('balanced_accuracy', 0),
                'Average_Precision': result['test_metrics'].get('average_precision', 0)
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # Ï†ÄÏû•
        results_dir = self.output_dir / 'results'
        results_dir.mkdir(exist_ok=True)
        summary_path = results_dir / 'summary_table.csv'
        summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
        
        print(f"\nüèÜ Ïã§Ìñâ Í≤∞Í≥º ÏöîÏïΩ:")
        print(summary_df.round(4))
        print(f"‚úÖ ÏöîÏïΩ ÌÖåÏù¥Î∏î Ï†ÄÏû•: {summary_path}")
        
        return summary_df
    
    def create_visualizations(self):
        """Î™®Îì† ÏãúÍ∞ÅÌôî ÏÉùÏÑ±"""
        print(f"\nüìà ÏãúÍ∞ÅÌôî ÏÉùÏÑ±")
        print("="*60)
        
        viz_dir = self.output_dir / 'visualizations'
        viz_dir.mkdir(exist_ok=True)
        
        # ÏöîÏïΩ ÌÖåÏù¥Î∏î ÏÉùÏÑ± (ÏãúÍ∞ÅÌôîÏóêÏÑú ÏÇ¨Ïö©)
        summary_df = self.create_summary_table()
        
        # 1. ÏÑ±Îä• ÏßÄÌëú ÎπÑÍµê Ï∞®Ìä∏
        self.plot_performance_comparison(summary_df, viz_dir)
        
        # 2. ROC Í≥°ÏÑ† ÎπÑÍµê
        self.plot_roc_curves(viz_dir)
        
        # 3. ÌäπÏÑ± Ï§ëÏöîÎèÑ ÎπÑÍµê (RF, XGBoost)
        self.plot_feature_importance_comparison(viz_dir)
        
        # 4. Normal vs SMOTE ÎπÑÍµê
        self.plot_normal_vs_smote_comparison(summary_df, viz_dir)
        
        # 5. CV vs Test ÏÑ±Îä• ÎπÑÍµê
        self.plot_cv_vs_test_comparison(summary_df, viz_dir)
        
        # 6. Threshold ÏµúÏ†ÅÌôî Í≤∞Í≥º ÏãúÍ∞ÅÌôî
        self.plot_threshold_optimization(viz_dir)
        
        # 7. Precision-Recall Í≥°ÏÑ†
        self.plot_precision_recall_curves(viz_dir)
        
        print(f"‚úÖ Î™®Îì† ÏãúÍ∞ÅÌôî ÏôÑÎ£å: {viz_dir}")
    
    def plot_performance_comparison(self, summary_df, viz_dir):
        """ÏÑ±Îä• ÏßÄÌëú ÎπÑÍµê Ï∞®Ìä∏"""
        print("üìä ÏÑ±Îä• ÏßÄÌëú ÎπÑÍµê Ï∞®Ìä∏ ÏÉùÏÑ±...")
        
        # ÏßÄÌëúÎ≥Ñ ÎπÑÍµê
        metrics = ['CV_AUC', 'Test_AUC', 'Test_Precision', 'Test_Recall', 'Test_F1']
        metric_names = ['CV AUC', 'Test AUC', 'Test Precision', 'Test Recall', 'Test F1']
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Î™®Îç∏ ÏÑ±Îä• ÏßÄÌëú ÎπÑÍµê (ÏïôÏÉÅÎ∏î Ìè¨Ìï®)', fontsize=16, fontweight='bold')
        
        # ÏÉâÏÉÅ ÏÑ§Ï†ï
        colors = {'NORMAL': 'skyblue', 'SMOTE': 'lightcoral', 'MIXED': 'gold', 'ENSEMBLE': 'purple'}
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            row, col = i // 3, i % 3
            ax = axes[row, col]
            
            # ÏïôÏÉÅÎ∏îÍ≥º ÏùºÎ∞ò Î™®Îç∏ Î∂ÑÎ¶¨
            ensemble_data = summary_df[summary_df['Model'] == 'ensemble']
            regular_data = summary_df[summary_df['Model'] != 'ensemble']
            
            if not regular_data.empty:
                # ÏùºÎ∞ò Î™®Îç∏Îì§ - Í∏∞Ï°¥ Î∞©Ïãù
                pivot_data = regular_data.pivot(index='Model', columns='Data_Type', values=metric)
                
                # Î∞î Ï∞®Ìä∏
                x = np.arange(len(pivot_data.index))
                width = 0.35
                
                if 'NORMAL' in pivot_data.columns:
                    bars1 = ax.bar(x - width/2, pivot_data['NORMAL'], width, 
                                  label='Normal', alpha=0.8, color=colors['NORMAL'])
                if 'SMOTE' in pivot_data.columns:
                    bars2 = ax.bar(x + width/2, pivot_data['SMOTE'], width, 
                                  label='SMOTE', alpha=0.8, color=colors['SMOTE'])
                
                # Í∞í ÌëúÏãú
                for col_name in pivot_data.columns:
                    if col_name in ['NORMAL', 'SMOTE']:
                        bars = ax.containers[list(pivot_data.columns).index(col_name)]
                        for bar in bars:
                            height = bar.get_height()
                            if not np.isnan(height):
                                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)
                
                ax.set_xticks(x)
                ax.set_xticklabels(pivot_data.index, rotation=45)
                
                # ÏïôÏÉÅÎ∏î Ï∂îÍ∞Ä
                if not ensemble_data.empty:
                    ensemble_x = len(pivot_data.index)
                    ensemble_value = ensemble_data[metric].iloc[0]
                    bars3 = ax.bar(ensemble_x, ensemble_value, width*2, 
                                  label='Ensemble', alpha=0.9, color=colors['ENSEMBLE'])
                    
                    # ÏïôÏÉÅÎ∏î Í∞í ÌëúÏãú
                    ax.text(ensemble_x, ensemble_value + 0.01,
                           f'{ensemble_value:.3f}', ha='center', va='bottom', fontsize=9)
                    
                    # xÏ∂ï ÎùºÎ≤® ÏóÖÎç∞Ïù¥Ìä∏
                    all_labels = list(pivot_data.index) + ['Ensemble']
                    ax.set_xticks(list(range(len(all_labels))))
                    ax.set_xticklabels(all_labels, rotation=45)
            
            elif not ensemble_data.empty:
                # ÏïôÏÉÅÎ∏îÎßå ÏûàÎäî Í≤ΩÏö∞
                ensemble_value = ensemble_data[metric].iloc[0]
                ax.bar(0, ensemble_value, width=0.5, 
                      label='Ensemble', alpha=0.9, color=colors['ENSEMBLE'])
                ax.text(0, ensemble_value + 0.01,
                       f'{ensemble_value:.3f}', ha='center', va='bottom', fontsize=9)
                ax.set_xticks([0])
                ax.set_xticklabels(['Ensemble'])
            
            ax.set_title(f'{name}', fontsize=12, fontweight='bold')
            ax.set_ylabel(name)
            ax.set_xlabel('Î™®Îç∏')
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1.1 if metric != 'Test_F1' else max(summary_df[metric].max() * 1.2, 0.5))
        
        # ÎßàÏßÄÎßâ subplot Ï†úÍ±∞
        axes[1, 2].remove()
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ ÏÑ±Îä• ÎπÑÍµê Ï∞®Ìä∏ Ï†ÄÏû•: performance_comparison.png")
    
    def plot_roc_curves(self, viz_dir):
        """ROC Í≥°ÏÑ† ÎπÑÍµê"""
        print("üìà ROC Í≥°ÏÑ† ÏÉùÏÑ±...")
        
        from sklearn.metrics import roc_curve, auc
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('ROC Í≥°ÏÑ† ÎπÑÍµê', fontsize=16, fontweight='bold')
        
        data_types = ['normal', 'smote']
        titles = ['Normal Îç∞Ïù¥ÌÑ∞', 'SMOTE Îç∞Ïù¥ÌÑ∞']
        colors = ['blue', 'red', 'green']
        
        for idx, (data_type, title) in enumerate(zip(data_types, titles)):
            ax = axes[idx]
            
            X_test = self.data[data_type]['X_test']
            y_test = self.data[data_type]['y_test']
            
            model_names = ['LogisticRegression', 'RandomForest', 'XGBoost']
            
            for i, model_name in enumerate(model_names):
                model_key = f'{model_name}_{data_type}'
                if model_key in self.models:
                    model = self.models[model_key]
                    y_proba = model.predict_proba(X_test)[:, 1]
                    
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    roc_auc = auc(fpr, tpr)
                    
                    ax.plot(fpr, tpr, color=colors[i], lw=2, 
                           label=f'{model_name} (AUC = {roc_auc:.4f})')
            
            # ÎåÄÍ∞ÅÏÑ† (ÎûúÎç§ Î∂ÑÎ•òÍ∏∞)
            ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', 
                   label='Random (AUC = 0.5000)')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(title)
            ax.legend(loc="lower right")
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'roc_curves_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ ROC Í≥°ÏÑ† Ï†ÄÏû•: roc_curves_comparison.png")
    
    def plot_feature_importance_comparison(self, viz_dir):
        """ÌäπÏÑ± Ï§ëÏöîÎèÑ ÎπÑÍµê (RF, XGBoostÎßå)"""
        print("üîç ÌäπÏÑ± Ï§ëÏöîÎèÑ ÎπÑÍµê ÏÉùÏÑ±...")
        
        tree_models = ['RandomForest', 'XGBoost']
        data_types = ['normal', 'smote']
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ÌäπÏÑ± Ï§ëÏöîÎèÑ ÎπÑÍµê (Tree-based Î™®Îç∏)', fontsize=16, fontweight='bold')
        
        for i, model_name in enumerate(tree_models):
            for j, data_type in enumerate(data_types):
                ax = axes[i, j]
                model_key = f'{model_name}_{data_type}'
                
                if model_key in self.results and 'feature_importances' in self.results[model_key]:
                    importances = self.results[model_key]['feature_importances']
                    
                    # ÏÉÅÏúÑ 10Í∞ú ÌäπÏÑ±
                    sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]
                    features, values = zip(*sorted_features)
                    
                    # ÏÉâÏÉÅ ÏÑ§Ï†ï
                    color = 'green' if model_name == 'RandomForest' else 'purple'
                    
                    bars = ax.barh(features, values, color=color, alpha=0.7)
                    
                    # Í∞í ÌëúÏãú
                    for k, v in enumerate(values):
                        ax.text(v + 0.001, k, f'{v:.3f}', va='center', fontsize=9)
                    
                    ax.set_title(f'{model_name} - {data_type.upper()}\nÌäπÏÑ± Ï§ëÏöîÎèÑ (Top 10)', 
                               fontsize=12, fontweight='bold')
                    ax.set_xlabel('Ï§ëÏöîÎèÑ')
                    ax.grid(True, alpha=0.3)
                else:
                    ax.text(0.5, 0.5, 'No data available', ha='center', va='center', 
                           transform=ax.transAxes, fontsize=12)
                    ax.set_title(f'{model_name} - {data_type.upper()}')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'feature_importance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ ÌäπÏÑ± Ï§ëÏöîÎèÑ ÎπÑÍµê Ï†ÄÏû•: feature_importance_comparison.png")
    
    def plot_normal_vs_smote_comparison(self, summary_df, viz_dir):
        """Normal vs SMOTE ÏÉÅÏÑ∏ ÎπÑÍµê"""
        print("‚öñÔ∏è Normal vs SMOTE ÎπÑÍµê ÏÉùÏÑ±...")
        
        # ensemble Î™®Îç∏ Ï†úÏô∏ (MIXED Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ)
        df_filtered = summary_df[summary_df['Data_Type'].isin(['NORMAL', 'SMOTE'])]
        
        if len(df_filtered) == 0:
            print("  ‚ö†Ô∏è Normal/SMOTE ÎπÑÍµêÌï† Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Normal vs SMOTE Îç∞Ïù¥ÌÑ∞ ÏÑ±Îä• ÎπÑÍµê', fontsize=16, fontweight='bold')
        
        models = df_filtered['Model'].unique()
        metrics = ['Test_AUC', 'Test_F1', 'Test_Precision', 'Test_Recall']
        metric_names = ['Test AUC', 'Test F1', 'Test Precision', 'Test Recall']
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i//2, i%2]
            
            normal_values = []
            smote_values = []
            
            for model in models:
                # NORMAL Îç∞Ïù¥ÌÑ∞ Í≤∞Í≥º ÌôïÏù∏
                normal_mask = (df_filtered['Model'] == model) & (df_filtered['Data_Type'] == 'NORMAL')
                normal_result = df_filtered[normal_mask][metric]
                normal_val = normal_result.iloc[0] if len(normal_result) > 0 else 0
                
                # SMOTE Îç∞Ïù¥ÌÑ∞ Í≤∞Í≥º ÌôïÏù∏  
                smote_mask = (df_filtered['Model'] == model) & (df_filtered['Data_Type'] == 'SMOTE')
                smote_result = df_filtered[smote_mask][metric]
                smote_val = smote_result.iloc[0] if len(smote_result) > 0 else 0
                
                # Îëò Îã§ ÏûàÎäî Í≤ΩÏö∞Îßå Ï∂îÍ∞Ä
                if len(normal_result) > 0 and len(smote_result) > 0:
                    normal_values.append(normal_val)
                    smote_values.append(smote_val)
            
            # Ïã§Ï†úÎ°ú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÎäî Î™®Îç∏Îßå ÏÇ¨Ïö©
            if len(normal_values) == 0 or len(smote_values) == 0:
                continue  # Ïù¥ Î©îÌä∏Î¶≠Ïóê ÎåÄÌï¥ Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏùå
                
            x = np.arange(len(normal_values))  # normal_values Í∏∏Ïù¥Ïóê ÎßûÏ∂§
            width = 0.35
            
            bars1 = ax.bar(x - width/2, normal_values, width, label='Normal', 
                          alpha=0.8, color='skyblue')
            bars2 = ax.bar(x + width/2, smote_values, width, label='SMOTE', 
                          alpha=0.8, color='lightcoral')
            
            # Í∞úÏÑ†ÎèÑ ÌëúÏãú
            for j, (normal, smote) in enumerate(zip(normal_values, smote_values)):
                improvement = ((smote - normal) / normal * 100) if normal > 0 else 0
                ax.text(j, max(normal, smote) + 0.02, f'{improvement:+.1f}%', 
                       ha='center', va='bottom', fontsize=10, fontweight='bold')
            
            # Í∞í ÌëúÏãú
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)
            
            # Ïã§Ï†ú ÏÇ¨Ïö©Îêú Î™®Îç∏Îì§Ïùò Î†àÏù¥Î∏î ÏÉùÏÑ±
            used_model_labels = []
            for model in models:
                # NORMALÍ≥º SMOTE Îëò Îã§ ÏûàÎäî Î™®Îç∏Îßå
                normal_mask = (df_filtered['Model'] == model) & (df_filtered['Data_Type'] == 'NORMAL')
                smote_mask = (df_filtered['Model'] == model) & (df_filtered['Data_Type'] == 'SMOTE')
                if len(df_filtered[normal_mask]) > 0 and len(df_filtered[smote_mask]) > 0:
                    used_model_labels.append(model)
            
            ax.set_title(f'{name} ÎπÑÍµê', fontsize=12, fontweight='bold')
            ax.set_ylabel(name)
            ax.set_xlabel('Î™®Îç∏')
            ax.set_xticks(x)
            ax.set_xticklabels(used_model_labels, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'normal_vs_smote_detailed.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ Normal vs SMOTE ÎπÑÍµê Ï†ÄÏû•: normal_vs_smote_detailed.png")
    
    def plot_cv_vs_test_comparison(self, summary_df, viz_dir):
        """CV vs Test ÏÑ±Îä• ÎπÑÍµê (Í≥ºÏ†ÅÌï© ÌôïÏù∏)"""
        print("üìä CV vs Test ÏÑ±Îä• ÎπÑÍµê ÏÉùÏÑ±...")
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('CV vs Test AUC ÎπÑÍµê (Í≥ºÏ†ÅÌï© ÌôïÏù∏)', fontsize=16, fontweight='bold')
        
        data_types = ['NORMAL', 'SMOTE']
        colors = ['skyblue', 'lightcoral']
        
        for idx, data_type in enumerate(data_types):
            ax = axes[idx]
            
            subset = summary_df[summary_df['Data_Type'] == data_type]
            models = subset['Model'].tolist()
            cv_scores = subset['CV_AUC'].tolist()
            test_scores = subset['Test_AUC'].tolist()
            
            x = np.arange(len(models))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, cv_scores, width, label='CV AUC', 
                          alpha=0.8, color='green')
            bars2 = ax.bar(x + width/2, test_scores, width, label='Test AUC', 
                          alpha=0.8, color=colors[idx])
            
            # Í≥ºÏ†ÅÌï© Ï†ïÎèÑ ÌëúÏãú
            for i, (cv, test) in enumerate(zip(cv_scores, test_scores)):
                overfitting = cv - test
                color = 'red' if overfitting > 0.05 else 'orange' if overfitting > 0.02 else 'green'
                ax.text(i, max(cv, test) + 0.01, f'{overfitting:.3f}', 
                       ha='center', va='bottom', fontsize=10, 
                       fontweight='bold', color=color)
            
            # Í∞í ÌëúÏãú
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                           f'{height:.3f}', ha='center', va='bottom', fontsize=9)
            
            ax.set_title(f'{data_type} Îç∞Ïù¥ÌÑ∞', fontsize=12, fontweight='bold')
            ax.set_ylabel('AUC Score')
            ax.set_xlabel('Î™®Îç∏')
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45)
            ax.legend()
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0.8, 1.0)
            
            # Í≥ºÏ†ÅÌï© Í∏∞Ï§ÄÏÑ†
            ax.axhline(y=0.95, color='red', linestyle='--', alpha=0.5, 
                      label='High Performance')
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'cv_vs_test_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ CV vs Test ÎπÑÍµê Ï†ÄÏû•: cv_vs_test_comparison.png")
    
    def plot_threshold_optimization(self, viz_dir):
        """Threshold ÏµúÏ†ÅÌôî Í≤∞Í≥º ÏãúÍ∞ÅÌôî"""
        print("üéØ Threshold ÏµúÏ†ÅÌôî Í≤∞Í≥º ÏãúÍ∞ÅÌôî...")
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Î™®Îç∏Î≥Ñ ÏµúÏ†Å Threshold Î∂ÑÏÑù', fontsize=16, fontweight='bold')
        
        model_names = []
        thresholds = []
        f1_scores = []
        precision_scores = []
        recall_scores = []
        
        # Í∞Å Î™®Îç∏Ïùò threshold ÏµúÏ†ÅÌôî Í≤∞Í≥º ÏàòÏßë
        for model_key, result in self.results.items():
            if 'threshold_analysis' in result:
                analysis = result['threshold_analysis']
                model_name = model_key.split('_')[0]
                data_type = result['data_type']
                label = f"{model_name}\n({data_type.upper()})"
                
                model_names.append(label)
                thresholds.append(analysis.get('final_threshold', 0.5))
                
                # ÏµúÏ†Å thresholdÏóêÏÑúÏùò ÏÑ±Îä•
                valid_metrics = result.get('valid_metrics', {})
                f1_scores.append(valid_metrics.get('f1', 0))
                precision_scores.append(valid_metrics.get('precision', 0))
                recall_scores.append(valid_metrics.get('recall', 0))
        
        if not model_names:
            print("  ‚ö†Ô∏è Threshold ÏµúÏ†ÅÌôî Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")
            return
        
        # 1. Î™®Îç∏Î≥Ñ ÏµúÏ†Å Threshold
        ax1 = axes[0, 0]
        bars = ax1.bar(model_names, thresholds, color=['blue', 'red', 'green', 'orange', 'purple', 'brown'][:len(model_names)])
        ax1.set_title('Î™®Îç∏Î≥Ñ ÏµúÏ†Å Threshold', fontweight='bold')
        ax1.set_ylabel('Threshold')
        ax1.set_ylim(0, 1)
        
        # Í∞í ÌëúÏãú
        for bar, thresh in zip(bars, thresholds):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{thresh:.3f}', ha='center', va='bottom', fontweight='bold')
        
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # 2. Threshold vs F1 Score
        ax2 = axes[0, 1]
        scatter = ax2.scatter(thresholds, f1_scores, c=range(len(model_names)), 
                             s=100, alpha=0.7, cmap='viridis')
        ax2.set_title('Threshold vs F1 Score', fontweight='bold')
        ax2.set_xlabel('Threshold')
        ax2.set_ylabel('F1 Score')
        ax2.grid(True, alpha=0.3)
        
        # Î™®Îç∏Î™Ö ÌëúÏãú
        for i, (thresh, f1, name) in enumerate(zip(thresholds, f1_scores, model_names)):
            ax2.annotate(name.split('\n')[0], (thresh, f1), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # 3. Precision vs Recall (ÏµúÏ†Å thresholdÏóêÏÑú)
        ax3 = axes[1, 0]
        scatter = ax3.scatter(recall_scores, precision_scores, c=range(len(model_names)),
                             s=100, alpha=0.7, cmap='viridis')
        ax3.set_title('Precision vs Recall (ÏµúÏ†Å Threshold)', fontweight='bold')
        ax3.set_xlabel('Recall')
        ax3.set_ylabel('Precision')
        ax3.grid(True, alpha=0.3)
        
        # Î™®Îç∏Î™Ö ÌëúÏãú
        for i, (recall, precision, name) in enumerate(zip(recall_scores, precision_scores, model_names)):
            ax3.annotate(name.split('\n')[0], (recall, precision),
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        # 4. Ï¢ÖÌï© ÏÑ±Îä• ÎπÑÍµê (Radar Chart)
        ax4 = axes[1, 1]
        
        # Í∞ÑÎã®Ìïú Î∞î Ï∞®Ìä∏Î°ú ÎåÄÏ≤¥ (radar chartÎäî Î≥µÏû°Ìï®)
        x = np.arange(len(model_names))
        width = 0.25
        
        bars1 = ax4.bar(x - width, f1_scores, width, label='F1', alpha=0.8)
        bars2 = ax4.bar(x, precision_scores, width, label='Precision', alpha=0.8)
        bars3 = ax4.bar(x + width, recall_scores, width, label='Recall', alpha=0.8)
        
        ax4.set_title('ÏµúÏ†Å ThresholdÏóêÏÑúÏùò ÏÑ±Îä•', fontweight='bold')
        ax4.set_ylabel('Score')
        ax4.set_xticks(x)
        ax4.set_xticklabels([name.split('\n')[0] for name in model_names], rotation=45)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'threshold_optimization_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ Threshold ÏµúÏ†ÅÌôî Î∂ÑÏÑù Ï†ÄÏû•: threshold_optimization_analysis.png")
    
    def plot_precision_recall_curves(self, viz_dir):
        """Precision-Recall Í≥°ÏÑ† ÏãúÍ∞ÅÌôî"""
        print("üìà Precision-Recall Í≥°ÏÑ† ÏÉùÏÑ±...")
        
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        fig.suptitle('Precision-Recall Í≥°ÏÑ† ÎπÑÍµê', fontsize=16, fontweight='bold')
        
        data_types = ['normal', 'smote']
        titles = ['Normal Îç∞Ïù¥ÌÑ∞', 'SMOTE Îç∞Ïù¥ÌÑ∞']
        colors = ['blue', 'red', 'green']
        
        for idx, (data_type, title) in enumerate(zip(data_types, titles)):
            ax = axes[idx]
            
            model_names = ['LogisticRegression', 'RandomForest', 'XGBoost']
            
            for i, model_name in enumerate(model_names):
                model_key = f'{model_name}_{data_type}'
                if model_key in self.results and 'threshold_analysis' in self.results[model_key]:
                    analysis = self.results[model_key]['threshold_analysis']
                    
                    if 'pr_curve' in analysis:
                        pr_data = analysis['pr_curve']
                        precision_vals = pr_data['precision']
                        recall_vals = pr_data['recall']
                        
                        # Average Precision Í≥ÑÏÇ∞
                        valid_metrics = self.results[model_key].get('valid_metrics', {})
                        avg_precision = valid_metrics.get('average_precision', 0)
                        
                        ax.plot(recall_vals, precision_vals, color=colors[i], lw=2,
                               label=f'{model_name} (AP = {avg_precision:.3f})')
                        
                        # ÏµúÏ†Å threshold Ìè¨Ïù∏Ìä∏ ÌëúÏãú
                        optimal_threshold = analysis.get('final_threshold', 0.5)
                        
                        # ÏµúÏ†Å thresholdÏóêÏÑúÏùò precision, recall Ï∞æÍ∏∞
                        opt_precision = valid_metrics.get('precision', 0)
                        opt_recall = valid_metrics.get('recall', 0)
                        
                        ax.scatter([opt_recall], [opt_precision], color=colors[i], 
                                 s=100, marker='*', edgecolors='black', linewidth=1,
                                 label=f'{model_name} ÏµúÏ†ÅÏ†ê (T={optimal_threshold:.3f})')
            
            # Í∏∞Ï§ÄÏÑ† (Random Classifier)
            y_true_ratio = self.data[data_type]['y_valid'].mean()
            ax.axhline(y=y_true_ratio, color='gray', linestyle='--', alpha=0.7,
                      label=f'Random (AP = {y_true_ratio:.3f})')
            
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('Recall')
            ax.set_ylabel('Precision')
            ax.set_title(title)
            ax.legend(loc="lower left", fontsize=9)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(viz_dir / 'precision_recall_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ‚úÖ Precision-Recall Í≥°ÏÑ† Ï†ÄÏû•: precision_recall_curves.png")

    def proper_cv_with_smote(self, model, X, y, cv_folds=5, sampling_strategy=0.1):
        """
        SMOTE Data LeakageÎ•º Î∞©ÏßÄÌïòÎäî Ïò¨Î∞îÎ•∏ Cross Validation
        Í∞Å CV foldÎßàÎã§ SMOTEÎ•º Î≥ÑÎèÑÎ°ú Ï†ÅÏö©
        """
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.config['random_state'])
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            # Í∞Å foldÎßàÎã§ Î≥ÑÎèÑÎ°ú Î∂ÑÌï†
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # ÌõàÎ†® foldÏóêÎßå SMOTE Ï†ÅÏö© (Data Leakage Î∞©ÏßÄ)
            smote = BorderlineSMOTE(
                sampling_strategy=sampling_strategy, 
                random_state=self.config['random_state'],
                k_neighbors=5,
                m_neighbors=10
            )
            
            try:
                X_fold_train_smote, y_fold_train_smote = smote.fit_resample(X_fold_train, y_fold_train)
                
                # Î™®Îç∏ Î≥µÏÇ¨ Î∞è ÌõàÎ†®
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train_smote, y_fold_train_smote)
                
                # Í≤ÄÏ¶ù foldÏóêÏÑú ÌèâÍ∞Ä (ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞Îßå ÏÇ¨Ïö©)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Fold {fold+1} SMOTE Ï†ÅÏö© Ïã§Ìå®: {e}")
                # SMOTE Ïã§Ìå® Ïãú ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞Î°ú ÌõàÎ†®
                model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_fold_train, y_fold_train)
                y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]
                score = roc_auc_score(y_fold_val, y_pred_proba)
                scores.append(score)
        
        return np.array(scores)

def load_config(config_path='master_config.json'):
    """ÏÑ§Ï†ï ÌååÏùº Î°úÎìú"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("üè¢ ÌïúÍµ≠ Í∏∞ÏóÖ Î∂ÄÏã§ÏòàÏ∏° - ÎßàÏä§ÌÑ∞ Î™®Îç∏ Îü¨ÎÑà")
    print("="*80)
    
    # ÏÑ§Ï†ï Î°úÎìú
    config = load_config()
    
    # Îü¨ÎÑà ÏÉùÏÑ± Î∞è Ïã§Ìñâ
    runner = MasterModelRunner(config)
    runner.load_data()
    runner.run_all_models()
    runner.save_all_results()
    
    print(f"\nüéâ Î™®Îì† Î™®Îç∏ Ïã§Ìñâ ÏôÑÎ£å!")
    print(f"üìÅ Í≤∞Í≥º Ï†ÄÏû• ÏúÑÏπò: {runner.output_dir}")
    print("="*80)

if __name__ == "__main__":
    main() 