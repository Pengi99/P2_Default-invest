"""
ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
==============================

ê¸°ëŠ¥:
1. ë°ì´í„° ë¡œë“œ ë° 5:3:2 ë¶„í• 
2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (20% ì´ìƒ ê²°ì¸¡ í–‰ ì‚­ì œ + median ëŒ€ì²´)
3. ìœˆì €ë¼ì´ì§• (ì–‘ ì˜† 0.5%)

Configë¥¼ í†µí•œ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì§€ì›
(ìŠ¤ì¼€ì¼ë§ ë° í”¼ì²˜ ì„ íƒ ì œì™¸)
"""

import pandas as pd
import numpy as np
import yaml
import pickle
import logging
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import warnings
warnings.filterwarnings('ignore')

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer, KNNImputer
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns


class DataPreprocessingPipeline:
    """
    ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
    
    Config íŒŒì¼ì„ í†µí•´ ëª¨ë“  ì„¤ì •ì„ ê´€ë¦¬í•˜ë©°,
    ë°ì´í„° ë¡œë“œë¶€í„° ìœˆì €ë¼ì´ì§•ê¹Œì§€ ì „ì²´ ê³¼ì •ì„ ìˆ˜í–‰ (ìŠ¤ì¼€ì¼ë§ ë° í”¼ì²˜ ì„ íƒ ì œì™¸)
    """
    
    def __init__(self, config_path: str):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            config_path: config YAML íŒŒì¼ ê²½ë¡œ
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.logger = self._setup_logging()
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.project_root = Path(__file__).parent.parent.parent
        
        # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        self.results = {
            'experiment_info': {},
            'data_info': {},
            'preprocessing_steps': {}
        }
        
        self.logger.info("ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤ (ìŠ¤ì¼€ì¼ë§ ë° í”¼ì²˜ ì„ íƒ ì œì™¸).")
    
    def _load_config(self) -> Dict:
        """Config íŒŒì¼ ë¡œë“œ"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    
    def _setup_logging(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger('DataPipeline')
        logger.setLevel(getattr(logging, self.config['logging']['level']))
        
        # í•¸ë“¤ëŸ¬ ì œê±° (ì¤‘ë³µ ë°©ì§€)
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬
        if self.config['logging']['save_to_file']:
            log_dir = Path(self.config['logging']['log_file']).parent
            log_dir.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.FileHandler(self.config['logging']['log_file'])
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ"""
        self.logger.info("ë°ì´í„° ë¡œë“œ ì‹œì‘")
        
        data_path = self.project_root / self.config['data']['input_path']
        df = pd.read_csv(data_path, dtype={'ê±°ë˜ì†Œì½”ë“œ': str})
        
        self.logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
        
        # ê¸°ë³¸ ì •ë³´ ì €ì¥
        self.results['data_info'] = {
            'original_shape': df.shape,
            'columns': list(df.columns),
            'target_distribution': df[self.config['feature_engineering']['target_column']].value_counts().to_dict(),
            'missing_data_summary': df.isnull().sum().to_dict()
        }
        
        return df
    
    def split_data(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ë˜ëŠ” ëœë¤)"""
        split_method = self.config['data_split']['split_method']
        
        if split_method == 'timeseries':
            return self._split_data_timeseries(df)
        elif split_method == 'random':
            return self._split_data_random(df)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„í•  ë°©ì‹: {split_method}")
    
    def _split_data_timeseries(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ì‹œê³„ì—´ ê¸°ë°˜ ë°ì´í„° ë¶„í• """
        self.logger.info("ì‹œê³„ì—´ ê¸°ë°˜ ë°ì´í„° ë¶„í•  ì‹œì‘")
        
        time_col = self.config['data_split']['timeseries']['time_column']
        train_end_year = self.config['data_split']['timeseries']['train_end_year']
        val_end_year = self.config['data_split']['timeseries']['val_end_year']
        target_col = self.config['feature_engineering']['target_column']
        
        # ì‹œê°„ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if time_col not in df.columns:
            raise ValueError(f"ì‹œê°„ ì»¬ëŸ¼ '{time_col}'ì´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ë…„ë„ë³„ ë¶„í• 
        train_df = df[df[time_col] <= train_end_year].copy()
        val_df = df[(df[time_col] > train_end_year) & (df[time_col] <= val_end_year)].copy()
        test_df = df[df[time_col] > val_end_year].copy()
        
        # ë¶„í•  ê²°ê³¼ í™•ì¸
        if len(train_df) == 0:
            raise ValueError(f"Train ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. train_end_year({train_end_year})ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        if len(val_df) == 0:
            raise ValueError(f"Validation ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. val_end_year({val_end_year})ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        if len(test_df) == 0:
            raise ValueError(f"Test ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. val_end_year({val_end_year}) ì´í›„ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì‹œê°„ ë²”ìœ„ ë¡œê¹…
        train_years = sorted(train_df[time_col].unique())
        val_years = sorted(val_df[time_col].unique())
        test_years = sorted(test_df[time_col].unique())
        
        self.logger.info(f"Train: {train_df.shape} ({train_years[0]}-{train_years[-1]}ë…„)")
        self.logger.info(f"Val: {val_df.shape} ({val_years[0]}-{val_years[-1]}ë…„)")
        self.logger.info(f"Test: {test_df.shape} ({test_years[0]}-{test_years[-1]}ë…„)")
        
        # ë¶„í•  ì •ë³´ ì €ì¥
        self.results['preprocessing_steps']['data_split'] = {
            'method': 'timeseries',
            'train_shape': train_df.shape,
            'val_shape': val_df.shape,
            'test_shape': test_df.shape,
            'train_years': train_years,
            'val_years': val_years,
            'test_years': test_years,
            'train_target_dist': train_df[target_col].value_counts().to_dict(),
            'val_target_dist': val_df[target_col].value_counts().to_dict(),
            'test_target_dist': test_df[target_col].value_counts().to_dict()
        }
        
        return train_df, val_df, test_df
    
    def _split_data_random(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ëœë¤ ê¸°ë°˜ ë°ì´í„° ë¶„í•  (ê¸°ì¡´ ë°©ì‹)"""
        self.logger.info("ëœë¤ ê¸°ë°˜ ë°ì´í„° ë¶„í•  ì‹œì‘ (5:3:2)")
        
        target_col = self.config['feature_engineering']['target_column']
        random_config = self.config['data_split']['random']
        
        # ë¨¼ì € trainê³¼ tempë¡œ ë¶„í•  (5:5)
        if random_config['stratify']:
            train_df, temp_df = train_test_split(
                df, 
                test_size=0.5,  # 50%ë¥¼ tempë¡œ
                random_state=random_config['random_state'],
                stratify=df[target_col]
            )
        else:
            train_df, temp_df = train_test_split(
                df,
                test_size=0.5,
                random_state=random_config['random_state']
            )
        
        # tempë¥¼ valê³¼ testë¡œ ë¶„í•  (3:2)
        val_ratio = random_config['val_ratio']
        test_ratio = random_config['test_ratio']
        val_size = val_ratio / (val_ratio + test_ratio)  # temp ë‚´ì—ì„œì˜ val ë¹„ìœ¨
        
        if random_config['stratify']:
            val_df, test_df = train_test_split(
                temp_df,
                test_size=(1-val_size),
                random_state=random_config['random_state'],
                stratify=temp_df[target_col]
            )
        else:
            val_df, test_df = train_test_split(
                temp_df,
                test_size=(1-val_size),
                random_state=random_config['random_state']
            )
        
        self.logger.info(f"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}")
        
        # ë¶„í•  ì •ë³´ ì €ì¥
        self.results['preprocessing_steps']['data_split'] = {
            'method': 'random',
            'train_shape': train_df.shape,
            'val_shape': val_df.shape,
            'test_shape': test_df.shape,
            'train_target_dist': train_df[target_col].value_counts().to_dict(),
            'val_target_dist': val_df[target_col].value_counts().to_dict(),
            'test_target_dist': test_df[target_col].value_counts().to_dict()
        }
        
        return train_df, val_df, test_df
    
    def handle_missing_data(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        self.logger.info("ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì‹œì‘")
        
        # í”¼ì²˜ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œ
        exclude_cols = self.config['feature_engineering']['exclude_columns'] + [self.config['feature_engineering']['target_column']]
        feature_cols = [col for col in train_df.columns if col not in exclude_cols]
        
        original_train_shape = train_df.shape
        original_val_shape = val_df.shape
        original_test_shape = test_df.shape
        
        # 1. 50% ì´ìƒ ê²°ì¸¡ì¹˜ì¸ í–‰ ì‚­ì œ
        threshold = self.config['missing_data']['row_missing_threshold']
        
        # ê° í–‰ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
        train_missing_rate = train_df[feature_cols].isnull().sum(axis=1) / len(feature_cols)
        val_missing_rate = val_df[feature_cols].isnull().sum(axis=1) / len(feature_cols)
        test_missing_rate = test_df[feature_cols].isnull().sum(axis=1) / len(feature_cols)
        
        # ì„ê³„ê°’ ì´í•˜ì¸ í–‰ë§Œ ìœ ì§€
        train_df = train_df[train_missing_rate <= threshold].copy()
        val_df = val_df[val_missing_rate <= threshold].copy()
        test_df = test_df[test_missing_rate <= threshold].copy()
        
        self.logger.info(f"í–‰ ì‚­ì œ í›„ - Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}")
        
        # 2. ê²°ì¸¡ê°’ ëŒ€ì²´
        imputation_method = self.config['missing_data']['imputation_method']
        
        if imputation_method == "median":
            imputer = SimpleImputer(strategy='median')
        elif imputation_method == "mean":
            imputer = SimpleImputer(strategy='mean')
        elif imputation_method == "mode":
            imputer = SimpleImputer(strategy='most_frequent')
        elif imputation_method == "knn":
            imputer = KNNImputer(n_neighbors=5)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ê²°ì¸¡ê°’ ëŒ€ì²´ ë°©ë²•: {imputation_method}")
        
        # Train ë°ì´í„°ë¡œ imputer í•™ìŠµ
        imputer.fit(train_df[feature_cols])
        
        # ëª¨ë“  ë°ì´í„°ì…‹ì— ì ìš©
        train_df[feature_cols] = imputer.transform(train_df[feature_cols])
        val_df[feature_cols] = imputer.transform(val_df[feature_cols])
        test_df[feature_cols] = imputer.transform(test_df[feature_cols])
        
        self.logger.info(f"ê²°ì¸¡ê°’ ëŒ€ì²´ ì™„ë£Œ ({imputation_method})")
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì •ë³´ ì €ì¥
        self.results['preprocessing_steps']['missing_data'] = {
            'method': imputation_method,
            'threshold': threshold,
            'before_shape': {
                'train': original_train_shape,
                'val': original_val_shape,
                'test': original_test_shape
            },
            'after_shape': {
                'train': train_df.shape,
                'val': val_df.shape,
                'test': test_df.shape
            },
            'rows_removed': {
                'train': original_train_shape[0] - train_df.shape[0],
                'val': original_val_shape[0] - val_df.shape[0],
                'test': original_test_shape[0] - test_df.shape[0]
            }
        }
        
        return train_df, val_df, test_df
    
    def apply_winsorization(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ìœˆì €ë¼ì´ì§• ì ìš©"""
        if not self.config['outlier_treatment']['enabled']:
            self.logger.info("ìœˆì €ë¼ì´ì§•ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return train_df, val_df, test_df
        
        self.logger.info("ìœˆì €ë¼ì´ì§• ì ìš© ì‹œì‘")
        
        # í”¼ì²˜ ì»¬ëŸ¼ë“¤ë§Œ ì¶”ì¶œ
        exclude_cols = self.config['feature_engineering']['exclude_columns'] + [self.config['feature_engineering']['target_column']]
        feature_cols = [col for col in train_df.columns if col not in exclude_cols]
        
        lower_pct = self.config['outlier_treatment']['winsorization']['lower_percentile']
        upper_pct = self.config['outlier_treatment']['winsorization']['upper_percentile']
        
        # Train ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ê³„ê°’ ê³„ì‚°
        winsor_limits = {}
        for col in feature_cols:
            lower_limit = train_df[col].quantile(lower_pct)
            upper_limit = train_df[col].quantile(upper_pct)
            winsor_limits[col] = (lower_limit, upper_limit)
        
        # ìœˆì €ë¼ì´ì§• ì ìš©
        for df_name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:
            for col in feature_cols:
                lower_limit, upper_limit = winsor_limits[col]
                df[col] = np.clip(df[col], lower_limit, upper_limit)
        
        self.logger.info(f"ìœˆì €ë¼ì´ì§• ì™„ë£Œ (í•˜ìœ„ {lower_pct*100}%, ìƒìœ„ {upper_pct*100}%)")
        
        # ìœˆì €ë¼ì´ì§• ì •ë³´ ì €ì¥
        self.results['preprocessing_steps']['winsorization'] = {
            'enabled': True,
            'lower_percentile': lower_pct,
            'upper_percentile': upper_pct,
            'limits': {col: limits for col, limits in winsor_limits.items()}
        }
        
        return train_df, val_df, test_df
    

    
    def save_results(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame):
        """ê²°ê³¼ ì €ì¥"""
        self.logger.info("ê²°ê³¼ ì €ì¥ ì‹œì‘")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = self.project_root / self.config['data']['output_dir']
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„± ì—¬ë¶€ í™•ì¸
        create_subdirectory = self.config['experiment'].get('create_subdirectory', True)
        
        if create_subdirectory:
            # ì‹¤í—˜ ì´ë¦„ ìƒì„±
            experiment_name = self.config['experiment']['name']
            if experiment_name is None:
                experiment_name = f"preprocessing_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            experiment_dir = output_dir / experiment_name
            experiment_dir.mkdir(parents=True, exist_ok=True)
        else:
            # data/finalì— ì§ì ‘ ì €ì¥
            experiment_dir = output_dir
        
        # 1. ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        if self.config['output']['save_processed_data']:
            self._save_processed_data(train_df, val_df, test_df, experiment_dir)
        

        
        # 2. ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        experiment_name = "preprocessing" if not create_subdirectory else self.config['experiment']['name']
        self.results['experiment_info'] = {
            'name': experiment_name,
            'config_path': str(self.config_path),
            'timestamp': datetime.now().isoformat(),
            'version': self.config['experiment']['version'],
            'description': self.config['experiment']['description']
        }
        
        # ê²°ê³¼ë¥¼ JSON í˜•íƒœë¡œ ì €ì¥ (ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„± ì‹œì—ë§Œ)
        if create_subdirectory:
            import json
            with open(experiment_dir / "experiment_results.json", 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
        
        # 3. Config íŒŒì¼ ë³µì‚¬ (ì„œë¸Œë””ë ‰í† ë¦¬ ìƒì„± ì‹œì—ë§Œ)
        if self.config['output']['save_config_log'] and create_subdirectory:
            import shutil
            shutil.copy2(self.config_path, experiment_dir / "config.yaml")
        
        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {experiment_dir}")
        
        return experiment_dir
    
    def _save_processed_data(self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame, experiment_dir: Path):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        file_naming = self.config['output']['file_naming']
        separate_features_target = file_naming.get('separate_features_target', False)
        prefix = file_naming.get('prefix', "")
        
        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ì»¬ëŸ¼ ë¶„ë¦¬
        exclude_cols = self.config['feature_engineering']['exclude_columns'] + [self.config['feature_engineering']['target_column']]
        feature_cols = [col for col in train_df.columns if col not in exclude_cols]
        target_col = self.config['feature_engineering']['target_column']
        
        datasets = {
            'train': train_df,
            'val': val_df,
            'test': test_df
        }
        
        for split_name, df in datasets.items():
            if separate_features_target:
                # X, y ë¶„ë¦¬ ì €ì¥
                feature_filename = prefix + file_naming['feature_format'].format(split=split_name)
                target_filename = prefix + file_naming['target_format'].format(split=split_name)
                
                # X ì €ì¥ (í”¼ì²˜ë§Œ)
                X = df[feature_cols]
                X.to_csv(experiment_dir / feature_filename, index=False)
                
                # y ì €ì¥ (íƒ€ê²Ÿë§Œ)
                y = df[target_col]
                y.to_csv(experiment_dir / target_filename, index=False)
                
                self.logger.info(f"ì €ì¥ ì™„ë£Œ: {feature_filename}, {target_filename}")
            else:
                # í†µí•© íŒŒì¼ë¡œ ì €ì¥
                combined_filename = prefix + file_naming['combined_format'].format(split=split_name)
                df.to_csv(experiment_dir / combined_filename, index=False)
                
                self.logger.info(f"ì €ì¥ ì™„ë£Œ: {combined_filename}")
    
    def generate_report(self, experiment_dir: Path):
        """ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.config['output']['generate_report']:
            return
        
        self.logger.info("ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        report_content = self._create_report_content()
        
        formats = self.config['output']['report_format']
        
        if 'txt' in formats:
            with open(experiment_dir / "preprocessing_report.txt", 'w', encoding='utf-8') as f:
                f.write(report_content)
        
        if 'html' in formats:
            html_content = self._convert_to_html(report_content)
            with open(experiment_dir / "preprocessing_report.html", 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        self.logger.info("ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def _create_report_content(self) -> str:
        """ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„±"""
        
        split_info = self.results['preprocessing_steps']['data_split']
        split_method = split_info['method']
        
        # ë°ì´í„° ë¶„í•  ì •ë³´ ìƒì„±
        if split_method == 'timeseries':
            split_details = f"""1. ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ê¸°ë°˜)
   - ë°©ë²•: ì‹œê³„ì—´ ë¶„í• 
   - Train: {split_info['train_shape']} ({split_info['train_years'][0]}-{split_info['train_years'][-1]}ë…„)
   - Validation: {split_info['val_shape']} ({split_info['val_years'][0]}-{split_info['val_years'][-1]}ë…„)
   - Test: {split_info['test_shape']} ({split_info['test_years'][0]}-{split_info['test_years'][-1]}ë…„)"""
        else:
            split_details = f"""1. ë°ì´í„° ë¶„í•  (ëœë¤ ê¸°ë°˜)
   - ë°©ë²•: ëœë¤ ë¶„í•  (5:3:2)
   - Train: {split_info['train_shape']}
   - Validation: {split_info['val_shape']}
   - Test: {split_info['test_shape']}"""
        
        report = f"""
ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼ ë¦¬í¬íŠ¸ (ìŠ¤ì¼€ì¼ë§ ë° í”¼ì²˜ ì„ íƒ ì œì™¸)
=================================================================

ì‹¤í—˜ ì •ë³´
--------
- ì‹¤í—˜ëª…: {self.results['experiment_info']['name']}
- ì‹¤í–‰ ì‹œê°„: {self.results['experiment_info']['timestamp']}
- ë²„ì „: {self.results['experiment_info']['version']}
- ì„¤ëª…: {self.results['experiment_info']['description']}

ì›ë³¸ ë°ì´í„° ì •ë³´
--------------
- ë°ì´í„° í¬ê¸°: {self.results['data_info']['original_shape']}
- íƒ€ê²Ÿ ë¶„í¬: {self.results['data_info']['target_distribution']}

ì „ì²˜ë¦¬ ë‹¨ê³„ë³„ ê²°ê³¼
----------------

{split_details}

2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
   - ë°©ë²•: {self.results['preprocessing_steps']['missing_data']['method']}
   - ì„ê³„ê°’: {self.results['preprocessing_steps']['missing_data']['threshold']}
   - ì œê±°ëœ í–‰ìˆ˜: {self.results['preprocessing_steps']['missing_data']['rows_removed']}

3. ìœˆì €ë¼ì´ì§•
   - ì ìš© ì—¬ë¶€: {self.results['preprocessing_steps']['winsorization']['enabled']}
   - í•˜ìœ„ ì„ê³„ê°’: {self.results['preprocessing_steps']['winsorization']['lower_percentile']}
   - ìƒìœ„ ì„ê³„ê°’: {self.results['preprocessing_steps']['winsorization']['upper_percentile']}

ì²˜ë¦¬ ì™„ë£Œ
--------
- ëª¨ë“  í”¼ì²˜ê°€ ìœ ì§€ë¨ (í”¼ì²˜ ì„ íƒ ì—†ìŒ)
- ë°ì´í„°ê°€ ëª¨ë¸ë§ ì¤€ë¹„ ì™„ë£Œ
"""

        return report
    
    def _convert_to_html(self, text_content: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ë°ì´í„° ì „ì²˜ë¦¬ ë¦¬í¬íŠ¸</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        h1 {{ color: #333; }}
        h2 {{ color: #666; border-bottom: 1px solid #ccc; }}
        pre {{ background-color: #f5f5f5; padding: 10px; }}
    </style>
</head>
<body>
    <pre>{text_content}</pre>
</body>
</html>
"""
        return html_content
    
    def run_pipeline(self) -> str:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        self.logger.info("=== ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            df = self.load_data()
            
            # 2. ë°ì´í„° ë¶„í• 
            train_df, val_df, test_df = self.split_data(df)
            
            # 3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            train_df, val_df, test_df = self.handle_missing_data(train_df, val_df, test_df)
            
            # 4. ìœˆì €ë¼ì´ì§•
            train_df, val_df, test_df = self.apply_winsorization(train_df, val_df, test_df)
            
            # 5. ê²°ê³¼ ì €ì¥
            experiment_dir = self.save_results(train_df, val_df, test_df)
            
            # 6. ë¦¬í¬íŠ¸ ìƒì„±
            self.generate_report(experiment_dir)
            
            self.logger.info("=== ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
            
            return str(experiment_dir)
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸')
    parser.add_argument('--config', '-c', type=str, 
                       default='config/preprocessing_config.yaml',
                       help='Config íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = DataPreprocessingPipeline(args.config)
    experiment_dir = pipeline.run_pipeline()
    
    print(f"\nâœ… ì „ì²˜ë¦¬ ì™„ë£Œ! (ìŠ¤ì¼€ì¼ë§ ë° í”¼ì²˜ ì„ íƒ ì œì™¸)")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {experiment_dir}")
    print(f"ğŸ“Š ëª¨ë“  í”¼ì²˜ ìœ ì§€ë¨")
    print(f"ğŸ¯ ë°ì´í„° ì²˜ë¦¬: ì™„ë£Œ")


if __name__ == "__main__":
    main()