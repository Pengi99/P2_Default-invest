import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

print("=== ìµœì¢… ëª¨ë¸ë§ìš© ë°ì´í„°ì…‹ ìƒì„± ===")

# 1. ë°ì´í„° ë¡œë“œ
print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ")
print("="*50)

# ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ
df = pd.read_csv('data/processed/FS_ratio_flow_labeled.csv', dtype={'ê±°ë˜ì†Œì½”ë“œ': str})
print(f"ë¼ë²¨ë§ëœ ë°ì´í„°: {df.shape}")
print(f"ë¶€ì‹¤(default=1): {(df['default']==1).sum():,}ê°œ ({(df['default']==1).sum()/len(df)*100:.2f}%)")
print(f"ì •ìƒ(default=0): {(df['default']==0).sum():,}ê°œ ({(df['default']==0).sum()/len(df)*100:.2f}%)")

# 2. íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬
print("\n2ï¸âƒ£ íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬")
print("="*50)

# ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼
meta_columns = ['íšŒì‚¬ëª…', 'ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„']

# ì¬ë¬´ë¹„ìœ¨ ì»¬ëŸ¼ (íŠ¹ì„±)
feature_columns = [col for col in df.columns 
                  if col not in meta_columns + ['default']]

print(f"íŠ¹ì„± ì»¬ëŸ¼: {len(feature_columns)}ê°œ")
print(f"íŠ¹ì„± ëª©ë¡: {feature_columns}")

# ê²°ì¸¡ê°’ í™•ì¸
print(f"\nê²°ì¸¡ê°’ í˜„í™©:")
missing_info = []
for col in feature_columns:
    missing_count = df[col].isna().sum()
    missing_pct = missing_count / len(df) * 100
    if missing_count > 0:
        missing_info.append((col, missing_count, missing_pct))
        print(f"  {col}: {missing_count:,}ê°œ ({missing_pct:.2f}%)")

if not missing_info:
    print("  ê²°ì¸¡ê°’ ì—†ìŒ âœ…")

# 3. ìŠ¤ì¼€ì¼ë§ ì ìš©
print("\n3ï¸âƒ£ ìŠ¤ì¼€ì¼ë§ ì ìš©")
print("="*50)

# ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ (ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
robust_scaler_columns = [
    'ROA', 'CFO_TD', 'RE_TA', 'EBIT_TA', 'MVE_TL', 'S_TA', 
    'CLCA', 'OENEG', 'CR', 'CFO_TA', 'RET_3M', 'RET_9M', 'MB'
]

standard_scaler_columns = [
    'TLTA', 'WC_TA', 'SIGMA', 'TLMTA'
]

print(f"RobustScaler ì ìš©: {len(robust_scaler_columns)}ê°œ ì»¬ëŸ¼")
print(f"StandardScaler ì ìš©: {len(standard_scaler_columns)}ê°œ ì»¬ëŸ¼")

# ìŠ¤ì¼€ì¼ë§ ì „ ë°ì´í„° ë³µì‚¬
df_scaled = df.copy()

# RobustScaler ì ìš©
robust_scaler = RobustScaler()
robust_cols_available = [col for col in robust_scaler_columns if col in df.columns]

if robust_cols_available:
    print(f"\nRobustScaler ì ìš© ì¤‘... ({len(robust_cols_available)}ê°œ ì»¬ëŸ¼)")
    
    # ê²°ì¸¡ê°’ì´ ì—†ëŠ” ë°ì´í„°ë§Œìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í›ˆë ¨
    robust_data = df[robust_cols_available].dropna()
    robust_scaler.fit(robust_data)
    
    # ì „ì²´ ë°ì´í„°ì— ì ìš© (ê²°ì¸¡ê°’ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)
    for col in robust_cols_available:
        non_null_mask = df[col].notna()
        if non_null_mask.sum() > 0:
            df_scaled.loc[non_null_mask, col] = robust_scaler.fit_transform(
                df.loc[non_null_mask, [col]]
            ).flatten()
    
    print(f"  âœ… RobustScaler ì ìš© ì™„ë£Œ")

# StandardScaler ì ìš©
standard_scaler = StandardScaler()
standard_cols_available = [col for col in standard_scaler_columns if col in df.columns]

if standard_cols_available:
    print(f"\nStandardScaler ì ìš© ì¤‘... ({len(standard_cols_available)}ê°œ ì»¬ëŸ¼)")
    
    # ê²°ì¸¡ê°’ì´ ì—†ëŠ” ë°ì´í„°ë§Œìœ¼ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í›ˆë ¨
    standard_data = df[standard_cols_available].dropna()
    standard_scaler.fit(standard_data)
    
    # ì „ì²´ ë°ì´í„°ì— ì ìš©
    for col in standard_cols_available:
        non_null_mask = df[col].notna()
        if non_null_mask.sum() > 0:
            df_scaled.loc[non_null_mask, col] = standard_scaler.fit_transform(
                df.loc[non_null_mask, [col]]
            ).flatten()
    
    print(f"  âœ… StandardScaler ì ìš© ì™„ë£Œ")

# 4. ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ íŠ¹ì„± ê³ ë ¤)
print("\n4ï¸âƒ£ ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ ê³ ë ¤)")
print("="*50)

# íšŒê³„ë…„ë„ì—ì„œ ì—°ë„ ì¶”ì¶œ
df_scaled['year'] = pd.to_datetime(df_scaled['íšŒê³„ë…„ë„'], format='%Y/%m').dt.year

# ì‹œê³„ì—´ ê¸°ë°˜ ë¶„í•  (Look-ahead Bias ë°©ì§€)
# 2012-2019: í›ˆë ¨ìš© (8ë…„)
# 2020-2021: ê²€ì¦ìš© (2ë…„) 
# 2022-2023: í…ŒìŠ¤íŠ¸ìš© (2ë…„)

train_years = [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
val_years = [2020, 2021]
test_years = [2022, 2023]

train_mask = df_scaled['year'].isin(train_years)
val_mask = df_scaled['year'].isin(val_years)
test_mask = df_scaled['year'].isin(test_years)

train_df = df_scaled[train_mask].copy()
val_df = df_scaled[val_mask].copy()
test_df = df_scaled[test_mask].copy()

print(f"í›ˆë ¨ ë°ì´í„° ({min(train_years)}-{max(train_years)}): {len(train_df):,}ê°œ")
print(f"  - ë¶€ì‹¤: {(train_df['default']==1).sum():,}ê°œ ({(train_df['default']==1).sum()/len(train_df)*100:.2f}%)")
print(f"  - ì •ìƒ: {(train_df['default']==0).sum():,}ê°œ")

print(f"ê²€ì¦ ë°ì´í„° ({min(val_years)}-{max(val_years)}): {len(val_df):,}ê°œ")
print(f"  - ë¶€ì‹¤: {(val_df['default']==1).sum():,}ê°œ ({(val_df['default']==1).sum()/len(val_df)*100:.2f}%)")
print(f"  - ì •ìƒ: {(val_df['default']==0).sum():,}ê°œ")

print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ({min(test_years)}-{max(test_years)}): {len(test_df):,}ê°œ")
print(f"  - ë¶€ì‹¤: {(test_df['default']==1).sum():,}ê°œ ({(test_df['default']==1).sum()/len(test_df)*100:.2f}%)")
print(f"  - ì •ìƒ: {(test_df['default']==0).sum():,}ê°œ")

# 5. ìµœì¢… íŠ¹ì„± í–‰ë ¬ ë° íƒ€ê²Ÿ ë²¡í„° ìƒì„±
print("\n5ï¸âƒ£ ìµœì¢… íŠ¹ì„± í–‰ë ¬ ë° íƒ€ê²Ÿ ë²¡í„° ìƒì„±")
print("="*50)

def create_feature_target(data, feature_cols):
    """íŠ¹ì„± í–‰ë ¬ê³¼ íƒ€ê²Ÿ ë²¡í„°ë¥¼ ìƒì„±í•˜ê³  ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    X = data[feature_cols].copy()
    y = data['default'].copy()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´)
    for col in feature_cols:
        if X[col].isna().sum() > 0:
            median_val = X[col].median()
            X[col] = X[col].fillna(median_val)
            print(f"  {col}: ê²°ì¸¡ê°’ {X[col].isna().sum()}ê°œë¥¼ ì¤‘ì•™ê°’({median_val:.4f})ìœ¼ë¡œ ëŒ€ì²´")
    
    return X, y

# í›ˆë ¨ ë°ì´í„°
X_train, y_train = create_feature_target(train_df, feature_columns)
print(f"í›ˆë ¨ íŠ¹ì„± í–‰ë ¬: {X_train.shape}")

# ê²€ì¦ ë°ì´í„°  
X_val, y_val = create_feature_target(val_df, feature_columns)
print(f"ê²€ì¦ íŠ¹ì„± í–‰ë ¬: {X_val.shape}")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
X_test, y_test = create_feature_target(test_df, feature_columns)
print(f"í…ŒìŠ¤íŠ¸ íŠ¹ì„± í–‰ë ¬: {X_test.shape}")

# 6. ë°ì´í„° ì €ì¥
print("\n6ï¸âƒ£ ìµœì¢… ë°ì´í„° ì €ì¥")
print("="*50)

# ì „ì²´ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì €ì¥
df_scaled_final = df_scaled.drop(columns=['year'])
df_scaled_final.to_csv('data/processed/FS_ratio_flow_scaled.csv', index=False, encoding='utf-8-sig')
print(f"âœ… ìŠ¤ì¼€ì¼ë§ëœ ì „ì²´ ë°ì´í„°: data/processed/FS_ratio_flow_scaled.csv")

# ë¶„í• ëœ ë°ì´í„° ì €ì¥
datasets = {
    'train': (train_df, X_train, y_train),
    'val': (val_df, X_val, y_val), 
    'test': (test_df, X_test, y_test)
}

for split_name, (df_split, X_split, y_split) in datasets.items():
    # ë©”íƒ€ë°ì´í„° í¬í•¨ ì „ì²´ ë°ì´í„°
    df_split_final = df_split.drop(columns=['year'])
    df_split_final.to_csv(f'data/processed/{split_name}_data.csv', index=False, encoding='utf-8-sig')
    
    # íŠ¹ì„± í–‰ë ¬ë§Œ
    X_split.to_csv(f'data/processed/X_{split_name}.csv', index=False, encoding='utf-8-sig')
    
    # íƒ€ê²Ÿ ë²¡í„°ë§Œ
    y_split.to_csv(f'data/processed/y_{split_name}.csv', index=False, encoding='utf-8-sig')
    
    print(f"âœ… {split_name.upper()} ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ")

# 7. ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ì €ì¥
print("\n7ï¸âƒ£ ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ìƒì„±")
print("="*50)

summary_info = {
    'dataset_info': {
        'total_samples': len(df_scaled_final),
        'total_features': len(feature_columns),
        'default_rate': (df_scaled_final['default']==1).sum() / len(df_scaled_final),
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df)
    },
    'feature_info': {
        'feature_columns': feature_columns,
        'robust_scaled': robust_cols_available,
        'standard_scaled': standard_cols_available,
        'meta_columns': meta_columns
    },
    'split_info': {
        'train_years': train_years,
        'val_years': val_years,
        'test_years': test_years,
        'split_method': 'time_based'
    }
}

import json
with open('data/processed/dataset_info.json', 'w', encoding='utf-8') as f:
    json.dump(summary_info, f, ensure_ascii=False, indent=2)

print(f"âœ… ë°ì´í„°ì…‹ ì •ë³´: data/processed/dataset_info.json")

# 8. ìµœì¢… ìš”ì•½
print("\n8ï¸âƒ£ ìµœì¢… ìš”ì•½")
print("="*50)

print(f"ğŸ“Š ìƒì„±ëœ íŒŒì¼ë“¤:")
print(f"  1. FS_ratio_flow_scaled.csv - ì „ì²´ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ({len(df_scaled_final):,}ê°œ)")
print(f"  2. train_data.csv - í›ˆë ¨ ë°ì´í„° ({len(train_df):,}ê°œ)")
print(f"  3. val_data.csv - ê²€ì¦ ë°ì´í„° ({len(val_df):,}ê°œ)")
print(f"  4. test_data.csv - í…ŒìŠ¤íŠ¸ ë°ì´í„° ({len(test_df):,}ê°œ)")
print(f"  5. X_train.csv, y_train.csv - í›ˆë ¨ìš© íŠ¹ì„±/íƒ€ê²Ÿ")
print(f"  6. X_val.csv, y_val.csv - ê²€ì¦ìš© íŠ¹ì„±/íƒ€ê²Ÿ")
print(f"  7. X_test.csv, y_test.csv - í…ŒìŠ¤íŠ¸ìš© íŠ¹ì„±/íƒ€ê²Ÿ")
print(f"  8. dataset_info.json - ë°ì´í„°ì…‹ ë©”íƒ€ì •ë³´")

print(f"\nğŸ¯ ë°ì´í„° íŠ¹ì„±:")
print(f"  - ì´ íŠ¹ì„±: {len(feature_columns)}ê°œ")
print(f"  - ë¶€ì‹¤ ë¹„ìœ¨: {(df_scaled_final['default']==1).sum()/len(df_scaled_final)*100:.2f}%")
print(f"  - ì‹œê³„ì—´ ë¶„í• : Look-ahead Bias ë°©ì§€")
print(f"  - ìŠ¤ì¼€ì¼ë§: RobustScaler({len(robust_cols_available)}ê°œ), StandardScaler({len(standard_cols_available)}ê°œ)")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"  1. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (SMOTE, ê°€ì¤‘ì¹˜ ì¡°ì •)")
print(f"  2. ëª¨ë¸ í›ˆë ¨ (XGBoost, Random Forest, Logistic Regression)")
print(f"  3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
print(f"  4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (AUC, Precision, Recall, F1)")
print(f"  5. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„")

print(f"\nâœ… ìµœì¢… ëª¨ë¸ë§ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!") 