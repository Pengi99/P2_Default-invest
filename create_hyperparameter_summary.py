"""
í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš”ì•½ ì •ë¦¬ê¸°
=========================

JSON íŒŒì¼ì„ ì½ì–´ì„œ ë” ë³´ê¸° ì¢‹ê²Œ ìš”ì•½ ì •ë¦¬í•˜ê³ ,
ëª¨ë¸ë³„ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë¹„êµ í…Œì´ë¸”ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path


def clean_nan_values(obj):
    """NaN ê°’ë“¤ì„ Noneìœ¼ë¡œ ë³€ê²½"""
    if isinstance(obj, dict):
        return {k: clean_nan_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_nan_values(v) for v in obj]
    elif isinstance(obj, float) and np.isnan(obj):
        return None
    else:
        return obj


def create_model_comparison_table(hyperparams_data):
    """ëª¨ë¸ë³„ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹„êµ í…Œì´ë¸” ìƒì„±"""
    
    # ëª¨ë¸ë³„ ë°ì´í„° ìˆ˜ì§‘
    models_data = []
    
    for model_key, model_info in hyperparams_data['detailed_hyperparameters'].items():
        data_type = model_info['file_info']['data_type']
        model_type = model_info['model_type']
        hyperparams = model_info['hyperparameters']
        
        row = {
            'Model_Key': model_key,
            'Model_Type': model_type,
            'Data_Type': data_type,
            'File_Size_MB': model_info['file_info']['file_size_mb']
        }
        
        # ëª¨ë¸ë³„ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        if model_type == 'logistic_regression':
            row.update({
                'C': hyperparams.get('C'),
                'penalty': hyperparams.get('penalty'),
                'solver': hyperparams.get('solver'),
                'max_iter': hyperparams.get('max_iter'),
                'class_weight': hyperparams.get('class_weight')
            })
            
        elif model_type == 'random_forest':
            row.update({
                'n_estimators': hyperparams.get('n_estimators'),
                'max_depth': hyperparams.get('max_depth'),
                'min_samples_split': hyperparams.get('min_samples_split'),
                'min_samples_leaf': hyperparams.get('min_samples_leaf'),
                'max_features': hyperparams.get('max_features'),
                'class_weight': hyperparams.get('class_weight')
            })
            
        elif model_type == 'xgboost':
            row.update({
                'n_estimators': hyperparams.get('n_estimators'),
                'max_depth': hyperparams.get('max_depth'),
                'learning_rate': hyperparams.get('learning_rate'),
                'subsample': hyperparams.get('subsample'),
                'colsample_bytree': hyperparams.get('colsample_bytree'),
                'reg_alpha': hyperparams.get('reg_alpha'),
                'reg_lambda': hyperparams.get('reg_lambda'),
                'scale_pos_weight': hyperparams.get('scale_pos_weight')
            })
        
        # íŠ¹ì„± ê°œìˆ˜ ì •ë³´
        if 'n_features' in model_info:
            row['n_features'] = model_info['n_features']
            
        models_data.append(row)
    
    return pd.DataFrame(models_data)


def create_summary_by_model_type(df):
    """ëª¨ë¸ íƒ€ì…ë³„ ìš”ì•½ í†µê³„"""
    summary = {}
    
    for model_type in df['Model_Type'].unique():
        if model_type == 'unknown':  # ì•™ìƒë¸” ëª¨ë¸ ì œì™¸
            continue
            
        model_df = df[df['Model_Type'] == model_type]
        
        if model_type == 'logistic_regression':
            summary[model_type] = {
                'count': len(model_df),
                'avg_file_size_mb': model_df['File_Size_MB'].mean(),
                'C_range': f"{model_df['C'].min():.4f} - {model_df['C'].max():.4f}",
                'penalties': model_df['penalty'].unique().tolist(),
                'solvers': model_df['solver'].unique().tolist(),
                'max_iter_range': f"{model_df['max_iter'].min()} - {model_df['max_iter'].max()}"
            }
            
        elif model_type == 'random_forest':
            summary[model_type] = {
                'count': len(model_df),
                'avg_file_size_mb': model_df['File_Size_MB'].mean(),
                'n_estimators_range': f"{model_df['n_estimators'].min()} - {model_df['n_estimators'].max()}",
                'max_depth_range': f"{model_df['max_depth'].min()} - {model_df['max_depth'].max()}",
                'max_features_range': f"{model_df['max_features'].min():.3f} - {model_df['max_features'].max():.3f}",
                'min_samples_split_range': f"{model_df['min_samples_split'].min()} - {model_df['min_samples_split'].max()}",
                'min_samples_leaf_range': f"{model_df['min_samples_leaf'].min()} - {model_df['min_samples_leaf'].max()}"
            }
            
        elif model_type == 'xgboost':
            summary[model_type] = {
                'count': len(model_df),
                'avg_file_size_mb': model_df['File_Size_MB'].mean(),
                'n_estimators_range': f"{model_df['n_estimators'].min()} - {model_df['n_estimators'].max()}",
                'max_depth_range': f"{model_df['max_depth'].min()} - {model_df['max_depth'].max()}",
                'learning_rate_range': f"{model_df['learning_rate'].min():.4f} - {model_df['learning_rate'].max():.4f}",
                'scale_pos_weight_range': f"{model_df['scale_pos_weight'].min():.1f} - {model_df['scale_pos_weight'].max():.1f}",
                'reg_alpha_range': f"{model_df['reg_alpha'].min():.2f} - {model_df['reg_alpha'].max():.2f}",
                'reg_lambda_range': f"{model_df['reg_lambda'].min():.2f} - {model_df['reg_lambda'].max():.2f}"
            }
    
    return summary


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš”ì•½ ì •ë¦¬ ì‹œì‘")
    
    # JSON íŒŒì¼ ì½ê¸°
    json_file = "final_models_hyperparameters.json"
    
    if not Path(json_file).exists():
        print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        return
    
    with open(json_file, 'r', encoding='utf-8') as f:
        hyperparams_data = json.load(f)
    
    # NaN ê°’ ì •ë¦¬
    hyperparams_data = clean_nan_values(hyperparams_data)
    
    # ì •ë¦¬ëœ JSON íŒŒì¼ ë‹¤ì‹œ ì €ì¥
    with open("final_models_hyperparameters_cleaned.json", 'w', encoding='utf-8') as f:
        json.dump(hyperparams_data, f, indent=2, ensure_ascii=False)
    
    print("âœ… NaN ê°’ ì •ë¦¬ ì™„ë£Œ: final_models_hyperparameters_cleaned.json")
    
    # ë¹„êµ í…Œì´ë¸” ìƒì„±
    comparison_df = create_model_comparison_table(hyperparams_data)
    
    # CSV íŒŒì¼ë¡œ ì €ì¥
    comparison_df.to_csv("model_hyperparameters_comparison.csv", index=False, encoding='utf-8-sig')
    print("âœ… ë¹„êµ í…Œì´ë¸” ì €ì¥: model_hyperparameters_comparison.csv")
    
    # ëª¨ë¸ íƒ€ì…ë³„ ìš”ì•½
    summary = create_summary_by_model_type(comparison_df)
    
    # ìš”ì•½ JSON ì €ì¥
    with open("model_hyperparameters_summary.json", 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print("âœ… ìš”ì•½ í†µê³„ ì €ì¥: model_hyperparameters_summary.json")
    
    # ì½˜ì†” ì¶œë ¥
    print("\nğŸ“‹ ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìš”ì•½:")
    print("="*60)
    
    for model_type, stats in summary.items():
        print(f"\nğŸ”¹ {model_type.upper()} ({stats['count']}ê°œ ëª¨ë¸)")
        print(f"   í‰ê·  íŒŒì¼ í¬ê¸°: {stats['avg_file_size_mb']:.2f}MB")
        
        for key, value in stats.items():
            if key not in ['count', 'avg_file_size_mb']:
                print(f"   {key}: {value}")
    
    # ë°ì´í„° íƒ€ì…ë³„ ë¶„í¬
    print(f"\nğŸ“Š ë°ì´í„° íƒ€ì…ë³„ ëª¨ë¸ ë¶„í¬:")
    data_type_counts = comparison_df['Data_Type'].value_counts()
    for data_type, count in data_type_counts.items():
        print(f"   {data_type}: {count}ê°œ")
    
    # íŠ¹ì„± ê°œìˆ˜ ë¶„í¬
    print(f"\nğŸ” ì‚¬ìš©ëœ íŠ¹ì„± ê°œìˆ˜:")
    feature_counts = comparison_df['n_features'].value_counts().sort_index()
    for n_features, count in feature_counts.items():
        print(f"   {n_features}ê°œ íŠ¹ì„±: {count}ê°œ ëª¨ë¸")
    
    print(f"\nğŸ’¾ ì´ ëª¨ë¸ íŒŒì¼ í¬ê¸°: {comparison_df['File_Size_MB'].sum():.2f}MB")
    
    return comparison_df, summary


if __name__ == "__main__":
    main()