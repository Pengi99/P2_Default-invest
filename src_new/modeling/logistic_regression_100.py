"""
ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ - 100% ì™„ì„±ë„ ë°ì´í„°ìš© Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
"""

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_curve, auc
)
import optuna
from optuna.samplers import TPESampler
import joblib

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic' if plt.matplotlib.get_backend() != 'Agg' else 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class LogisticRegressionOptimizer:
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.best_model = None
        self.best_params = None
        self.best_score = None
        self.study = None
        
    def objective(self, trial, X_train, y_train, cv_folds=5):
        """Optuna ëª©ì  í•¨ìˆ˜"""
        # penalty ì„ íƒ
        penalty = trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet'])
        
        # penaltyì— ë”°ë¥¸ solver ì„¤ì •
        if penalty == 'l1':
            solver = 'liblinear'
        elif penalty == 'l2':
            solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs', 'saga'])
        else:  # elasticnet
            solver = 'saga'
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
        params = {
            'penalty': penalty,
            'C': trial.suggest_float('C', 1e-4, 100, log=True),
            'solver': solver,
            'max_iter': trial.suggest_int('max_iter', 100, 2000),
            'random_state': self.random_state
        }
        
        # elasticnetì¸ ê²½ìš° l1_ratio ì¶”ê°€
        if penalty == 'elasticnet':
            params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.1, 0.9)
        
        # ëª¨ë¸ ìƒì„±
        model = LogisticRegression(**params)
        
        # Stratified K-fold Cross Validation
        skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=self.random_state)
        
        # AUC ì ìˆ˜ ê³„ì‚°
        auc_scores = cross_val_score(
            model, X_train, y_train, 
            cv=skf, scoring='roc_auc', n_jobs=-1
        )
        
        return auc_scores.mean()
    
    def optimize(self, X_train, y_train, n_trials=100, cv_folds=5):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print("ğŸ” ë¡œì§€ìŠ¤í‹± íšŒê·€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘")
        print("="*60)
        
        # Optuna study ìƒì„±
        sampler = TPESampler(seed=self.random_state)
        self.study = optuna.create_study(
            direction='maximize',
            sampler=sampler,
            study_name='logistic_regression_100_optimization'
        )
        
        # ìµœì í™” ì‹¤í–‰
        self.study.optimize(
            lambda trial: self.objective(trial, X_train, y_train, cv_folds),
            n_trials=n_trials,
            show_progress_bar=True
        )
        
        # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥
        self.best_params = self.study.best_params
        self.best_score = self.study.best_value
        
        print(f"âœ… ìµœì í™” ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì  AUC: {self.best_score:.4f}")
        print(f"ğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
        for key, value in self.best_params.items():
            print(f"   {key}: {value}")
        
        return self.best_params, self.best_score
    
    def train_best_model(self, X_train, y_train, X_valid, y_valid):
        """ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸš€ ìµœì  ëª¨ë¸ í›ˆë ¨")
        print("="*60)
        
        # solver ì¬ì„¤ì • (ìµœì í™” ì‹œì™€ ë™ì¼í•˜ê²Œ)
        best_params = self.best_params.copy()
        if best_params['penalty'] == 'l1':
            best_params['solver'] = 'liblinear'
        elif best_params['penalty'] == 'elasticnet':
            best_params['solver'] = 'saga'
        
        # ìµœì  ëª¨ë¸ ìƒì„±
        self.best_model = LogisticRegression(**best_params)
        self.best_model.fit(X_train, y_train)
        
        # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
        y_valid_pred = self.best_model.predict(X_valid)
        y_valid_proba = self.best_model.predict_proba(X_valid)[:, 1]
        
        # ì„±ëŠ¥ í‰ê°€
        metrics = {
            'auc': roc_auc_score(y_valid, y_valid_proba),
            'precision': precision_score(y_valid, y_valid_pred),
            'recall': recall_score(y_valid, y_valid_pred),
            'f1': f1_score(y_valid, y_valid_pred)
        }
        
        print(f"ğŸ“ˆ ê²€ì¦ ë°ì´í„° ì„±ëŠ¥:")
        print(f"   AUC: {metrics['auc']:.4f}")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall: {metrics['recall']:.4f}")
        print(f"   F1-Score: {metrics['f1']:.4f}")
        
        return metrics
    
    def evaluate_test(self, X_test, y_test):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€"""
        print("\nğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… í‰ê°€")
        print("="*60)
        
        # ì˜ˆì¸¡
        y_test_pred = self.best_model.predict(X_test)
        y_test_proba = self.best_model.predict_proba(X_test)[:, 1]
        
        # ì„±ëŠ¥ ì§€í‘œ
        test_metrics = {
            'auc': roc_auc_score(y_test, y_test_proba),
            'precision': precision_score(y_test, y_test_pred),
            'recall': recall_score(y_test, y_test_pred),
            'f1': f1_score(y_test, y_test_pred)
        }
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
        for metric, value in test_metrics.items():
            print(f"   {metric.upper()}: {value:.4f}")
        
        # ë¶„ë¥˜ ë¦¬í¬íŠ¸
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        print(classification_report(y_test, y_test_pred))
        
        return test_metrics, y_test_pred, y_test_proba

def run_optimization(data_type='smote'):
    """ëª¨ë¸ ìµœì í™” ì‹¤í–‰"""
    print(f"=== 100% ì™„ì„±ë„ ë°ì´í„° ë¡œì§€ìŠ¤í‹± íšŒê·€ ìµœì í™” ({data_type.upper()}) ===")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    X_train = pd.read_csv(f'data_new/final/X_train_100_{data_type}.csv')
    X_valid = pd.read_csv(f'data_new/final/X_valid_100_{data_type}.csv')
    X_test = pd.read_csv(f'data_new/final/X_test_100_{data_type}.csv')
    
    y_train = pd.read_csv(f'data_new/final/y_train_100_{data_type}.csv').iloc[:, 0]
    y_valid = pd.read_csv(f'data_new/final/y_valid_100_{data_type}.csv').iloc[:, 0]
    y_test = pd.read_csv(f'data_new/final/y_test_100_{data_type}.csv').iloc[:, 0]
    
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    print(f"   Train: {X_train.shape}, ë¶€ì‹¤ ë¹„ìœ¨: {y_train.mean():.4f}")
    print(f"   Valid: {X_valid.shape}, ë¶€ì‹¤ ë¹„ìœ¨: {y_valid.mean():.4f}")
    print(f"   Test: {X_test.shape}, ë¶€ì‹¤ ë¹„ìœ¨: {y_test.mean():.4f}")
    
    # 2. ëª¨ë¸ ìµœì í™”
    optimizer = LogisticRegressionOptimizer(random_state=42)
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    best_params, best_score = optimizer.optimize(X_train, y_train, n_trials=100)
    
    # ìµœì  ëª¨ë¸ í›ˆë ¨
    valid_metrics = optimizer.train_best_model(X_train, y_train, X_valid, y_valid)
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€
    test_metrics, y_test_pred, y_test_proba = optimizer.evaluate_test(X_test, y_test)
    
    # 3. ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ëª¨ë¸ ì €ì¥
    model_path = f'outputs/models/logistic_regression_100_{data_type}_{timestamp}.joblib'
    joblib.dump(optimizer.best_model, model_path)
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_name': 'LogisticRegression',
        'data_type': data_type,
        'timestamp': timestamp,
        'best_params': best_params,
        'cv_score': best_score,
        'validation_metrics': valid_metrics,
        'test_metrics': test_metrics,
        'data_info': {
            'train_shape': X_train.shape,
            'valid_shape': X_valid.shape,
            'test_shape': X_test.shape,
            'train_default_ratio': float(y_train.mean()),
            'valid_default_ratio': float(y_valid.mean()),
            'test_default_ratio': float(y_test.mean())
        }
    }
    
    results_path = f'outputs/reports/logistic_regression_100_{data_type}_{timestamp}.json'
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"   ëª¨ë¸: {model_path}")
    print(f"   ê²°ê³¼: {results_path}")
    
    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # Normal ë°ì´í„°ë¡œ ìµœì í™”
    print("ğŸ”„ Normal ë°ì´í„° ìµœì í™” ì‹œì‘")
    normal_results = run_optimization('normal')
    
    print("\n" + "="*80)
    
    # SMOTE ë°ì´í„°ë¡œ ìµœì í™”  
    print("ğŸ”„ SMOTE ë°ì´í„° ìµœì í™” ì‹œì‘")
    smote_results = run_optimization('smote')
    
    # ìµœì¢… ë¹„êµ
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼ ë¹„êµ")
    print("="*80)
    
    print(f"Normal ë°ì´í„°:")
    print(f"   CV AUC: {normal_results['cv_score']:.4f}")
    print(f"   Test AUC: {normal_results['test_metrics']['auc']:.4f}")
    print(f"   Test F1: {normal_results['test_metrics']['f1']:.4f}")
    
    print(f"\nSMOTE ë°ì´í„°:")
    print(f"   CV AUC: {smote_results['cv_score']:.4f}")
    print(f"   Test AUC: {smote_results['test_metrics']['auc']:.4f}")
    print(f"   Test F1: {smote_results['test_metrics']['f1']:.4f}")
    
    print(f"\nì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    main() 