#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FS_100_complete.csvë¥¼ ê¸°ì¤€ìœ¼ë¡œ 4:3:3 ë¹„ìœ¨ ë°ì´í„° ë¶„í• 
ê¸°ì¡´ íŒŒì¼ë“¤ì„ ë®ì–´ì“°ê¸°
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import BorderlineSMOTE
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def main():
    print("ğŸ¢ FS_100_complete.csv ê¸°ì¤€ 4:3:3 ë¶„í•  ë° ë®ì–´ì“°ê¸°")
    print("="*80)
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ FS_100_complete.csv ë¡œë“œ")
    print("="*60)
    
    df = pd.read_csv('data_new/final/FS_100_complete.csv')
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape}")
    print(f"   ì´ ìƒ˜í”Œ: {len(df):,}ê°œ")
    print(f"   ë¶€ì‹¤ê¸°ì—…: {df['default'].sum()}ê°œ ({df['default'].mean():.4f})")
    print(f"   ì •ìƒê¸°ì—…: {(~df['default'].astype(bool)).sum()}ê°œ")
    
    # 2. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    print("\nğŸ” íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬")
    print("="*60)
    
    # ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼
    meta_columns = ['íšŒì‚¬ëª…', 'ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„']
    
    # íŠ¹ì„± ì»¬ëŸ¼ (K2_Score_Original ì œì™¸)
    feature_columns = [col for col in df.columns 
                      if col not in meta_columns + ['default', 'K2_Score_Original']]
    
    print(f"íŠ¹ì„± ì»¬ëŸ¼: {len(feature_columns)}ê°œ")
    print(f"íŠ¹ì„± ëª©ë¡: {feature_columns}")
    
    X = df[feature_columns].copy()
    y = df['default'].copy()
    meta_data = df[meta_columns].copy()
    
    # ê²°ì¸¡ê°’ í™•ì¸
    missing_check = X.isnull().sum()
    if missing_check.sum() > 0:
        print(f"\nâš ï¸ ê²°ì¸¡ê°’ ë°œê²¬:")
        for col, count in missing_check[missing_check > 0].items():
            print(f"   {col}: {count}ê°œ")
        print("âŒ FS_100_complete.csvì—ëŠ” ê²°ì¸¡ê°’ì´ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤!")
        return
    else:
        print("âœ… ê²°ì¸¡ê°’ ì—†ìŒ í™•ì¸")
    
    # 3. ë°ì´í„° ë¶„í•  (4:3:3)
    print("\nğŸ“Š ë°ì´í„° ë¶„í•  (Train:Valid:Test = 4:3:3)")
    print("="*60)
    
    # 1ì°¨ ë¶„í• : Train(40%) vs Temp(60%)
    X_train, X_temp, y_train, y_temp, meta_train, meta_temp = train_test_split(
        X, y, meta_data,
        test_size=0.6,  # 60% for Valid+Test
        random_state=42,
        stratify=y
    )
    
    # 2ì°¨ ë¶„í• : Temp(60%) -> Valid(30%) + Test(30%)
    X_valid, X_test, y_valid, y_test, meta_valid, meta_test = train_test_split(
        X_temp, y_temp, meta_temp,
        test_size=0.5,  # Validì™€ Testë¥¼ 1:1ë¡œ ë¶„í• 
        random_state=42,
        stratify=y_temp
    )
    
    print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train):,}ê°œ ({len(X_train)/len(df)*100:.1f}%)")
    print(f"  - ë¶€ì‹¤: {y_train.sum()}ê°œ ({y_train.mean():.4f})")
    print(f"  - ì •ìƒ: {(~y_train.astype(bool)).sum()}ê°œ")
    
    print(f"ê²€ì¦ ë°ì´í„°: {len(X_valid):,}ê°œ ({len(X_valid)/len(df)*100:.1f}%)")
    print(f"  - ë¶€ì‹¤: {y_valid.sum()}ê°œ ({y_valid.mean():.4f})")
    print(f"  - ì •ìƒ: {(~y_valid.astype(bool)).sum()}ê°œ")
    
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test):,}ê°œ ({len(X_test)/len(df)*100:.1f}%)")
    print(f"  - ë¶€ì‹¤: {y_test.sum()}ê°œ ({y_test.mean():.4f})")
    print(f"  - ì •ìƒ: {(~y_test.astype(bool)).sum()}ê°œ")
    
    # 4. ìŠ¤ì¼€ì¼ë§ ì ìš©
    print("\nâš–ï¸ ìŠ¤ì¼€ì¼ë§ ì ìš©")
    print("="*60)
    
    # StandardScaler ì ìš©
    scaler = StandardScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    X_valid_scaled = pd.DataFrame(
        scaler.transform(X_valid),
        columns=X_valid.columns,
        index=X_valid.index
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )
    
    print("âœ… StandardScaler ì ìš© ì™„ë£Œ")
    
    # 5. SMOTE ì ìš© (í›ˆë ¨ ë°ì´í„°ì—ë§Œ)
    print("\nğŸ”„ SMOTE ì ìš©")
    print("="*60)
    
    # BorderlineSMOTE ì„¤ì •
    smote = BorderlineSMOTE(
        sampling_strategy=0.1,  # ë¶€ì‹¤:ì •ìƒ = 1:10 ë¹„ìœ¨
        random_state=42,
        k_neighbors=5,
        m_neighbors=10
    )
    
    print("SMOTE ì ìš© ì „:")
    print(f"  - ë¶€ì‹¤: {y_train.sum()}ê°œ")
    print(f"  - ì •ìƒ: {(~y_train.astype(bool)).sum()}ê°œ")
    print(f"  - ë¹„ìœ¨: {y_train.sum()/(~y_train.astype(bool)).sum():.4f}")
    
    # SMOTE ì ìš©
    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)
    
    print("\nSMOTE ì ìš© í›„:")
    print(f"  - ë¶€ì‹¤: {y_train_smote.sum()}ê°œ")
    print(f"  - ì •ìƒ: {(~y_train_smote.astype(bool)).sum()}ê°œ")
    print(f"  - ë¹„ìœ¨: {y_train_smote.sum()/(~y_train_smote.astype(bool)).sum():.4f}")
    print(f"  - ì´ ì¦ê°€: {len(X_train_smote) - len(X_train_scaled):,}ê°œ ìƒ˜í”Œ")
    
    # 6. íŒŒì¼ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°)
    print("\nğŸ’¾ íŒŒì¼ ì €ì¥ (ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°)")
    print("="*60)
    
    # Normal ë²„ì „ ì €ì¥
    print("Normal ë²„ì „ ì €ì¥:")
    X_train_scaled.to_csv('data_new/final/X_train_100_normal.csv', index=False)
    X_valid_scaled.to_csv('data_new/final/X_valid_100_normal.csv', index=False)
    X_test_scaled.to_csv('data_new/final/X_test_100_normal.csv', index=False)
    
    y_train.to_csv('data_new/final/y_train_100_normal.csv', index=False)
    y_valid.to_csv('data_new/final/y_valid_100_normal.csv', index=False)
    y_test.to_csv('data_new/final/y_test_100_normal.csv', index=False)
    print("  âœ… Normal ë²„ì „ ì €ì¥ ì™„ë£Œ")
    
    # SMOTE ë²„ì „ ì €ì¥
    print("SMOTE ë²„ì „ ì €ì¥:")
    pd.DataFrame(X_train_smote, columns=feature_columns).to_csv('data_new/final/X_train_100_smote.csv', index=False)
    X_valid_scaled.to_csv('data_new/final/X_valid_100_smote.csv', index=False)
    X_test_scaled.to_csv('data_new/final/X_test_100_smote.csv', index=False)
    
    pd.Series(y_train_smote).to_csv('data_new/final/y_train_100_smote.csv', index=False)
    y_valid.to_csv('data_new/final/y_valid_100_smote.csv', index=False)
    y_test.to_csv('data_new/final/y_test_100_smote.csv', index=False)
    print("  âœ… SMOTE ë²„ì „ ì €ì¥ ì™„ë£Œ")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    print("ë©”íƒ€ë°ì´í„° ì €ì¥:")
    meta_train.to_csv('data_new/final/meta_train_100.csv', index=False)
    meta_valid.to_csv('data_new/final/meta_valid_100.csv', index=False)
    meta_test.to_csv('data_new/final/meta_test_100.csv', index=False)
    print("  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
    
    # 7. ì •ë³´ íŒŒì¼ ì—…ë°ì´íŠ¸
    print("\nğŸ“‹ ì •ë³´ íŒŒì¼ ì—…ë°ì´íŠ¸")
    print("="*60)
    
    dataset_info = {
        "dataset_name": "100% Complete Financial Data (4:3:3 Split)",
        "creation_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        "original_data": {
            "total_samples": len(df),
            "features": len(feature_columns),
            "default_count": int(df['default'].sum()),
            "default_ratio": float(df['default'].mean())
        },
        "data_split": {
            "train_samples": len(X_train),
            "valid_samples": len(X_valid),
            "test_samples": len(X_test),
            "train_ratio": 0.4,
            "valid_ratio": 0.3,
            "test_ratio": 0.3
        },
        "smote_applied": {
            "original_train_samples": len(X_train),
            "smote_train_samples": len(X_train_smote),
            "target_ratio": 0.1,
            "smote_type": "BorderlineSMOTE"
        },
        "features": feature_columns,
        "scaling": "StandardScaler",
        "files": {
            "normal": {
                "X_train": "X_train_100_normal.csv",
                "X_valid": "X_valid_100_normal.csv", 
                "X_test": "X_test_100_normal.csv",
                "y_train": "y_train_100_normal.csv",
                "y_valid": "y_valid_100_normal.csv",
                "y_test": "y_test_100_normal.csv"
            },
            "smote": {
                "X_train": "X_train_100_smote.csv",
                "X_valid": "X_valid_100_smote.csv",
                "X_test": "X_test_100_smote.csv", 
                "y_train": "y_train_100_smote.csv",
                "y_valid": "y_valid_100_smote.csv",
                "y_test": "y_test_100_smote.csv"
            },
            "meta": {
                "meta_train": "meta_train_100.csv",
                "meta_valid": "meta_valid_100.csv",
                "meta_test": "meta_test_100.csv"
            }
        }
    }
    
    with open('data_new/final/dataset_info_100_complete.json', 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    
    print("âœ… dataset_info_100_complete.json ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    # 8. ìµœì¢… ìš”ì•½
    print("\nğŸ‰ ì‘ì—… ì™„ë£Œ!")
    print("="*80)
    print()
    print("ğŸ“Š ìƒˆë¡œìš´ 4:3:3 ë¶„í•  ê²°ê³¼:")
    print(f"  - Train: {len(X_train):,}ê°œ (40%) | ë¶€ì‹¤: {y_train.sum()}ê°œ")
    print(f"  - Valid: {len(X_valid):,}ê°œ (30%) | ë¶€ì‹¤: {y_valid.sum()}ê°œ") 
    print(f"  - Test:  {len(X_test):,}ê°œ  (30%) | ë¶€ì‹¤: {y_test.sum()}ê°œ")
    print()
    print("ğŸ”„ SMOTE ì ìš©:")
    print(f"  - Train: {len(X_train_smote):,}ê°œ | ë¶€ì‹¤: {y_train_smote.sum()}ê°œ (10%)")
    print()
    print("âœ… ëª¨ë“  ê¸°ì¡´ íŒŒì¼ì´ ìƒˆë¡œìš´ 4:3:3 ë¶„í•  ë°ì´í„°ë¡œ ë®ì–´ì¨ì¡ŒìŠµë‹ˆë‹¤!")
    print("ğŸš€ ì´ì œ ê¸°ì¡´ ëª¨ë¸ë§ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 