import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import BorderlineSMOTE
import warnings
warnings.filterwarnings('ignore')

print("=== ìµœì¢… ëª¨ë¸ë§ìš© ë°ì´í„°ì…‹ ìƒì„± (SMOTE í¬í•¨) ===")

# 1. ë°ì´í„° ë¡œë“œ
print("\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ")
print("="*50)

# ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ
df = pd.read_csv('data_new/final/FS_ratio_flow_labeled.csv', dtype={'ê±°ë˜ì†Œì½”ë“œ': str})
print(f"ë¼ë²¨ë§ëœ ë°ì´í„°: {df.shape}")
print(f"ë¶€ì‹¤(default=1): {(df['default']==1).sum():,}ê°œ ({(df['default']==1).sum()/len(df)*100:.2f}%)")
print(f"ì •ìƒ(default=0): {(df['default']==0).sum():,}ê°œ ({(df['default']==0).sum()/len(df)*100:.2f}%)")

# 2. íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬
print("\n2ï¸âƒ£ íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬")
print("="*50)

# ë©”íƒ€ë°ì´í„° ì»¬ëŸ¼
meta_columns = ['íšŒì‚¬ëª…', 'ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„']

# ì¬ë¬´ë¹„ìœ¨ ì»¬ëŸ¼ (íŠ¹ì„±)
feature_columns = [col for col in df.columns 
                  if col not in meta_columns + ['default']]

print(f"íŠ¹ì„± ì»¬ëŸ¼: {len(feature_columns)}ê°œ")
print(f"íŠ¹ì„± ëª©ë¡: {feature_columns}")

# ê²°ì¸¡ê°’ í™•ì¸ ë° ì²˜ë¦¬
print(f"\nê²°ì¸¡ê°’ í˜„í™©:")
missing_info = []
for col in feature_columns:
    missing_count = df[col].isna().sum()
    missing_pct = missing_count / len(df) * 100
    if missing_count > 0:
        missing_info.append((col, missing_count, missing_pct))
        print(f"  {col}: {missing_count:,}ê°œ ({missing_pct:.2f}%)")
        # ì¤‘ì•™ê°’ìœ¼ë¡œ ê²°ì¸¡ê°’ ëŒ€ì²´
        df[col] = df[col].fillna(df[col].median())

if not missing_info:
    print("  ê²°ì¸¡ê°’ ì—†ìŒ âœ…")
else:
    print("  âœ… ê²°ì¸¡ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´ ì™„ë£Œ")

# 3. ë°ì´í„° ë¶„í•  (4:3:3 ë¹„ìœ¨)
print("\n3ï¸âƒ£ ë°ì´í„° ë¶„í•  (Train:Valid:Test = 4:3:3)")
print("="*50)

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = df[feature_columns].copy()
y = df['default'].copy()
meta_data = df[meta_columns].copy()

# 1ì°¨ ë¶„í• : Train(40%) vs Temp(60%)
X_train, X_temp, y_train, y_temp, meta_train, meta_temp = train_test_split(
    X, y, meta_data, 
    test_size=0.6,  # 60% for Valid+Test
    random_state=42, 
    stratify=y
)

# 2ì°¨ ë¶„í• : Temp(60%) -> Valid(30%) + Test(30%)
# Temp ì¤‘ì—ì„œ Valid:Test = 1:1 ë¹„ìœ¨ì´ë¯€ë¡œ test_size = 0.5
X_valid, X_test, y_valid, y_test, meta_valid, meta_test = train_test_split(
    X_temp, y_temp, meta_temp,
    test_size=0.5,  # Validì™€ Testë¥¼ 1:1ë¡œ ë¶„í• 
    random_state=42,
    stratify=y_temp
)

print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train):,}ê°œ ({len(X_train)/len(df)*100:.1f}%)")
print(f"  - ë¶€ì‹¤: {(y_train==1).sum():,}ê°œ ({(y_train==1).sum()/len(y_train)*100:.2f}%)")
print(f"  - ì •ìƒ: {(y_train==0).sum():,}ê°œ")

print(f"ê²€ì¦ ë°ì´í„°: {len(X_valid):,}ê°œ ({len(X_valid)/len(df)*100:.1f}%)")
print(f"  - ë¶€ì‹¤: {(y_valid==1).sum():,}ê°œ ({(y_valid==1).sum()/len(y_valid)*100:.2f}%)")
print(f"  - ì •ìƒ: {(y_valid==0).sum():,}ê°œ")

print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test):,}ê°œ ({len(X_test)/len(df)*100:.1f}%)")
print(f"  - ë¶€ì‹¤: {(y_test==1).sum():,}ê°œ ({(y_test==1).sum()/len(y_test)*100:.2f}%)")
print(f"  - ì •ìƒ: {(y_test==0).sum():,}ê°œ")

# 4. SMOTE ì ìš© (í›ˆë ¨ ë°ì´í„°ì—ë§Œ)
print("\n4ï¸âƒ£ BorderlineSMOTE ì ìš©")
print("="*50)

# BorderlineSMOTE ì„¤ì • (1:10 ë¹„ìœ¨ë¡œ ì¡°ì •)
smote = BorderlineSMOTE(
    sampling_strategy=0.1,  # ë¶€ì‹¤:ì •ìƒ = 1:10 ë¹„ìœ¨
    random_state=42,
    k_neighbors=5,
    m_neighbors=10
)

print("SMOTE ì ìš© ì „ í›ˆë ¨ ë°ì´í„°:")
print(f"  - ë¶€ì‹¤: {(y_train==1).sum():,}ê°œ")
print(f"  - ì •ìƒ: {(y_train==0).sum():,}ê°œ")
print(f"  - ë¹„ìœ¨: {(y_train==1).sum()/(y_train==0).sum():.4f}")

# SMOTE ì ìš©
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

print("\nSMOTE ì ìš© í›„ í›ˆë ¨ ë°ì´í„°:")
print(f"  - ë¶€ì‹¤: {(y_train_smote==1).sum():,}ê°œ")
print(f"  - ì •ìƒ: {(y_train_smote==0).sum():,}ê°œ")
print(f"  - ë¹„ìœ¨: {(y_train_smote==1).sum()/(y_train_smote==0).sum():.4f}")
print(f"  - ì´ ì¦ê°€: {len(X_train_smote) - len(X_train):,}ê°œ ìƒ˜í”Œ")

# 5. ìŠ¤ì¼€ì¼ë§ ì ìš©
print("\n5ï¸âƒ£ ìŠ¤ì¼€ì¼ë§ ì ìš©")
print("="*50)

# ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ (ì´ì „ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)
robust_scaler_columns = [
    'ROA', 'CFO_TD', 'RE_TA', 'EBIT_TA', 'MVE_TL', 'S_TA', 
    'CLCA', 'OENEG', 'CR', 'CFO_TA', 'RET_3M', 'RET_9M', 'MB'
]

standard_scaler_columns = [
    'TLTA', 'WC_TA', 'SIGMA', 'TLMTA'
]

# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
robust_cols_available = [col for col in robust_scaler_columns if col in feature_columns]
standard_cols_available = [col for col in standard_scaler_columns if col in feature_columns]

print(f"RobustScaler ì ìš©: {len(robust_cols_available)}ê°œ ì»¬ëŸ¼")
print(f"StandardScaler ì ìš©: {len(standard_cols_available)}ê°œ ì»¬ëŸ¼")

def apply_scaling(X_train_data, X_valid_data, X_test_data, dataset_name=""):
    """ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\n{dataset_name} ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘...")
    
    X_train_scaled = X_train_data.copy()
    X_valid_scaled = X_valid_data.copy()
    X_test_scaled = X_test_data.copy()
    
    # RobustScaler ì ìš©
    if robust_cols_available:
        robust_scaler = RobustScaler()
        robust_scaler.fit(X_train_data[robust_cols_available])
        
        X_train_scaled[robust_cols_available] = robust_scaler.transform(X_train_data[robust_cols_available])
        X_valid_scaled[robust_cols_available] = robust_scaler.transform(X_valid_data[robust_cols_available])
        X_test_scaled[robust_cols_available] = robust_scaler.transform(X_test_data[robust_cols_available])
        
        print(f"  âœ… RobustScaler ì ìš© ì™„ë£Œ ({len(robust_cols_available)}ê°œ ì»¬ëŸ¼)")
    
    # StandardScaler ì ìš©
    if standard_cols_available:
        standard_scaler = StandardScaler()
        standard_scaler.fit(X_train_data[standard_cols_available])
        
        X_train_scaled[standard_cols_available] = standard_scaler.transform(X_train_data[standard_cols_available])
        X_valid_scaled[standard_cols_available] = standard_scaler.transform(X_valid_data[standard_cols_available])
        X_test_scaled[standard_cols_available] = standard_scaler.transform(X_test_data[standard_cols_available])
        
        print(f"  âœ… StandardScaler ì ìš© ì™„ë£Œ ({len(standard_cols_available)}ê°œ ì»¬ëŸ¼)")
    
    return X_train_scaled, X_valid_scaled, X_test_scaled

# ì¼ë°˜ ë²„ì „ ìŠ¤ì¼€ì¼ë§
X_train_scaled, X_valid_scaled, X_test_scaled = apply_scaling(
    X_train, X_valid, X_test, "ì¼ë°˜ ë²„ì „"
)

# SMOTE ë²„ì „ ìŠ¤ì¼€ì¼ë§ (SMOTE ì ìš©ëœ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©)
X_train_smote_scaled, X_valid_scaled_smote, X_test_scaled_smote = apply_scaling(
    X_train_smote, X_valid, X_test, "SMOTE ë²„ì „"
)

# 6. ë°ì´í„° ì €ì¥
print("\n6ï¸âƒ£ ìµœì¢… ë°ì´í„° ì €ì¥")
print("="*50)

# ì¼ë°˜ ë²„ì „ ì €ì¥
print("ì¼ë°˜ ë²„ì „ ë°ì´í„° ì €ì¥:")
X_train_scaled.to_csv('data_new/final/X_train_normal.csv', index=False, encoding='utf-8-sig')
X_valid_scaled.to_csv('data_new/final/X_valid_normal.csv', index=False, encoding='utf-8-sig')  
X_test_scaled.to_csv('data_new/final/X_test_normal.csv', index=False, encoding='utf-8-sig')

y_train.to_csv('data_new/final/y_train_normal.csv', index=False, encoding='utf-8-sig')
y_valid.to_csv('data_new/final/y_valid_normal.csv', index=False, encoding='utf-8-sig')
y_test.to_csv('data_new/final/y_test_normal.csv', index=False, encoding='utf-8-sig')

print("  âœ… ì¼ë°˜ ë²„ì „ ì €ì¥ ì™„ë£Œ")

# SMOTE ë²„ì „ ì €ì¥
print("SMOTE ë²„ì „ ë°ì´í„° ì €ì¥:")
X_train_smote_scaled.to_csv('data_new/final/X_train_smote.csv', index=False, encoding='utf-8-sig')
X_valid_scaled_smote.to_csv('data_new/final/X_valid_smote.csv', index=False, encoding='utf-8-sig')
X_test_scaled_smote.to_csv('data_new/final/X_test_smote.csv', index=False, encoding='utf-8-sig')

pd.Series(y_train_smote).to_csv('data_new/final/y_train_smote.csv', index=False, encoding='utf-8-sig')
y_valid.to_csv('data_new/final/y_valid_smote.csv', index=False, encoding='utf-8-sig')
y_test.to_csv('data_new/final/y_test_smote.csv', index=False, encoding='utf-8-sig')

print("  âœ… SMOTE ë²„ì „ ì €ì¥ ì™„ë£Œ")

# ë©”íƒ€ë°ì´í„°ë„ ì €ì¥
meta_train.to_csv('data_new/final/meta_train.csv', index=False, encoding='utf-8-sig')
meta_valid.to_csv('data_new/final/meta_valid.csv', index=False, encoding='utf-8-sig')
meta_test.to_csv('data_new/final/meta_test.csv', index=False, encoding='utf-8-sig')

print("  âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")

# 7. ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ì €ì¥
print("\n7ï¸âƒ£ ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´ ìƒì„±")
print("="*50)

summary_info = {
    'dataset_info': {
        'original_samples': len(df),
        'total_features': len(feature_columns),
        'original_default_rate': (df['default']==1).sum() / len(df),
        'split_ratio': [4, 3, 3],  # train:valid:test
        'split_method': 'stratified_random'
    },
    'normal_version': {
        'train_samples': len(X_train),
        'valid_samples': len(X_valid),
        'test_samples': len(X_test),
        'train_default_rate': (y_train==1).sum() / len(y_train),
        'valid_default_rate': (y_valid==1).sum() / len(y_valid),
        'test_default_rate': (y_test==1).sum() / len(y_test)
    },
    'smote_version': {
        'train_samples': len(X_train_smote),
        'valid_samples': len(X_valid),  # ê²€ì¦/í…ŒìŠ¤íŠ¸ëŠ” ë™ì¼
        'test_samples': len(X_test),
        'train_default_rate': (y_train_smote==1).sum() / len(y_train_smote),
        'smote_method': 'BorderlineSMOTE',
        'samples_added': len(X_train_smote) - len(X_train)
    },
    'feature_info': {
        'feature_columns': feature_columns,
        'robust_scaled': robust_cols_available,
        'standard_scaled': standard_cols_available,
        'meta_columns': meta_columns
    },
    'scaling_info': {
        'robust_scaler_features': len(robust_cols_available),
        'standard_scaler_features': len(standard_cols_available),
        'scaling_fit_on': 'train_data_only'
    }
}

import json
with open('data_new/final/dataset_info_final.json', 'w', encoding='utf-8') as f:
    json.dump(summary_info, f, ensure_ascii=False, indent=2)

print(f"âœ… ë°ì´í„°ì…‹ ì •ë³´: data_new/final/dataset_info_final.json")

# 8. ìµœì¢… ìš”ì•½
print("\n8ï¸âƒ£ ìµœì¢… ìš”ì•½")
print("="*50)

print(f"ğŸ“Š ìƒì„±ëœ íŒŒì¼ë“¤:")
print(f"\nğŸ”¹ ì¼ë°˜ ë²„ì „ (Original):")
print(f"  - X_train_normal.csv: {len(X_train):,}ê°œ ìƒ˜í”Œ")
print(f"  - X_valid_normal.csv: {len(X_valid):,}ê°œ ìƒ˜í”Œ")
print(f"  - X_test_normal.csv: {len(X_test):,}ê°œ ìƒ˜í”Œ")
print(f"  - y_train_normal.csv, y_valid_normal.csv, y_test_normal.csv")

print(f"\nğŸ”¹ SMOTE ë²„ì „ (Balanced):")
print(f"  - X_train_smote.csv: {len(X_train_smote):,}ê°œ ìƒ˜í”Œ (+{len(X_train_smote)-len(X_train):,})")
print(f"  - X_valid_smote.csv: {len(X_valid):,}ê°œ ìƒ˜í”Œ (ë™ì¼)")
print(f"  - X_test_smote.csv: {len(X_test):,}ê°œ ìƒ˜í”Œ (ë™ì¼)")
print(f"  - y_train_smote.csv, y_valid_smote.csv, y_test_smote.csv")

print(f"\nğŸ”¹ ë©”íƒ€ë°ì´í„°:")
print(f"  - meta_train.csv, meta_valid.csv, meta_test.csv")
print(f"  - dataset_info_final.json")

print(f"\nğŸ¯ ë°ì´í„° íŠ¹ì„±:")
print(f"  - ì´ íŠ¹ì„±: {len(feature_columns)}ê°œ")
print(f"  - ë¶„í•  ë¹„ìœ¨: Train {len(X_train)/len(df)*100:.1f}% : Valid {len(X_valid)/len(df)*100:.1f}% : Test {len(X_test)/len(df)*100:.1f}%")
print(f"  - ì›ë³¸ ë¶€ì‹¤ ë¹„ìœ¨: {(df['default']==1).sum()/len(df)*100:.2f}%")
print(f"  - SMOTE í›„ í›ˆë ¨ ë¶€ì‹¤ ë¹„ìœ¨: {(y_train_smote==1).sum()/len(y_train_smote)*100:.2f}%")
print(f"  - ìŠ¤ì¼€ì¼ë§: RobustScaler({len(robust_cols_available)}ê°œ), StandardScaler({len(standard_cols_available)}ê°œ)")

print(f"\nğŸš€ ë‹¤ìŒ ë‹¨ê³„:")
print(f"  1. ì¼ë°˜ ë²„ì „ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í›ˆë ¨")
print(f"  2. SMOTE ë²„ì „ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ëª¨ë¸ í›ˆë ¨")
print(f"  3. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (Logistic Regression, Random Forest, XGBoost)")
print(f"  4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
print(f"  5. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° í•´ì„")

print(f"\nâœ… ìµœì¢… ëª¨ë¸ë§ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!")
print(f"ğŸ’¡ SMOTEëŠ” í›ˆë ¨ ë°ì´í„°ì—ë§Œ ì ìš©ë˜ì–´ ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë°©ì§€í–ˆìŠµë‹ˆë‹¤.") 