"""
Altman Z-Score ë° K2-Score ë¶€ì‹¤ì˜ˆì¸¡ ì„±ëŠ¥ ë¶„ì„
ê¸°ì¡´ ML ëª¨ë¸ê³¼ì˜ ì„±ëŠ¥ ë¹„êµ ë° ì „í†µì  ì¬ë¬´ì§€í‘œì˜ íš¨ê³¼ì„± ê²€ì¦

Author: AI Assistant
Date: 2025-06-21
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, roc_curve
from sklearn.metrics import classification_report, confusion_matrix
import warnings
import os

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class ScorePerformanceAnalyzer:
    """Altman Z-Score ë° K2-Score ì„±ëŠ¥ ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.scores_df = None
        self.labeled_df = None
        
    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë“œ ë° ë¶€ì‹¤ ë¼ë²¨ ê²°í•©"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ì ìˆ˜ ë°ì´í„° ë¡œë“œ
        scores_path = 'outputs/reports/altman_k2_scores.csv'
        if not os.path.exists(scores_path):
            raise FileNotFoundError(f"ì ìˆ˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scores_path}")
        
        self.scores_df = pd.read_csv(scores_path)
        print(f"âœ… ì ìˆ˜ ë°ì´í„° ë¡œë“œ: {self.scores_df.shape}")
        
        # ë¼ë²¨ëœ ë°ì´í„° ë¡œë“œ
        labeled_path = 'data_new/final/FS_ratio_flow_labeled.csv'
        if not os.path.exists(labeled_path):
            raise FileNotFoundError(f"ë¼ë²¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {labeled_path}")
        
        labeled_df = pd.read_csv(labeled_path)
        print(f"âœ… ë¼ë²¨ ë°ì´í„° ë¡œë“œ: {labeled_df.shape}")
        
        # ë°ì´í„° ê²°í•©
        merge_cols = ['íšŒì‚¬ëª…', 'ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„']
        self.labeled_df = pd.merge(
            self.scores_df, 
            labeled_df[merge_cols + ['default']],
            on=merge_cols,
            how='inner'
        )
        print(f"âœ… ë°ì´í„° ê²°í•© ì™„ë£Œ: {self.labeled_df.shape}")
        print(f"ğŸ“Š ë¶€ì‹¤ ê¸°ì—… ë¹„ìœ¨: {self.labeled_df['default'].mean():.2%}")
        
        return self.labeled_df
    
    def analyze_score_thresholds(self) -> dict:
        """ê° ì ìˆ˜ë³„ ìµœì  ì„ê³„ê°’ ë¶„ì„"""
        print("\nğŸ¯ ì ìˆ˜ë³„ ìµœì  ì„ê³„ê°’ ë¶„ì„")
        print("="*60)
        
        score_columns = [col for col in self.labeled_df.columns 
                        if 'K2_Score' in col and col != 'default']
        
        results = {}
        
        for score_col in score_columns:
            # ê²°ì¸¡ê°’ ì œê±°
            valid_data = self.labeled_df[[score_col, 'default']].dropna()
            if len(valid_data) == 0:
                continue
                
            y_true = valid_data['default']
            y_scores = valid_data[score_col]
            
            # ROC ê³¡ì„  ê³„ì‚°
            fpr, tpr, thresholds = roc_curve(y_true, y_scores)
            auc = roc_auc_score(y_true, y_scores)
            
            # Youden's J statisticìœ¼ë¡œ ìµœì  ì„ê³„ê°’ ì°¾ê¸°
            j_scores = tpr - fpr
            optimal_idx = np.argmax(j_scores)
            optimal_threshold = thresholds[optimal_idx]
            
            # ìµœì  ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥
            y_pred = (y_scores >= optimal_threshold).astype(int)
            precision = precision_score(y_true, y_pred)
            recall = recall_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred)
            
            results[score_col] = {
                'auc': auc,
                'optimal_threshold': optimal_threshold,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'n_samples': len(valid_data)
            }
            
            print(f"\nğŸ“Š {score_col}:")
            print(f"   AUC: {auc:.4f}")
            print(f"   ìµœì  ì„ê³„ê°’: {optimal_threshold:.4f}")
            print(f"   Precision: {precision:.4f}")
            print(f"   Recall: {recall:.4f}")
            print(f"   F1-Score: {f1:.4f}")
            print(f"   ìƒ˜í”Œ ìˆ˜: {len(valid_data):,}")
        
        return results
    
    def analyze_traditional_thresholds(self) -> dict:
        """ì „í†µì ì¸ Altman Z-Score ì„ê³„ê°’ ë¶„ì„"""
        print("\nğŸ“š ì „í†µì ì¸ Altman Z-Score ì„ê³„ê°’ ë¶„ì„")
        print("="*60)
        
        # ì „í†µì ì¸ ì„ê³„ê°’ë“¤
        traditional_thresholds = {
            'K2_Score_Original': {
                'safe': 2.6,      # ì•ˆì „ êµ¬ê°„
                'distress': 1.1   # ìœ„í—˜ êµ¬ê°„
            }
        }
        
        results = {}
        
        for score_col, thresholds in traditional_thresholds.items():
            if score_col not in self.labeled_df.columns:
                continue
                
            valid_data = self.labeled_df[[score_col, 'default']].dropna()
            if len(valid_data) == 0:
                continue
            
            y_true = valid_data['default']
            scores = valid_data[score_col]
            
            # êµ¬ê°„ë³„ ë¶„ë¥˜
            safe_zone = scores >= thresholds['safe']
            gray_zone = (scores >= thresholds['distress']) & (scores < thresholds['safe'])
            distress_zone = scores < thresholds['distress']
            
            # ê° êµ¬ê°„ë³„ ë¶€ì‹¤ë¥ 
            safe_default_rate = y_true[safe_zone].mean() if safe_zone.sum() > 0 else 0
            gray_default_rate = y_true[gray_zone].mean() if gray_zone.sum() > 0 else 0
            distress_default_rate = y_true[distress_zone].mean() if distress_zone.sum() > 0 else 0
            
            # ìœ„í—˜ êµ¬ê°„ì„ ë¶€ì‹¤ ì˜ˆì¸¡ìœ¼ë¡œ ì‚¬ìš©
            y_pred = (scores < thresholds['distress']).astype(int)
            
            precision = precision_score(y_true, y_pred) if y_pred.sum() > 0 else 0
            recall = recall_score(y_true, y_pred)
            f1 = f1_score(y_true, y_pred) if y_pred.sum() > 0 else 0
            auc = roc_auc_score(y_true, -scores)  # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ìœ„í—˜í•˜ë¯€ë¡œ ìŒìˆ˜
            
            results[score_col] = {
                'safe_zone_count': safe_zone.sum(),
                'gray_zone_count': gray_zone.sum(),
                'distress_zone_count': distress_zone.sum(),
                'safe_default_rate': safe_default_rate,
                'gray_default_rate': gray_default_rate,
                'distress_default_rate': distress_default_rate,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc
            }
            
            print(f"\nğŸ“Š {score_col} (ì „í†µì  ì„ê³„ê°’):")
            print(f"   ì•ˆì „ êµ¬ê°„ (â‰¥{thresholds['safe']}): {safe_zone.sum():,}ê°œ, ë¶€ì‹¤ë¥ : {safe_default_rate:.2%}")
            print(f"   íšŒìƒ‰ì§€ëŒ€ ({thresholds['distress']}~{thresholds['safe']}): {gray_zone.sum():,}ê°œ, ë¶€ì‹¤ë¥ : {gray_default_rate:.2%}")
            print(f"   ìœ„í—˜ êµ¬ê°„ (<{thresholds['distress']}): {distress_zone.sum():,}ê°œ, ë¶€ì‹¤ë¥ : {distress_default_rate:.2%}")
            print(f"   AUC: {auc:.4f}")
            print(f"   Precision: {precision:.4f}")
            print(f"   Recall: {recall:.4f}")
            print(f"   F1-Score: {f1:.4f}")
        
        return results
    
    def compare_with_ml_models(self, score_results: dict) -> pd.DataFrame:
        """ML ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ"""
        print("\nğŸ¤– ML ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ")
        print("="*60)
        
        # ML ëª¨ë¸ ê²°ê³¼ ë¡œë“œ
        ml_results_path = 'outputs/reports/model_comparison_report.json'
        if os.path.exists(ml_results_path):
            import json
            with open(ml_results_path, 'r', encoding='utf-8') as f:
                ml_data = json.load(f)
            
            ml_performance = pd.DataFrame(ml_data['performance_summary'])
            ml_performance = ml_performance.set_index('Model')
            
            print("ğŸ“Š ML ëª¨ë¸ ì„±ëŠ¥:")
            print(ml_performance[['AUC', 'Precision', 'Recall', 'F1-Score']].round(4))
        else:
            print("âš ï¸ ML ëª¨ë¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            ml_performance = pd.DataFrame()
        
        # ì „í†µì  ì ìˆ˜ ì„±ëŠ¥
        score_performance = []
        for score_name, metrics in score_results.items():
            score_performance.append({
                'Model': score_name,
                'AUC': metrics['auc'],
                'Precision': metrics['precision'],
                'Recall': metrics['recall'],
                'F1-Score': metrics['f1']
            })
        
        score_df = pd.DataFrame(score_performance).set_index('Model')
        
        print("\nğŸ“Š ì „í†µì  ì ìˆ˜ ì„±ëŠ¥:")
        print(score_df.round(4))
        
        # í†µí•© ë¹„êµ
        if not ml_performance.empty:
            combined_df = pd.concat([ml_performance[['AUC', 'Precision', 'Recall', 'F1-Score']], 
                                   score_df], axis=0)
            
            print("\nğŸ† í†µí•© ì„±ëŠ¥ ë¹„êµ:")
            print(combined_df.round(4))
            
            return combined_df
        
        return score_df
    
    def visualize_performance(self, score_results: dict) -> None:
        """ì„±ëŠ¥ ì‹œê°í™”"""
        print("\nğŸ“ˆ ì„±ëŠ¥ ì‹œê°í™”")
        print("="*60)
        
        # ROC ê³¡ì„  ê·¸ë¦¬ê¸°
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Altman Z-Score ë° K2-Score ROC ê³¡ì„  ë¶„ì„', fontsize=16, fontweight='bold')
        
        score_columns = [col for col in self.labeled_df.columns 
                        if 'K2_Score' in col and col != 'default']
        
        for i, score_col in enumerate(score_columns[:4]):
            row, col = divmod(i, 2)
            ax = axes[row, col]
            
            # ê²°ì¸¡ê°’ ì œê±°
            valid_data = self.labeled_df[[score_col, 'default']].dropna()
            if len(valid_data) == 0:
                ax.set_visible(False)
                continue
            
            y_true = valid_data['default']
            y_scores = valid_data[score_col]
            
            # ROC ê³¡ì„ 
            fpr, tpr, _ = roc_curve(y_true, y_scores)
            auc = roc_auc_score(y_true, y_scores)
            
            ax.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')
            ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'{score_col}')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
        for i in range(len(score_columns), 4):
            row, col = divmod(i, 2)
            axes[row, col].set_visible(False)
        
        plt.tight_layout()
        
        # ì €ì¥
        os.makedirs('outputs/visualizations', exist_ok=True)
        plt.savefig('outputs/visualizations/traditional_scores_roc_curves.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        # ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
        self._plot_performance_comparison(score_results)
    
    def _plot_performance_comparison(self, score_results: dict) -> None:
        """ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸"""
        # ì„±ëŠ¥ ë°ì´í„° ì¤€ë¹„
        metrics = ['auc', 'precision', 'recall', 'f1']
        score_names = list(score_results.keys())
        
        performance_data = []
        for metric in metrics:
            for score_name in score_names:
                if metric in score_results[score_name]:
                    performance_data.append({
                        'Score': score_name.replace('K2_Score_', ''),
                        'Metric': metric.upper(),
                        'Value': score_results[score_name][metric]
                    })
        
        if not performance_data:
            return
        
        perf_df = pd.DataFrame(performance_data)
        
        # íˆíŠ¸ë§µ
        pivot_df = perf_df.pivot(index='Score', columns='Metric', values='Value')
        
        plt.figure(figsize=(10, 6))
        sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='RdYlGn', 
                   center=0.5, vmin=0, vmax=1)
        plt.title('ì „í†µì  ì ìˆ˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        plt.savefig('outputs/visualizations/traditional_scores_performance_heatmap.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¢ Altman Z-Score ë° K2-Score ì„±ëŠ¥ ë¶„ì„")
    print("="*60)
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ScorePerformanceAnalyzer()
    
    try:
        # ë°ì´í„° ë¡œë“œ
        df = analyzer.load_data()
        
        # ìµœì  ì„ê³„ê°’ ë¶„ì„
        score_results = analyzer.analyze_score_thresholds()
        
        # ì „í†µì  ì„ê³„ê°’ ë¶„ì„
        traditional_results = analyzer.analyze_traditional_thresholds()
        
        # ML ëª¨ë¸ê³¼ ë¹„êµ
        comparison_df = analyzer.compare_with_ml_models(score_results)
        
        # ì‹œê°í™”
        analyzer.visualize_performance(score_results)
        
        print("\nğŸ‰ ì „í†µì  ì ìˆ˜ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ!")
        print("ğŸ“Š ëª¨ë“  ê²°ê³¼ëŠ” outputs/visualizations/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("ğŸ’¡ ë¨¼ì € altman_score_analysis.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main() 