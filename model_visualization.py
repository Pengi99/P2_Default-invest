"""
ëª¨ë¸ë³„ ì„±ëŠ¥ ì‹œê°í™” ë„êµ¬
==============================

í´ë”ë³„ë¡œ ì €ì¥ëœ ëª¨ë¸ë“¤ì„ ë¶ˆëŸ¬ì™€ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ 
AUC, F1-Score, Precision, Recallì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
python model_visualization.py
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import joblib
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (
    roc_auc_score, precision_score, recall_score, f1_score,
    roc_curve, precision_recall_curve, average_precision_score
)
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
if platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Windows':  # Windows
    plt.rcParams['font.family'] = 'Malgun Gothic'
else:  # Linux
    plt.rcParams['font.family'] = 'DejaVu Sans'

plt.rcParams['axes.unicode_minus'] = False


class ModelVisualizer:
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ì‹œê°í™” í´ë˜ìŠ¤"""
    
    def __init__(self, models_dir: str, data_dir: str = "data/final"):
        """
        ì´ˆê¸°í™”
        
        Args:
            models_dir: ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            data_dir: í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.models_dir = Path(models_dir)
        self.data_dir = Path(data_dir)
        self.models = {}
        self.results = {}
        
        # ì‹¤ì œ í›ˆë ¨ëœ ëª¨ë¸ë“¤ì€ ëª¨ë‘ íŠ¹ì„± ì„ íƒìœ¼ë¡œ ë™ì¼í•œ 5ê°œ íŠ¹ì„± ì‚¬ìš©
        self.selected_features = [
            "ë§¤ì¶œì•¡ì¦ê°€ìœ¨", "ìë³¸ë¹„ìœ¨", "ì´ìë¶€ë‹´ì°¨ì…ê¸ˆë¹„ìœ¨", 
            "ì´ìì‚°ìˆ˜ìµë¥ (ROA)", "ë¡œê·¸ì´ìì‚°"
        ]
        
        # ëª¨ë¸ ë¶„ë¥˜ ì •ì˜ (ì˜ˆì‹œ ì´ë¯¸ì§€ ê¸°ì¤€)
        self.model_categories = {
            'logistic': ['logistic_regression'],
            'RF': ['random_forest'], 
            'XGboost': ['xgboost']
        }
        
        # ê¸°ì¡´ modeling pipelineì—ì„œ ê³„ì‚°ëœ optimal threshold ì‚¬ìš©
        self.optimal_thresholds = {
            'logistic_regression': {
                'normal': 0.7658325538018845,
                'smote': 0.8097071105720812,
                'undersampling': 0.7718764133112883,
                'combined': 0.9919230523872429
            },
            'random_forest': {
                'normal': 0.38369141074694035,
                'smote': 0.6427416447841382,
                'undersampling': 0.43599113112432086,
                'combined': 0.5553100264299876
            },
            'xgboost': {
                'normal': 0.4256463646888733,
                'smote': 0.5636553168296814,
                'undersampling': 0.343456894159317,
                'combined': 0.6189958453178406
            }
        }
        
        # ë°ì´í„° ë¡œë“œ
        self._load_test_data()
        
        print("ğŸš€ ëª¨ë¸ ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬: {self.models_dir}")
        print(f"ğŸ“Š ë°ì´í„° ë””ë ‰í† ë¦¬: {self.data_dir}")
    
    def _load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        X_test_full = pd.read_csv(self.data_dir / "X_test.csv")
        self.y_test = pd.read_csv(self.data_dir / "y_test.csv").iloc[:, 0]  # ì²« ë²ˆì§¸ ì»¬ëŸ¼ë§Œ
        
        # í›ˆë ¨ ë°ì´í„°ë„ ë¡œë“œ (ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•´)
        X_train_full = pd.read_csv(self.data_dir / "X_train.csv")
        
        # ì„ íƒëœ íŠ¹ì„±ë§Œ ì‚¬ìš©
        self.X_test = X_test_full[self.selected_features]
        X_train = X_train_full[self.selected_features]
        
        # Robust ìŠ¤ì¼€ì¼ë§ ì ìš©
        print("ğŸ”§ Robust ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘...")
        self.scaler = RobustScaler()
        self.scaler.fit(X_train)
        
        self.X_test_scaled = pd.DataFrame(
            self.scaler.transform(self.X_test),
            columns=self.X_test.columns,
            index=self.X_test.index
        )
        
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ:")
        print(f"   - X_test shape: {self.X_test.shape}")
        print(f"   - y_test shape: {self.y_test.shape}")
        print(f"   - ì„ íƒëœ íŠ¹ì„±: {', '.join(self.selected_features)}")
        print(f"   - ì–‘ì„± í´ë˜ìŠ¤ ë¹„ìœ¨: {self.y_test.mean():.3%}")
    
    def _load_models(self):
        """ëª¨ë¸ íŒŒì¼ë“¤ì„ ë¡œë“œ"""
        print("ğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        model_files = list(self.models_dir.glob("*.joblib"))
        
        for model_file in model_files:
            # íŒŒì¼ëª…ì—ì„œ ë°ì´í„° íƒ€ì…ê³¼ ëª¨ë¸ íƒ€ì… ì¶”ì¶œ
            # ì˜ˆ: "smote__logistic_regression_model.joblib"
            filename = model_file.stem
            
            if "__" in filename:
                data_type, model_info = filename.split("__", 1)
                model_type = model_info.replace("_model", "")
                
                # ì•™ìƒë¸” ëª¨ë¸ ì œì™¸
                if "ensemble" in model_type:
                    continue
                    
                try:
                    model = joblib.load(model_file)
                    key = f"{data_type}__{model_type}"
                    self.models[key] = {
                        'model': model,
                        'data_type': data_type,
                        'model_type': model_type,
                        'file_path': model_file
                    }
                    print(f"   âœ… {key}")
                    
                except Exception as e:
                    print(f"   âŒ {model_file.name}: {e}")
        
        print(f"ğŸ“ˆ ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _find_optimal_f1_threshold(self, y_true, y_pred_proba):
        """F1 ìŠ¤ì½”ì–´ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ì„ê³„ê°’ ì°¾ê¸° (ê¸°ì¡´ modeling pipeline ë°©ì‹ê³¼ ë™ì¼)"""
        thresholds = np.arange(0.1, 0.9, 0.05)
        best_f1 = 0
        best_threshold = 0.5
        
        for threshold in thresholds:
            y_pred = (y_pred_proba >= threshold).astype(int)
            if len(np.unique(y_pred)) > 1:  # ì˜ˆì¸¡ì´ í•œ í´ë˜ìŠ¤ë¡œë§Œ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ê²½ìš°
                f1 = f1_score(y_true, y_pred, zero_division=0)
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = threshold
        
        return best_threshold, best_f1
    
    def _evaluate_models(self):
        """ëª¨ë“  ëª¨ë¸ í‰ê°€"""
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        for key, model_info in self.models.items():
            model = model_info['model']
            data_type = model_info['data_type']
            model_type = model_info['model_type']
            
            try:
                # ì˜ˆì¸¡ (ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ íŠ¹ì„± ì‚¬ìš©)
                y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1]
                
                # ê¸°ì¡´ modeling pipelineì—ì„œ ê³„ì‚°ëœ optimal threshold ì‚¬ìš©
                optimal_threshold = self.optimal_thresholds.get(model_type, {}).get(data_type, 0.5)
                y_pred = (y_pred_proba >= optimal_threshold).astype(int)
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = {
                    'auc': roc_auc_score(self.y_test, y_pred_proba),
                    'f1': f1_score(self.y_test, y_pred, zero_division=0),
                    'precision': precision_score(self.y_test, y_pred, zero_division=0),
                    'recall': recall_score(self.y_test, y_pred, zero_division=0),
                    'average_precision': average_precision_score(self.y_test, y_pred_proba),
                    'optimal_threshold': optimal_threshold,
                    'y_pred_proba': y_pred_proba,
                    'y_pred': y_pred
                }
                
                self.results[key] = {
                    **model_info,
                    **metrics
                }
                
                print(f"   âœ… {key}: AUC={metrics['auc']:.3f}, F1={metrics['f1']:.3f}, Threshold={optimal_threshold:.3f}")
                
            except Exception as e:
                print(f"   âŒ {key}: {e}")
    
    def _create_category_visualization(self, category: str, model_types: List[str]):
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹œê°í™” ìƒì„±"""
        print(f"ğŸ¨ {category} ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë¸ë“¤ í•„í„°ë§
        category_models = {}
        for key, result in self.results.items():
            if result['model_type'] in model_types:
                category_models[key] = result
        
        if not category_models:
            print(f"   âš ï¸ {category} ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ë°ì´í„° íƒ€ì…ë³„ë¡œ ì •ë ¬ (normal, smote, undersampling, combined ìˆœì„œ)
        data_type_order = ['normal', 'smote', 'undersampling', 'combined']
        sorted_models = sorted(category_models.items(), 
                             key=lambda x: data_type_order.index(x[1]['data_type']) 
                             if x[1]['data_type'] in data_type_order else 999)
        
        # ë°ì´í„° ì¤€ë¹„
        model_names = []
        metrics_data = {'AUC': [], 'F1-Score': [], 'Precision': [], 'Recall': []}
        
        for key, result in sorted_models:
            display_name = f"{result['model_type']}_{result['data_type']}"
            model_names.append(display_name)
            
            metrics_data['AUC'].append(result['auc'])
            metrics_data['F1-Score'].append(result['f1'])
            metrics_data['Precision'].append(result['precision'])
            metrics_data['Recall'].append(result['recall'])
        
        # ì‹œê°í™” ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        # ìƒ‰ìƒ ì„¤ì • (ì˜ˆì‹œ ì´ë¯¸ì§€ì™€ ìœ ì‚¬í•˜ê²Œ)
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange', 'plum', 'wheat', 'lightgray']
        
        for i, (metric_name, values) in enumerate(metrics_data.items()):
            ax = axes[i]
            
            # ë°” ì°¨íŠ¸ ìƒì„±
            bars = ax.bar(range(len(model_names)), values, 
                         color=colors[:len(model_names)], alpha=0.8)
            
            # ê°’ í‘œì‹œ
            for j, (bar, value) in enumerate(zip(bars, values)):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                       f'{value:.3f}', ha='center', va='bottom', 
                       fontweight='bold', fontsize=10)
            
            # ì¶• ì„¤ì •
            ax.set_title(f'{metric_name} Comparison', fontsize=14, fontweight='bold')
            ax.set_ylabel(metric_name, fontsize=12)
            ax.set_ylim(0, 1.0)
            ax.grid(True, alpha=0.3, axis='y')
            
            # xì¶• ë ˆì´ë¸” ì„¤ì •
            ax.set_xticks(range(len(model_names)))
            ax.set_xticklabels(model_names, rotation=45, ha='right')
        
        # ì „ì²´ ì œëª©
        fig.suptitle(f'{category} Models Performance Comparison', 
                    fontsize=18, fontweight='bold', y=0.98)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.93)
        
        # ì €ì¥
        output_path = f"{category}.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   âœ… {category} ì‹œê°í™” ì €ì¥: {output_path}")
    
    def create_all_visualizations(self):
        """ëª¨ë“  ì‹œê°í™” ìƒì„±"""
        print("ğŸ¨ ëª¨ë¸ë³„ ì‹œê°í™” ìƒì„± ì‹œì‘...")
        
        # ëª¨ë¸ ë¡œë“œ ë° í‰ê°€
        self._load_models()
        self._evaluate_models()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì‹œê°í™” ìƒì„±
        for category, model_types in self.model_categories.items():
            self._create_category_visualization(category, model_types)
        
        print("âœ… ëª¨ë“  ì‹œê°í™” ìƒì„± ì™„ë£Œ!")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        for key, result in self.results.items():
            print(f"   {key}:")
            print(f"     AUC: {result['auc']:.3f}, F1: {result['f1']:.3f}, "
                  f"Precision: {result['precision']:.3f}, Recall: {result['recall']:.3f}, "
                  f"Optimal Threshold: {result['optimal_threshold']:.3f}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ëª¨ë¸ ì„±ëŠ¥ ì‹œê°í™” ë„êµ¬ ì‹œì‘")
    
    # ê²½ë¡œ ì„¤ì •
    models_dir = "/Users/jojongho/KDT/P2_Default-invest/outputs/modeling_runs/default_modeling_run_20250706_131209/models"
    data_dir = "/Users/jojongho/KDT/P2_Default-invest/data/final"
    
    # ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™” ë° ì‹¤í–‰
    visualizer = ModelVisualizer(models_dir, data_dir)
    visualizer.create_all_visualizations()
    
    print("ğŸ‰ ì‹œê°í™” ì™„ë£Œ!")


if __name__ == "__main__":
    main()