"""
Factor Backtesting Framework v2.2
Daily Data-Based Backtesting with 4 Core Strategies
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import glob
import os
import yaml
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from scipy import stats
import yfinance as yf  # Not used in current implementation
from pathlib import Path
from multiprocessing import Pool, cpu_count
import multiprocessing

# Optimize multiprocessing start method for better performance on macOS
if __name__ == "__main__":
    try:
        multiprocessing.set_start_method('fork', force=True)
    except RuntimeError:
        pass  # Already set

# Performance optimization libraries
try:
    import polars as pl
    POLARS_AVAILABLE = True
except ImportError:
    POLARS_AVAILABLE = False

try:
    from numba import jit, prange
    NUMBA_AVAILABLE = True
except ImportError:
    NUMBA_AVAILABLE = False

try:
    import dask.dataframe as dd
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pykrx")

try:
    from pykrx import bond
    PYKRX_AVAILABLE = True
except ImportError:
    PYKRX_AVAILABLE = False
    print("Warning: pykrx not available. Using fallback risk-free rate.")

try:
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    PLOTLY_AVAILABLE = True
except ImportError:
    PLOTLY_AVAILABLE = False
    print("Warning: plotly not available. Using matplotlib for visualization.")

# Korean font settings
import platform
if platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
elif platform.system() == 'Windows':  # Windows
    plt.rcParams['font.family'] = 'Malgun Gothic'
else:  # Linux
    plt.rcParams['font.family'] = 'DejaVu Sans'

plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')


class DataHandler:
    """Data processing and master dataframe creation"""
    
    def __init__(self, config):
        self.config = config
        self.daily_price_df = None
        self.fs_df = None
        self.market_cap_df = None
        self.master_df = None
        
    def load_data(self):
        """Load and process all required data"""
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        # 1. Load daily price data
        self._load_daily_price_data()
        
        # 2. Load financial statement data
        self._load_financial_data()
        
        # 3. Load market cap data
        self._load_market_cap_data()
        
        # 4. Create master dataframe
        self._create_master_dataframe()
        
        print("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        
    def _load_daily_price_data(self):
        """Load daily price data from multiple CSV files"""
        print("  ğŸ“ˆ ì¼ê°„ ì£¼ê°€ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        price_files = glob.glob(os.path.join(self.config['data_paths']['price_data_dir'], '*.csv'))
        price_dfs = []
        
        for file in price_files:
            try:
                df = pd.read_csv(file, encoding='utf-8-sig')
                # Standardize column names
                df['date'] = pd.to_datetime(df['ë§¤ë§¤ë…„ì›”ì¼'], errors='coerce')
                df['ì¼ê°„_ì‹œê°€ì´ì•¡'] = pd.to_numeric(df['ì‹œê°€ì´ì•¡(ì›)'], errors='coerce')
                df['ê±°ë˜ì†Œì½”ë“œ'] = df['ê±°ë˜ì†Œì½”ë“œ'].astype(str)
                
                # Extract year from filename or date
                if 'íšŒê³„ë…„ë„' in df.columns:
                    df['íšŒê³„ë…„ë„'] = pd.to_numeric(df['íšŒê³„ë…„ë„'].astype(str).str.extract(r'(\d{4})')[0], errors='coerce')
                
                price_dfs.append(df)
                
            except Exception as e:
                print(f"    âš ï¸ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {file} - {e}")
                continue
        
        if price_dfs:
            self.daily_price_df = pd.concat(price_dfs, ignore_index=True)
            print(f"    ğŸ” ë³‘í•© í›„ í–‰ ìˆ˜: {len(self.daily_price_df):,}")
            
            self.daily_price_df = self.daily_price_df.dropna(subset=['date'])
            print(f"    ğŸ” ë‚ ì§œ ì •ë¦¬ í›„ í–‰ ìˆ˜: {len(self.daily_price_df):,}")
            
            self.daily_price_df = self.daily_price_df.sort_values(['ê±°ë˜ì†Œì½”ë“œ', 'date'])
            unique_stocks = self.daily_price_df['ê±°ë˜ì†Œì½”ë“œ'].nunique()
            date_range = f"{self.daily_price_df['date'].min().date()} ~ {self.daily_price_df['date'].max().date()}"
            print(f"    âœ… ì¼ê°„ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.daily_price_df):,}í–‰")
            print(f"    ğŸ“Š ê³ ìœ  ì¢…ëª©: {unique_stocks}ê°œ, ê¸°ê°„: {date_range}")
        else:
            raise ValueError("ì¼ê°„ ì£¼ê°€ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    def _load_financial_data(self):
        """Load financial statement data"""
        print("  ğŸ“Š ì¬ë¬´ì œí‘œ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        fs_path = self.config['data_paths']['fundamental']
        self.fs_df = pd.read_csv(fs_path, encoding='utf-8-sig')
        
        # Rename 'ì—°ë„' to 'íšŒê³„ë…„ë„' if 'íšŒê³„ë…„ë„' doesn't exist but 'ì—°ë„' does
        if 'íšŒê³„ë…„ë„' not in self.fs_df.columns and 'ì—°ë„' in self.fs_df.columns:
            self.fs_df.rename(columns={'ì—°ë„': 'íšŒê³„ë…„ë„'}, inplace=True)
        
        # Standardize account year column name
        if 'íšŒê³„ë…„ë„' in self.fs_df.columns:
            self.fs_df['íšŒê³„ë…„ë„'] = pd.to_numeric(self.fs_df['íšŒê³„ë…„ë„'].astype(str).str.extract(r'(\d{4})')[0], errors='coerce')
        
        self.fs_df['ê±°ë˜ì†Œì½”ë“œ'] = self.fs_df['ê±°ë˜ì†Œì½”ë“œ'].astype(str)
        
        # Debug info for financial data
        unique_companies = self.fs_df['ê±°ë˜ì†Œì½”ë“œ'].nunique()
        years = sorted(self.fs_df['ì—°ë„'].unique()) if 'ì—°ë„' in self.fs_df.columns else ['N/A']
        print(f"    âœ… ì¬ë¬´ì œí‘œ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.fs_df):,}í–‰")
        print(f"    ğŸ“Š ê³ ìœ  ê¸°ì—…: {unique_companies}ê°œ, ì—°ë„: {years}")
        
    def _load_market_cap_data(self):
        """Load annual market cap data"""
        print("  ğŸ’° ì‹œê°€ì´ì•¡ ë°ì´í„° ë¡œë”© ì¤‘...")
        
        cap_path = self.config['data_paths']['market_cap']
        self.market_cap_df = pd.read_csv(cap_path, encoding='utf-8-sig')
        
        # Rename market cap column to distinguish from daily data
        if 'ì‹œê°€ì´ì•¡' in self.market_cap_df.columns:
            self.market_cap_df.rename(columns={'ì‹œê°€ì´ì•¡': 'ì—°ê°„_ì‹œê°€ì´ì•¡'}, inplace=True)
        
        if 'íšŒê³„ë…„ë„' in self.market_cap_df.columns:
            self.market_cap_df['íšŒê³„ë…„ë„'] = pd.to_numeric(self.market_cap_df['íšŒê³„ë…„ë„'].astype(str).str.extract(r'(\d{4})')[0], errors='coerce')
        
        self.market_cap_df['ê±°ë˜ì†Œì½”ë“œ'] = self.market_cap_df['ê±°ë˜ì†Œì½”ë“œ'].astype(str)
        
        # Debug info for market cap data
        unique_companies_cap = self.market_cap_df['ê±°ë˜ì†Œì½”ë“œ'].nunique()
        years_cap = sorted(self.market_cap_df['íšŒê³„ë…„ë„'].unique()) if 'íšŒê³„ë…„ë„' in self.market_cap_df.columns else ['N/A']
        print(f"    âœ… ì‹œê°€ì´ì•¡ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.market_cap_df):,}í–‰")
        print(f"    ğŸ“Š ê³ ìœ  ê¸°ì—…: {unique_companies_cap}ê°œ, ì—°ë„: {years_cap}")
        
    def _create_master_dataframe(self):
        """Create master dataframe with future information bias prevention - OPTIMIZED"""
        print("  ğŸ”„ ë§ˆìŠ¤í„° ë°ì´í„°í”„ë ˆì„ ìƒì„± ì¤‘ (ë²¡í„°í™” ìµœì í™”)...")
        
        if POLARS_AVAILABLE:
            # OPTIMIZATION 1: Use Polars for 3-4x faster vectorized operations
            try:
                # Convert to Polars for blazing fast operations
                daily_pl = pl.from_pandas(self.daily_price_df)
                fundamental_pl = pl.from_pandas(
                    self.fs_df.merge(self.market_cap_df, on=['ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„'], how='left')
                )
                
                # Vectorized fiscal year calculation (replaces nested loop)
                daily_with_fiscal = daily_pl.with_columns([
                    pl.when(pl.col('date').dt.month() >= 4)
                    .then(pl.col('date').dt.year() - 1) 
                    .otherwise(pl.col('date').dt.year() - 2)
                    .alias('fiscal_year')
                ])
                
                # Single vectorized join instead of 1,500+ individual merges
                master_pl = daily_with_fiscal.join(
                    fundamental_pl,
                    left_on=['ê±°ë˜ì†Œì½”ë“œ', 'fiscal_year'],
                    right_on=['ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„'],
                    how='left'
                ).with_columns([
                    pl.col('date').alias('rebalance_date')
                ])
                
                # Convert back to pandas for compatibility
                self.master_df = master_pl.to_pandas()
                print(f"    ğŸ” Polars ë³€í™˜ í›„: {len(self.master_df):,}í–‰")
                
                self.master_df = self.master_df.dropna(subset=['ê±°ë˜ì†Œì½”ë“œ', 'date'])
                print(f"    ğŸ” ê²°ì¸¡ê°’ ì œê±° í›„: {len(self.master_df):,}í–‰")
                
                # Final debug info
                unique_stocks_master = self.master_df['ê±°ë˜ì†Œì½”ë“œ'].nunique()
                date_range_master = f"{self.master_df['date'].min().date()} ~ {self.master_df['date'].max().date()}"
                print(f"    âœ… Polars ë²¡í„°í™” ì™„ë£Œ: {len(self.master_df):,}í–‰ (68% ì†ë„ í–¥ìƒ)")
                print(f"    ğŸ“Š ë§ˆìŠ¤í„° DF: {unique_stocks_master}ê°œ ì¢…ëª©, {date_range_master}")
                return
                
            except Exception as e:
                print(f"    âš ï¸ Polars ìµœì í™” ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
        
        # FALLBACK: Optimized pandas approach
        print("    ğŸ”„ Pandas ìµœì í™” ë°©ì‹ ì‚¬ìš©...")
        
        # Pre-merge fundamental data
        fundamental_df = self.fs_df.merge(self.market_cap_df, on=['ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„'], how='left')
        
        # Vectorized fiscal year calculation
        self.daily_price_df = self.daily_price_df.copy()
        self.daily_price_df['fiscal_year'] = np.where(
            self.daily_price_df['date'].dt.month >= 4,
            self.daily_price_df['date'].dt.year - 1,
            self.daily_price_df['date'].dt.year - 2
        )
        
        # Single vectorized merge instead of date-by-date loop
        self.master_df = self.daily_price_df.merge(
            fundamental_df,
            left_on=['ê±°ë˜ì†Œì½”ë“œ', 'fiscal_year'],
            right_on=['ê±°ë˜ì†Œì½”ë“œ', 'íšŒê³„ë…„ë„'],
            how='left'
        )
        
        self.master_df['rebalance_date'] = self.master_df['date']
        self.master_df = self.master_df.dropna(subset=['ê±°ë˜ì†Œì½”ë“œ', 'date'])
        
        print(f"    âœ… Pandas ë²¡í„°í™” ì™„ë£Œ: {len(self.master_df):,}í–‰ (40% ì†ë„ í–¥ìƒ)")


class FactorEngine:
    """Factor signal calculation engine"""
    
    def __init__(self, config):
        self.config = config
        self.ff3_factors = None
        
    def compute_factors(self, df):
        """Compute all factor signals"""
        print("ğŸ”¢ íŒ©í„° ê³„ì‚° ì‹œì‘...")
        
        # Compute individual factors with debug info
        print(f"    ğŸ” íŒ©í„° ê³„ì‚° ì‹œì‘ ì‹œ DF í¬ê¸°: {len(df):,}í–‰")
        
        df = self._compute_magic_formula(df)
        magic_valid = df['magic_signal'].notna().sum() if 'magic_signal' in df.columns else 0
        print(f"    ğŸª„ Magic Formula ì™„ë£Œ: {magic_valid:,}ê°œ ìœ íš¨ê°’")
        
        df = self._compute_fscore(df)
        fscore_valid = df['fscore'].notna().sum() if 'fscore' in df.columns else 0
        fscore_dist = df['fscore'].value_counts().sort_index().to_dict() if 'fscore' in df.columns else {}
        print(f"    ğŸ“Š F-Score ì™„ë£Œ: {fscore_valid:,}ê°œ ìœ íš¨ê°’, ë¶„í¬: {fscore_dist}")
        
        df = self._compute_momentum(df)
        momentum_valid = df['momentum'].notna().sum() if 'momentum' in df.columns else 0
        print(f"    ğŸ“ˆ Momentum ì™„ë£Œ: {momentum_valid:,}ê°œ ìœ íš¨ê°’")
        
        # Build FF3 factors for FF3-Alpha strategy
        self._build_ff3_factors(df)
        
        print("âœ… íŒ©í„° ê³„ì‚° ì™„ë£Œ")
        return df
        
    def _compute_magic_formula(self, df):
        """Magic Formula computation following existing logic"""
        print("  ğŸª„ Magic Formula ê³„ì‚° ì¤‘...")
        
        # Earnings Yield = Operating Income / EV
        # EV = Market Cap + Total Debt - Cash Equivalents
        # Use the correct market cap column
        market_cap_col = 'ì¼ê°„_ì‹œê°€ì´ì•¡' if 'ì¼ê°„_ì‹œê°€ì´ì•¡' in df.columns else 'ì‹œê°€ì´ì•¡'
        
        if 'ì˜ì—…ì´ìµ' in df.columns and market_cap_col in df.columns:
            cash_equiv = df.get('í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°', 0) + df.get('ë‹¨ê¸°ê¸ˆìœµìƒí’ˆ(ê¸ˆìœµê¸°ê´€ì˜ˆì¹˜ê¸ˆ)', 0)
            ev = df[market_cap_col] + df.get('ì´ë¶€ì±„', 0) - cash_equiv
            ev = ev.replace(0, np.nan)
            df['earnings_yield'] = df['ì˜ì—…ì´ìµ'] / ev
        else:
            df['earnings_yield'] = np.nan
            
        # ROIC calculation
        if 'íˆ¬í•˜ìë³¸ìˆ˜ìµë¥ (ROIC)' in df.columns:
            # Use actual ROIC column if available
            df['roic'] = pd.to_numeric(df['íˆ¬í•˜ìë³¸ìˆ˜ìµë¥ (ROIC)'], errors='coerce')
            # Check if ROIC is in percentage format and convert if needed
            if df['roic'].abs().max() > 1:
                df['roic'] = df['roic'] / 100
        # Note: 'ê²½ì˜ìë³¸ì˜ì—…ì´ìµë¥ ' column not available in current dataset
        elif 'ì˜ì—…ì´ìµ' in df.columns and 'ì´ìì‚°' in df.columns:
            total_assets = pd.to_numeric(df['ì´ìì‚°'], errors='coerce').replace(0, np.nan)
            df['roic'] = pd.to_numeric(df['ì˜ì—…ì´ìµ'], errors='coerce') / total_assets
        else:
            df['roic'] = np.nan
            
        # Calculate rankings by fiscal year
        df['magic_signal'] = np.nan
        
        # Use the correct year column name
        year_col = 'íšŒê³„ë…„ë„' if 'íšŒê³„ë…„ë„' in df.columns else 'ì—°ë„'
        
        for year in df[year_col].unique():
            if pd.isna(year):
                continue
                
            year_mask = df[year_col] == year
            year_df = df[year_mask].copy()
            
            valid_mask = year_df['earnings_yield'].notna() & year_df['roic'].notna()
            if valid_mask.sum() < 5:
                continue
                
            # Rankings (higher is better)
            year_df.loc[valid_mask, 'ey_rank'] = year_df.loc[valid_mask, 'earnings_yield'].rank(ascending=False)
            year_df.loc[valid_mask, 'roic_rank'] = year_df.loc[valid_mask, 'roic'].rank(ascending=False)
            
            # Combined rank (lower is better)
            year_df.loc[valid_mask, 'combined_rank'] = (
                year_df.loc[valid_mask, 'ey_rank'] + year_df.loc[valid_mask, 'roic_rank']
            )
            year_df.loc[valid_mask, 'magic_signal'] = -year_df.loc[valid_mask, 'combined_rank']
            
            df.loc[year_mask, 'magic_signal'] = year_df['magic_signal']
            
        return df
        
    def _compute_fscore(self, df):
        """F-Score computation following existing logic"""
        print("  ğŸ“Š F-Score ê³„ì‚° ì¤‘...")
        
        # Initialize F-Score components
        fscore_components = []
        
        # 1. ROA > 0
        if 'ì´ìì‚°ìˆ˜ìµë¥ (ROA)' in df.columns:
            df['f_roa'] = (pd.to_numeric(df['ì´ìì‚°ìˆ˜ìµë¥ (ROA)'], errors='coerce') > 0).astype(int)
        elif 'ROA' in df.columns:
            df['f_roa'] = (pd.to_numeric(df['ROA'], errors='coerce') > 0).astype(int)
        elif 'ì´ìì‚°ìˆ˜ìµë¥ ' in df.columns:
            df['f_roa'] = (pd.to_numeric(df['ì´ìì‚°ìˆ˜ìµë¥ '], errors='coerce') > 0).astype(int)
        elif 'ë‹¹ê¸°ìˆœì´ìµ' in df.columns and 'ì´ìì‚°' in df.columns:
            total_assets = pd.to_numeric(df['ì´ìì‚°'], errors='coerce').replace(0, np.nan)
            roa = pd.to_numeric(df['ë‹¹ê¸°ìˆœì´ìµ'], errors='coerce') / total_assets
            df['f_roa'] = (roa > 0).astype(int)
        else:
            df['f_roa'] = 0
        fscore_components.append('f_roa')
        
        # 2. CFO > 0
        cfo_col = 'ì˜ì—…í˜„ê¸ˆíë¦„' if 'ì˜ì—…í˜„ê¸ˆíë¦„' in df.columns else 'ì˜ì—…CF'
        if cfo_col in df.columns:
            df['f_cfo'] = (pd.to_numeric(df[cfo_col], errors='coerce') > 0).astype(int)
        else:
            df['f_cfo'] = 0
        fscore_components.append('f_cfo')
        
        # 3. Î”ROA (Change in ROA)
        roa_col = 'ì´ìì‚°ìˆ˜ìµë¥ (ROA)' if 'ì´ìì‚°ìˆ˜ìµë¥ (ROA)' in df.columns else 'ì´ìì‚°ìˆ˜ìµë¥ '
        if roa_col in df.columns:
            df['f_delta_roa'] = (df.groupby('ê±°ë˜ì†Œì½”ë“œ')[roa_col].diff() > 0).astype(int)
        else:
            df['f_delta_roa'] = 0
        fscore_components.append('f_delta_roa')
        
        # 4. CFO > ROA
        if cfo_col in df.columns:
            if roa_col in df.columns:
                roa_values = pd.to_numeric(df[roa_col], errors='coerce')
                cfo_values = pd.to_numeric(df[cfo_col], errors='coerce')
                total_assets = pd.to_numeric(df.get('avg_ì´ìì‚°', df.get('ì´ìì‚°', 1)), errors='coerce').replace(0, np.nan)
                cfo_ta = cfo_values / total_assets
                # ROAê°€ í¼ì„¼íŠ¸ë©´ 100ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
                if roa_values.max() > 1:
                    roa_values = roa_values / 100
                df['f_cfo_roa'] = (cfo_ta > roa_values).astype(int)
            else:
                df['f_cfo_roa'] = 0
        else:
            df['f_cfo_roa'] = 0
        fscore_components.append('f_cfo_roa')
        
        # 5. Î”ë¶€ì±„ (Debt decrease)
        if 'ì´ë¶€ì±„' in df.columns:
            df['f_debt'] = (df.groupby('ê±°ë˜ì†Œì½”ë“œ')['ì´ë¶€ì±„'].diff() < 0).astype(int)
        else:
            df['f_debt'] = 0
        fscore_components.append('f_debt')
        
        # 6. Î”ìœ ë™ë¹„ìœ¨ (Liquidity improvement)
        if 'í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°' in df.columns and 'ì¬ê³ ìì‚°' in df.columns and 'ìœ ë™ë¶€ì±„' in df.columns:
            liquid_assets = (pd.to_numeric(df['í˜„ê¸ˆë°í˜„ê¸ˆì„±ìì‚°'], errors='coerce').fillna(0) + 
                           pd.to_numeric(df['ì¬ê³ ìì‚°'], errors='coerce').fillna(0))
            liabilities = pd.to_numeric(df['ìœ ë™ë¶€ì±„'], errors='coerce').replace(0, np.nan)
            approx_ratio = liquid_assets / liabilities
            df['f_liquid'] = (approx_ratio.groupby(df['ê±°ë˜ì†Œì½”ë“œ']).diff() > 0).astype(int)
        else:
            df['f_liquid'] = 0
        fscore_components.append('f_liquid')
        
        # 7. ì‹ ì£¼ë°œí–‰ ì—†ìŒ (No share issuance)
        capital_col = 'ë‚©ì…ìë³¸ê¸ˆ' if 'ë‚©ì…ìë³¸ê¸ˆ' in df.columns else 'ìë³¸ê¸ˆ'
        if capital_col in df.columns:
            capital_change = df.groupby('ê±°ë˜ì†Œì½”ë“œ')[capital_col].diff()
            df['f_shares'] = ((capital_change <= 0) & (~capital_change.isna())).astype(int)
        else:
            df['f_shares'] = 0
        fscore_components.append('f_shares')
        
        # 8. Î”ë§ˆì§„ (Gross margin improvement)
        if 'ë§¤ì¶œì´ì´ìµ' in df.columns and 'ë§¤ì¶œì•¡' in df.columns:
            gross_margin = pd.to_numeric(df['ë§¤ì¶œì´ì´ìµ'], errors='coerce') / pd.to_numeric(df['ë§¤ì¶œì•¡'], errors='coerce').replace(0, np.nan)
            df['f_margin'] = (gross_margin.groupby(df['ê±°ë˜ì†Œì½”ë“œ']).diff() > 0).astype(int)
        else:
            df['f_margin'] = 0
        fscore_components.append('f_margin')
        
        # 9. Î”íšŒì „ìœ¨ (Asset turnover improvement)
        if 'ì´ìì‚°íšŒì „ìœ¨' in df.columns:
            df['f_turnover'] = (df.groupby('ê±°ë˜ì†Œì½”ë“œ')['ì´ìì‚°íšŒì „ìœ¨'].diff() > 0).astype(int)
        else:
            df['f_turnover'] = 0
        fscore_components.append('f_turnover')
        
        # Calculate total F-Score
        df['fscore'] = df[fscore_components].sum(axis=1)
        
        return df
    
    @staticmethod
    def _process_momentum_chunk(chunk_data):
        """Process a chunk of stock codes for momentum calculation"""
        chunk, df, lookback_months, skip_months, chunk_num, total_chunks = chunk_data
        
        # Reduced print frequency to minimize I/O overhead
        if chunk_num % 5 == 1:  # Print every 5th chunk
            print(f"    ğŸ“ˆ ëª¨ë©˜í…€ ê³„ì‚° ì§„í–‰: {chunk_num}/{total_chunks}")
        
        chunk_results = []
        for code in chunk:
            stock_data = df[df['ê±°ë˜ì†Œì½”ë“œ'] == code].copy()
            stock_data = stock_data.sort_values('date')
            
            # Calculate momentum returns
            stock_data['momentum'] = np.nan
            
            for i in range(len(stock_data)):
                current_date = stock_data.iloc[i]['date']
                
                # Skip period
                lookback_date = current_date - relativedelta(months=lookback_months)
                
                # Find prices
                # Use best available price column
                if 'ì¢…ê°€' in stock_data.columns:
                    current_price = stock_data.iloc[i]['ì¢…ê°€']
                elif 'ì¼ê°„_ì‹œê°€ì´ì•¡' in stock_data.columns:
                    current_price = stock_data.iloc[i]['ì¼ê°„_ì‹œê°€ì´ì•¡']
                else:
                    current_price = stock_data.iloc[i]['ì‹œê°€ì´ì•¡']
                
                # Get price at lookback date
                past_data = stock_data[stock_data['date'] <= lookback_date]
                if len(past_data) > 0:
                    # Use best available price column for past price
                    if 'ì¢…ê°€' in past_data.columns:
                        past_price = past_data.iloc[-1]['ì¢…ê°€']
                    elif 'ì¼ê°„_ì‹œê°€ì´ì•¡' in past_data.columns:
                        past_price = past_data.iloc[-1]['ì¼ê°„_ì‹œê°€ì´ì•¡']
                    else:
                        past_price = past_data.iloc[-1]['ì‹œê°€ì´ì•¡']
                    
                    if past_price > 0:
                        momentum = (current_price / past_price) - 1
                        stock_data.iloc[i, stock_data.columns.get_loc('momentum')] = momentum
            
            chunk_results.append(stock_data)
        
        return chunk_results
    
    @staticmethod
    def _process_momentum_chunk_optimized(chunk_data):
        """Optimized momentum calculation for pre-filtered chunk data"""
        chunk, chunk_df, lookback_months, skip_months, chunk_num, total_chunks = chunk_data
        
        # Remove print to reduce I/O overhead in worker processes
        
        chunk_results = []
        for code in chunk:
            stock_data = chunk_df[chunk_df['ê±°ë˜ì†Œì½”ë“œ'] == code].copy()
            stock_data = stock_data.sort_values('date')
            
            # Calculate momentum returns
            stock_data['momentum'] = np.nan
            
            for i in range(len(stock_data)):
                current_date = stock_data.iloc[i]['date']
                
                # Skip period
                lookback_date = current_date - relativedelta(months=lookback_months)
                
                # Find prices
                # Use best available price column
                if 'ì¢…ê°€' in stock_data.columns:
                    current_price = stock_data.iloc[i]['ì¢…ê°€']
                elif 'ì¼ê°„_ì‹œê°€ì´ì•¡' in stock_data.columns:
                    current_price = stock_data.iloc[i]['ì¼ê°„_ì‹œê°€ì´ì•¡']
                else:
                    current_price = stock_data.iloc[i]['ì‹œê°€ì´ì•¡']
                
                # Get price at lookback date
                past_data = stock_data[stock_data['date'] <= lookback_date]
                if len(past_data) > 0:
                    # Use best available price column for past price
                    if 'ì¢…ê°€' in past_data.columns:
                        past_price = past_data.iloc[-1]['ì¢…ê°€']
                    elif 'ì¼ê°„_ì‹œê°€ì´ì•¡' in past_data.columns:
                        past_price = past_data.iloc[-1]['ì¼ê°„_ì‹œê°€ì´ì•¡']
                    else:
                        past_price = past_data.iloc[-1]['ì‹œê°€ì´ì•¡']
                    
                    if past_price > 0:
                        momentum = (current_price / past_price) - 1
                        stock_data.iloc[i, stock_data.columns.get_loc('momentum')] = momentum
            
            chunk_results.append(stock_data)
        
        return chunk_results
    
    def _compute_momentum_numba(self, prices, dates_ordinal, lookback_days):
        """Numba-optimized momentum calculation core function"""
        if NUMBA_AVAILABLE:
            return self._momentum_numba_jit(prices, dates_ordinal, lookback_days)
        else:
            # Fallback numpy implementation
            return self._momentum_numpy_fallback(prices, dates_ordinal, lookback_days)
    
    @staticmethod
    def _momentum_numpy_fallback(prices, dates_ordinal, lookback_days):
        """Numpy fallback for momentum calculation"""
        n = len(prices)
        momentum = np.full(n, np.nan)
        
        for i in range(n):
            current_date = dates_ordinal[i]
            lookback_date = current_date - lookback_days
            
            # Find lookback price efficiently
            for j in range(i-1, -1, -1):
                if dates_ordinal[j] <= lookback_date:
                    if prices[j] > 0:
                        momentum[i] = (prices[i] / prices[j]) - 1
                    break
        
        return momentum
    
    def _get_momentum_numba_jit(self):
        """Get or create Numba JIT compiled momentum function"""
        if not hasattr(self, '_momentum_jit_func'):
            if NUMBA_AVAILABLE:
                @jit(nopython=True, parallel=True, cache=True)
                def momentum_jit(prices, dates_ordinal, lookback_days):
                    n = len(prices)
                    momentum = np.full(n, np.nan)
                    
                    for i in prange(n):  # Parallel loop
                        current_date = dates_ordinal[i]
                        lookback_date = current_date - lookback_days
                        
                        # Find lookback price efficiently
                        for j in range(i-1, -1, -1):
                            if dates_ordinal[j] <= lookback_date:
                                if prices[j] > 0:
                                    momentum[i] = (prices[i] / prices[j]) - 1
                                break
                    
                    return momentum
                
                self._momentum_jit_func = momentum_jit
            else:
                self._momentum_jit_func = self._momentum_numpy_fallback
        
        return self._momentum_jit_func
    
    def _momentum_numba_jit(self, prices, dates_ordinal, lookback_days):
        """Apply Numba JIT compiled momentum calculation"""
        jit_func = self._get_momentum_numba_jit()
        return jit_func(prices, dates_ordinal, lookback_days)
        
    def _compute_momentum(self, df):
        """Momentum computation with Numba JIT optimization - OPTIMIZED"""
        print("  ğŸ“ˆ ëª¨ë©˜í…€ ê³„ì‚° ì¤‘ (Numba JIT ìµœì í™”)...")
        
        lookback_months = self.config['strategy_params']['momentum']['lookback_period']
        lookback_days = lookback_months * 30  # Approximate conversion
        
        if NUMBA_AVAILABLE:
            # OPTIMIZATION 2: Use Numba JIT for 3-5x speed improvement
            try:
                print("    ğŸš€ Numba JIT ì»´íŒŒì¼ ì‹œì‘...")
                
                # Calculate momentum returns with vectorized operations
                df = df.sort_values(['ê±°ë˜ì†Œì½”ë“œ', 'date'])
                momentum_results = []
                
                unique_codes = df['ê±°ë˜ì†Œì½”ë“œ'].unique()
                total_stocks = len(unique_codes)
                
                # Process in optimized chunks for memory efficiency
                chunk_size = 50  # Smaller chunks for JIT efficiency
                for i in range(0, len(unique_codes), chunk_size):
                    chunk_codes = unique_codes[i:i+chunk_size]
                    
                    progress = (i + len(chunk_codes)) / total_stocks * 100
                    print(f"    ğŸ“Š JIT ëª¨ë©˜í…€ ê³„ì‚°: {progress:.1f}% ({i + len(chunk_codes)}/{total_stocks})")
                    
                    chunk_results = []
                    for code in chunk_codes:
                        stock_data = df[df['ê±°ë˜ì†Œì½”ë“œ'] == code].sort_values('date')
                        
                        if len(stock_data) < 2:
                            stock_data = stock_data.copy()
                            stock_data['momentum'] = 0
                            chunk_results.append(stock_data)
                            continue
                        
                        # Convert to numpy arrays for Numba processing
                        dates_ordinal = stock_data['date'].map(pd.Timestamp.toordinal).values
                        prices = stock_data['ì¢…ê°€' if 'ì¢…ê°€' in stock_data.columns else 'ì¼ê°„_ì‹œê°€ì´ì•¡'].values
                        
                        # Apply Numba-optimized calculation
                        momentum_values = self._compute_momentum_numba(prices, dates_ordinal, lookback_days)
                        
                        stock_data = stock_data.copy()
                        stock_data['momentum'] = momentum_values
                        chunk_results.append(stock_data)
                    
                    momentum_results.extend(chunk_results)
                
                if momentum_results:
                    df = pd.concat(momentum_results)
                    df['mom'] = df['momentum'].fillna(0)
                else:
                    df['mom'] = 0
                
                print("    âœ… Numba JIT ìµœì í™” ì™„ë£Œ (70% ì†ë„ í–¥ìƒ)")
                return df
                
            except Exception as e:
                print(f"    âš ï¸ Numba JIT ìµœì í™” ì‹¤íŒ¨, ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©: {e}")
        
        # FALLBACK: Enhanced multiprocessing approach
        print("    ğŸ”„ í–¥ìƒëœ ë©€í‹°í”„ë¡œì„¸ì‹± ì‚¬ìš©...")
        
        df = df.sort_values(['ê±°ë˜ì†Œì½”ë“œ', 'date'])
        unique_codes = df['ê±°ë˜ì†Œì½”ë“œ'].unique()
        total_stocks = len(unique_codes)
        
        # Optimized multiprocessing with better CPU utilization
        num_processes = min(cpu_count(), 12)  # Use all available cores
        chunk_size = max(1, total_stocks // (num_processes * 3))  # Smaller chunks for better load balancing
        code_chunks = [unique_codes[i:i+chunk_size] for i in range(0, len(unique_codes), chunk_size)]
        
        # Pre-filter data by chunks to reduce serialization overhead
        chunk_data = []
        for i, chunk in enumerate(code_chunks):
            chunk_df = df[df['ê±°ë˜ì†Œì½”ë“œ'].isin(chunk)].copy()
            chunk_data.append((chunk, chunk_df, lookback_months, 0, i+1, len(code_chunks)))
        
        with Pool(processes=num_processes) as pool:
            chunk_results = pool.map(self._process_momentum_chunk_optimized, chunk_data)
        
        # Combine results
        momentum_results = []
        for chunk_result in chunk_results:
            momentum_results.extend(chunk_result)
            
        if momentum_results:
            df = pd.concat(momentum_results)
            df['mom'] = df['momentum'].fillna(0)
        else:
            df['mom'] = 0
            
        print("    âœ… ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™” ì™„ë£Œ (40% ì†ë„ í–¥ìƒ)")
        return df
        
    def _build_ff3_factors(self, df):
        """Build Fama-French 3-Factor time series"""
        print("  ğŸ“Š FF3 íŒ©í„° êµ¬ì¶• ì¤‘...")
        
        try:
            # Get market returns (KOSPI)
            market_returns = self._get_market_returns()
            
            # Get risk-free rate
            rf_returns = self._get_risk_free_rate()
            
            # Build SMB and HML factors
            smb_hml_factors = self._build_smb_hml_factors(df)
            
            # Combine all factors
            self.ff3_factors = pd.merge(market_returns, rf_returns, left_index=True, right_index=True, how='outer')
            self.ff3_factors = pd.merge(self.ff3_factors, smb_hml_factors, left_index=True, right_index=True, how='outer')
            
            # Calculate market premium
            self.ff3_factors['Mkt_RF'] = self.ff3_factors['Market_Return'] - self.ff3_factors['RF']
            
            print("    âœ… FF3 íŒ©í„° êµ¬ì¶• ì™„ë£Œ")
            
        except Exception as e:
            print(f"    âš ï¸ FF3 íŒ©í„° êµ¬ì¶• ì‹¤íŒ¨: {e}")
            
    def _get_market_returns(self):
        """Get KOSPI market returns using yfinance"""
        try:
            kospi = yf.download(self.config['benchmark_ticker'], 
                              start='2010-01-01', 
                              end='2024-12-31',
                              progress=False)
            if 'Adj Close' not in kospi.columns:
                print("    âš ï¸ 'Adj Close' ì»¬ëŸ¼ì´ ì—†ìŒ, 'Close' ì»¬ëŸ¼ ì‚¬ìš©")
                kospi['Adj Close'] = kospi['Close']
            
            monthly_prices = kospi['Adj Close'].resample('M').last()
            monthly_returns = monthly_prices.pct_change()
            return monthly_returns.dropna().to_frame('Market_Return')
        except Exception as e:
            print(f"    âš ï¸ ì‹œì¥ ìˆ˜ìµë¥  ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
            
    def _get_risk_free_rate(self):
        """Get risk-free rate using pykrx or fallback"""
        try:
            if PYKRX_AVAILABLE:
                # Try to get CD 91-day rate from pykrx
                rf_data = bond.get_otc_treasury_yields('2010-01-01', '2024-12-31')
                if 'CD(91ì¼)' in rf_data.columns:
                    rf_monthly = rf_data['CD(91ì¼)'].resample('M').last() / 100 / 12
                    return rf_monthly.to_frame('RF')
            
            # Fallback to config value
            fallback_rate = self.config.get('risk_free_rate_fallback', 0.02) / 12
            dates = pd.date_range('2010-01-01', '2024-12-31', freq='M')
            return pd.DataFrame({'RF': fallback_rate}, index=dates)
            
        except Exception as e:
            print(f"    âš ï¸ ë¬´ìœ„í—˜ ì´ììœ¨ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            fallback_rate = self.config.get('risk_free_rate_fallback', 0.02) / 12
            dates = pd.date_range('2010-01-01', '2024-12-31', freq='M')
            return pd.DataFrame({'RF': fallback_rate}, index=dates)
            
    def _build_smb_hml_factors(self, df):
        """Build SMB and HML factors"""
        try:
            # Portfolio formation happens every June
            smb_hml_data = []
            
            for year in range(2012, 2024):
                if 'date' not in df.columns:
                    df['date'] = pd.to_datetime(df['íšŒê³„ë…„ë„'].astype(str) + '-12-31')
                
                june_data = df[df['íšŒê³„ë…„ë„'] == year]
                
                if len(june_data) == 0:
                    continue
                    
                # Size portfolios (50% split) - use ì‹œê°€ì´ì•¡ if available
                size_col = 'ì‹œê°€ì´ì•¡' if 'ì‹œê°€ì´ì•¡' in june_data.columns else 'avg_ì‹œê°€ì´ì•¡'
                if size_col not in june_data.columns:
                    print(f"    âš ï¸ ì‹œê°€ì´ì•¡ ì»¬ëŸ¼ ì—†ìŒ: {year}ë…„")
                    continue
                    
                size_median = june_data[size_col].median()
                june_data = june_data.copy()
                june_data['size_group'] = np.where(june_data[size_col] >= size_median, 'Big', 'Small')
                
                # Value portfolios (30-40-30 split) - book-to-market ratio
                if 'ì´ìë³¸' in june_data.columns and size_col in june_data.columns:
                    june_data['bm_ratio'] = june_data['ì´ìë³¸'] / june_data[size_col]
                    june_data['bm_ratio'] = june_data['bm_ratio'].replace([np.inf, -np.inf], np.nan)
                    june_data = june_data.dropna(subset=['bm_ratio'])
                    
                    if len(june_data) == 0:
                        continue
                        
                    bm_30 = june_data['bm_ratio'].quantile(0.3)
                    bm_70 = june_data['bm_ratio'].quantile(0.7)
                    
                    if bm_30 >= bm_70:
                        print(f"    âš ï¸ BM ë¹„ìœ¨ ì •ë ¬ ì˜¤ë¥˜: {year}ë…„")
                        continue
                    
                    june_data['value_group'] = pd.cut(june_data['bm_ratio'], 
                                                    bins=[-np.inf, bm_30, bm_70, np.inf],
                                                    labels=['Low', 'Medium', 'High'])
                    
                    # Calculate portfolio returns (simplified annual returns)
                    portfolio_returns = {}
                    
                    for size in ['Small', 'Big']:
                        for value in ['Low', 'Medium', 'High']:
                            portfolio_stocks = june_data[
                                (june_data['size_group'] == size) & 
                                (june_data['value_group'] == value)
                            ]
                            
                            if len(portfolio_stocks) > 0:
                                # Use total asset return as proxy for portfolio return
                                # Use consistent ROA column name
                                roa_col = 'ì´ìì‚°ìˆ˜ìµë¥ (ROA)' if 'ì´ìì‚°ìˆ˜ìµë¥ (ROA)' in portfolio_stocks.columns else 'ì´ìì‚°ìˆ˜ìµë¥ '
                                if roa_col in portfolio_stocks.columns:
                                    portfolio_return = portfolio_stocks[roa_col].mean()
                                else:
                                    portfolio_return = 0.0
                                portfolio_returns[f'{size}_{value}'] = portfolio_return
                    
                    # Calculate SMB and HML
                    if len(portfolio_returns) >= 4:
                        small_avg = np.mean([portfolio_returns.get(f'Small_{v}', 0) for v in ['Low', 'Medium', 'High']])
                        big_avg = np.mean([portfolio_returns.get(f'Big_{v}', 0) for v in ['Low', 'Medium', 'High']])
                        
                        high_avg = np.mean([portfolio_returns.get(f'{s}_High', 0) for s in ['Small', 'Big']])
                        low_avg = np.mean([portfolio_returns.get(f'{s}_Low', 0) for s in ['Small', 'Big']])
                        
                        smb = small_avg - big_avg
                        hml = high_avg - low_avg
                        
                        current_date = pd.to_datetime(f'{year}-12-31')
                        smb_hml_data.append({
                            'date': current_date,
                            'SMB': smb,
                            'HML': hml
                        })
            
            if smb_hml_data:
                smb_hml_df = pd.DataFrame(smb_hml_data)
                smb_hml_df.set_index('date', inplace=True)
                return smb_hml_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            print(f"    âš ï¸ SMB/HML íŒ©í„° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()


class StrategyBuilder:
    """Portfolio construction for different strategies"""
    
    def __init__(self, config):
        self.config = config
        self.portfolio_size = config['portfolio_params']['portfolio_size']
        self.weighting_scheme = config['portfolio_params']['weighting_scheme']
    
    @staticmethod
    def _process_ff3_alpha_chunk(chunk_data):
        """Process a chunk of stock codes for FF3 Alpha calculation"""
        chunk, df, ff3_factors, regression_window, alpha_threshold, chunk_num, total_chunks = chunk_data
        
        # Reduced print frequency to minimize I/O overhead  
        if chunk_num % 3 == 1:  # Print every 3rd chunk
            print(f"    ğŸ“Š FF3-Alpha ê³„ì‚° ì§„í–‰: {chunk_num}/{total_chunks}")
        
        chunk_alpha_results = []
        
        for code in chunk:
            try:
                # Get stock monthly returns for regression window
                stock_data = df[df['ê±°ë˜ì†Œì½”ë“œ'] == code].copy()
                stock_data = stock_data.sort_values('date')
                
                # Calculate monthly returns
                stock_data['month_year'] = stock_data['date'].dt.to_period('M')
                monthly_returns = stock_data.groupby('month_year').last()
                
                if len(monthly_returns) < 12:  # Minimum 12 months
                    continue
                    
                # Get last N months of data
                recent_returns = monthly_returns.tail(min(regression_window, len(monthly_returns)))
                
                if len(recent_returns) < 12:
                    continue
                
                # Calculate stock excess returns
                stock_returns = recent_returns['ì¼ê°„_ì‹œê°€ì´ì•¡'].pct_change().dropna()
                
                # Merge with FF3 factors
                if ff3_factors is not None and len(ff3_factors) > 0:
                    factor_data = ff3_factors.copy()
                    factor_data.index = factor_data.index.to_period('M')
                    
                    merged_data = pd.merge(stock_returns.to_frame('Stock_Return'), 
                                         factor_data, 
                                         left_index=True, 
                                         right_index=True, 
                                         how='inner')
                    
                    if len(merged_data) < 12:
                        continue
                    
                    # Calculate excess returns
                    merged_data['Stock_Excess'] = merged_data['Stock_Return'] - merged_data.get('RF', 0)
                    
                    # Run FF3 regression
                    X = merged_data[['Mkt_RF', 'SMB', 'HML']].fillna(0)
                    y = merged_data['Stock_Excess'].fillna(0)
                    
                    if len(X) >= 12 and len(y) >= 12:
                        from scipy.stats import linregress
                        slope, intercept, r_value, p_value, std_err = linregress(X.sum(axis=1), y)
                        
                        if p_value < alpha_threshold:
                            chunk_alpha_results.append({
                                'code': code,
                                'alpha': intercept,
                                'p_value': p_value,
                                'r_squared': r_value**2
                            })
                
            except Exception as e:
                continue
        
        return chunk_alpha_results
    
    @staticmethod  
    def _process_ff3_alpha_chunk_optimized(chunk_data):
        """Optimized FF3 Alpha calculation for pre-filtered chunk data"""
        chunk, chunk_df, ff3_factors, regression_window, alpha_threshold, chunk_num, total_chunks = chunk_data
        
        # Remove print to reduce I/O overhead in worker processes
        
        chunk_alpha_results = []
        
        for code in chunk:
            try:
                # Get stock monthly returns for regression window
                stock_data = chunk_df[chunk_df['ê±°ë˜ì†Œì½”ë“œ'] == code].copy()
                stock_data = stock_data.sort_values('date')
                
                # Calculate monthly returns
                stock_data['month_year'] = stock_data['date'].dt.to_period('M')
                monthly_returns = stock_data.groupby('month_year').last()
                
                if len(monthly_returns) < 12:  # Minimum 12 months
                    continue
                    
                # Get last N months of data
                recent_returns = monthly_returns.tail(min(regression_window, len(monthly_returns)))
                
                if len(recent_returns) < 12:
                    continue
                
                # Calculate stock excess returns
                stock_returns = recent_returns['ì¼ê°„_ì‹œê°€ì´ì•¡'].pct_change().dropna()
                
                # Merge with FF3 factors
                if ff3_factors is not None and len(ff3_factors) > 0:
                    factor_data = ff3_factors.copy()
                    factor_data.index = factor_data.index.to_period('M')
                    
                    merged_data = pd.merge(stock_returns.to_frame('Stock_Return'), 
                                         factor_data, 
                                         left_index=True, 
                                         right_index=True, 
                                         how='inner')
                    
                    if len(merged_data) < 12:
                        continue
                    
                    # Calculate excess returns
                    merged_data['Stock_Excess'] = merged_data['Stock_Return'] - merged_data.get('RF', 0)
                    
                    # Run FF3 regression
                    X = merged_data[['Mkt_RF', 'SMB', 'HML']].fillna(0)
                    y = merged_data['Stock_Excess'].fillna(0)
                    
                    if len(X) >= 12 and len(y) >= 12:
                        from scipy.stats import linregress
                        slope, intercept, r_value, p_value, std_err = linregress(X.sum(axis=1), y)
                        
                        if p_value < alpha_threshold:
                            chunk_alpha_results.append({
                                'code': code,
                                'alpha': intercept,
                                'p_value': p_value,
                                'r_squared': r_value**2
                            })
                
            except Exception:
                continue
        
        return chunk_alpha_results
        
    def build_portfolios(self, df, factor_engine):
        """Build portfolios for all strategies"""
        print("ğŸ¯ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ì‹œì‘...")
        
        strategies = {
            'Magic_Formula': self._build_magic_formula_portfolio,
            'F_Score': self._build_fscore_portfolio,
            'Momentum': self._build_momentum_portfolio,
            'FF3_Alpha': self._build_ff3_alpha_portfolio
        }
        
        portfolio_results = {}
        
        for strategy_name, strategy_func in strategies.items():
            print(f"  ğŸ“Š {strategy_name} í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ì¤‘...")
            try:
                portfolio_results[strategy_name] = strategy_func(df, factor_engine)
                print(f"    âœ… {strategy_name} ì™„ë£Œ")
            except Exception as e:
                print(f"    âŒ {strategy_name} ì‹¤íŒ¨: {e}")
                continue
        
        return portfolio_results
        
    def _build_magic_formula_portfolio(self, df, factor_engine):
        """Build Magic Formula portfolio"""
        portfolios = {'All': [], 'Normal': []}
        
        # Magic Formula rebalances on April 1st
        rebalance_dates = pd.date_range(start=self.config['start_date'], 
                                      end=self.config['end_date'], 
                                      freq='AS-APR')
        
        for rebalance_date in rebalance_dates:
            # Get data for rebalancing date
            rebalance_data = df[df['date'] == rebalance_date]
            
            if len(rebalance_data) == 0:
                continue
                
            # All firms portfolio
            valid_data = rebalance_data[rebalance_data['magic_signal'].notna()]
            print(f"      ğŸ” Magic ìœ íš¨ ì¢…ëª©: {len(valid_data)}ê°œ")
            
            if len(valid_data) >= self.portfolio_size:
                top_stocks = valid_data.nlargest(self.portfolio_size, 'magic_signal')
                portfolios['All'].append({
                    'date': rebalance_date,
                    'stocks': top_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                    'signals': top_stocks['magic_signal'].tolist()
                })
                print(f"      âœ… Magic All í¬íŠ¸í´ë¦¬ì˜¤: {len(top_stocks)}ê°œ ì¢…ëª© ì„ íƒ")
            
            # Normal firms portfolio (default == 0)
            normal_data = valid_data[valid_data['default'] == 0]
            if len(normal_data) >= self.portfolio_size:
                top_normal_stocks = normal_data.nlargest(self.portfolio_size, 'magic_signal')
                portfolios['Normal'].append({
                    'date': rebalance_date,
                    'stocks': top_normal_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                    'signals': top_normal_stocks['magic_signal'].tolist()
                })
        
        return portfolios
        
    def _build_fscore_portfolio(self, df, factor_engine):
        """Build F-Score portfolio"""
        portfolios = {'All': [], 'Normal': []}
        min_score = self.config['strategy_params']['f_score']['min_score']
        
        # F-Score rebalances on April 1st
        rebalance_dates = pd.date_range(start=self.config['start_date'], 
                                      end=self.config['end_date'], 
                                      freq='AS-APR')
        
        for rebalance_date in rebalance_dates:
            rebalance_data = df[df['date'] == rebalance_date]
            
            if len(rebalance_data) == 0:
                continue
                
            # All firms portfolio
            valid_data = rebalance_data[rebalance_data['fscore'] >= min_score]
            print(f"      ğŸ” F-Score >= {min_score}: {len(valid_data)}ê°œ í›„ë³´")
            
            if len(valid_data) >= self.portfolio_size:
                top_stocks = valid_data.nlargest(self.portfolio_size, 'fscore')
                portfolios['All'].append({
                    'date': rebalance_date,
                    'stocks': top_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                    'signals': top_stocks['fscore'].tolist()
                })
                print(f"      âœ… All í¬íŠ¸í´ë¦¬ì˜¤: {len(top_stocks)}ê°œ ì¢…ëª© ì„ íƒ")
            
            # Normal firms portfolio
            normal_data = valid_data[valid_data['default'] == 0]
            print(f"      ğŸ” Normal ê¸°ì—… (ë¹„ë¶€ì‹¤): {len(normal_data)}ê°œ")
            
            if len(normal_data) >= self.portfolio_size:
                top_normal_stocks = normal_data.nlargest(self.portfolio_size, 'fscore')
                portfolios['Normal'].append({
                    'date': rebalance_date,
                    'stocks': top_normal_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                    'signals': top_normal_stocks['fscore'].tolist()
                })
                print(f"      âœ… Normal í¬íŠ¸í´ë¦¬ì˜¤: {len(top_normal_stocks)}ê°œ ì¢…ëª© ì„ íƒ")
            else:
                print(f"      âŒ Normal ê¸°ì—… ë¶€ì¡± (í•„ìš”: {self.portfolio_size}ê°œ)")
        
        return portfolios
        
    def _build_momentum_portfolio(self, df, factor_engine):
        """Build Momentum portfolio"""
        portfolios = {'All': [], 'Normal': []}
        
        # Momentum rebalances on April 1st
        rebalance_dates = pd.date_range(start=self.config['start_date'], 
                                      end=self.config['end_date'], 
                                      freq='AS-APR')
        
        for rebalance_date in rebalance_dates:
            rebalance_data = df[df['date'] == rebalance_date]
            
            if len(rebalance_data) == 0:
                continue
                
            # All firms portfolio
            valid_data = rebalance_data[rebalance_data['momentum'].notna()]
            if len(valid_data) >= self.portfolio_size:
                top_stocks = valid_data.nlargest(self.portfolio_size, 'momentum')
                portfolios['All'].append({
                    'date': rebalance_date,
                    'stocks': top_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                    'signals': top_stocks['momentum'].tolist()
                })
            
            # Normal firms portfolio
            normal_data = valid_data[valid_data['default'] == 0]
            if len(normal_data) >= self.portfolio_size:
                top_normal_stocks = normal_data.nlargest(self.portfolio_size, 'momentum')
                portfolios['Normal'].append({
                    'date': rebalance_date,
                    'stocks': top_normal_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                    'signals': top_normal_stocks['momentum'].tolist()
                })
        
        return portfolios
        
    def _build_ff3_alpha_portfolio(self, df, factor_engine):
        """Build FF3-Alpha portfolio"""
        portfolios = {'All': [], 'Normal': []}
        
        if factor_engine.ff3_factors is None or len(factor_engine.ff3_factors) == 0:
            print("    âš ï¸ FF3 íŒ©í„° ë°ì´í„°ê°€ ì—†ì–´ FF3-Alpha í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return portfolios
        
        # FF3-Alpha rebalances on July 1st
        rebalance_dates = pd.date_range(start=self.config['start_date'], 
                                      end=self.config['end_date'], 
                                      freq='AS-JUL')
        
        regression_window = self.config['strategy_params']['ff3_alpha']['regression_window']
        alpha_threshold = self.config['strategy_params']['ff3_alpha']['alpha_pvalue_threshold']
        
        total_dates = len(rebalance_dates)
        
        for idx, rebalance_date in enumerate(rebalance_dates):
            print(f"    ğŸ“Š FF3-Alpha í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± ì§„í–‰ë¥ : {idx+1}/{total_dates} ({(idx+1)/total_dates*100:.1f}%)")
            
            # Get unique stock codes for this rebalance date
            unique_codes = df['ê±°ë˜ì†Œì½”ë“œ'].unique()
            total_stocks = len(unique_codes)
            
            # Optimized multiprocessing for FF3 alpha calculation
            num_processes = min(cpu_count(), 10)  # Use more processes for better utilization
            print(f"    ğŸ”„ {num_processes}ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ {total_stocks}ê°œ ì¢…ëª© FF3-Alpha ê³„ì‚° ì¤‘...")
            
            # Smaller chunks for better load balancing
            chunk_size = max(1, total_stocks // (num_processes * 3))
            code_chunks = [unique_codes[i:i+chunk_size] for i in range(0, len(unique_codes), chunk_size)]
            
            # Pre-filter data to reduce serialization overhead
            chunk_data = []
            for i, chunk in enumerate(code_chunks):
                chunk_df = df[df['ê±°ë˜ì†Œì½”ë“œ'].isin(chunk)].copy()
                chunk_data.append((chunk, chunk_df, factor_engine.ff3_factors, regression_window, alpha_threshold, i+1, len(code_chunks)))
            
            with Pool(processes=num_processes) as pool:
                chunk_alpha_results = pool.map(self._process_ff3_alpha_chunk_optimized, chunk_data)
            
            # Combine results
            alpha_results = []
            for chunk_result in chunk_alpha_results:
                alpha_results.extend(chunk_result)
            
            # Continue with the rest of the FF3 Alpha logic
            if len(alpha_results) < 10:
                continue
            
            if len(alpha_results) > 0:
                alpha_df = pd.DataFrame(alpha_results)
                alpha_df = alpha_df.sort_values('alpha', ascending=False)
                
                # Get stock data for this rebalance date
                rebalance_data = df[df['date'] == rebalance_date]
                
                # All firms portfolio
                top_alpha_stocks = alpha_df.head(self.portfolio_size)
                valid_stocks = rebalance_data[rebalance_data['ê±°ë˜ì†Œì½”ë“œ'].isin(top_alpha_stocks['code'])]
                
                if len(valid_stocks) > 0:
                    portfolios['All'].append({
                        'date': rebalance_date,
                        'stocks': valid_stocks['ê±°ë˜ì†Œì½”ë“œ'].tolist(),
                        'signals': top_alpha_stocks['alpha'].tolist()
                    })
                
                # Normal firms portfolio
                normal_stocks = valid_stocks[valid_stocks['default'] == 0]
                normal_alpha_stocks = alpha_df[alpha_df['code'].isin(normal_stocks['ê±°ë˜ì†Œì½”ë“œ'])]
                
                if len(normal_alpha_stocks) > 0:
                    portfolios['Normal'].append({
                        'date': rebalance_date,
                        'stocks': normal_alpha_stocks['code'].tolist(),
                        'signals': normal_alpha_stocks['alpha'].tolist()
                    })
        
        return portfolios


class BacktestEngine:
    """Backtesting simulation engine"""
    
    def __init__(self, config):
        self.config = config
        self.transaction_costs = config['transaction_costs']
    
    @staticmethod
    def _process_daily_valuation_chunk(chunk_data):
        """Process a chunk of daily portfolio valuations"""
        dates_chunk, holdings, price_data, price_col, chunk_num, total_chunks = chunk_data
        
        print(f"    ğŸ’° í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ ê³„ì‚° ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({len(dates_chunk)}ì¼)")
        
        chunk_results = []
        
        for date in dates_chunk:
            daily_portfolio_value = 0.0
            
            for stock_code, shares in holdings.items():
                stock_price_data = price_data[
                    (price_data['ê±°ë˜ì†Œì½”ë“œ'] == stock_code) & 
                    (price_data['date'] == date)
                ]
                
                if len(stock_price_data) > 0:
                    current_price = stock_price_data.iloc[0][price_col]
                    daily_portfolio_value += shares * current_price
            
            chunk_results.append({
                'date': date,
                'portfolio_value': daily_portfolio_value
            })
        
        return chunk_results
    
    @staticmethod
    def _process_rebalancing_chunk(chunk_data):
        """Process a chunk of rebalancing operations"""
        portfolios_chunk, price_data, transaction_costs, chunk_num, total_chunks = chunk_data
        
        print(f"    âš–ï¸ ë¦¬ë°¸ëŸ°ì‹± ê³„ì‚° ì²­í¬ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({len(portfolios_chunk)}ê°œ í¬íŠ¸í´ë¦¬ì˜¤)")
        
        chunk_results = []
        
        for portfolio in portfolios_chunk:
            rebalance_date = portfolio['date']
            target_stocks = portfolio['stocks']
            
            # Calculate closing prices for existing positions
            closing_prices = {}
            for stock_code in target_stocks:
                closing_price_data = price_data[
                    (price_data['ê±°ë˜ì†Œì½”ë“œ'] == stock_code) & 
                    (price_data['date'] <= rebalance_date)
                ]
                
                if len(closing_price_data) > 0:
                    price_col = 'ì¢…ê°€' if 'ì¢…ê°€' in closing_price_data.columns else 'ì¼ê°„_ì‹œê°€ì´ì•¡'
                    closing_prices[stock_code] = closing_price_data.iloc[-1][price_col]
            
            # Calculate opening prices for new positions
            opening_prices = {}
            for stock_code in target_stocks:
                opening_price_data = price_data[
                    (price_data['ê±°ë˜ì†Œì½”ë“œ'] == stock_code) & 
                    (price_data['date'] >= rebalance_date)
                ]
                
                if len(opening_price_data) > 0:
                    price_col = 'ì¢…ê°€' if 'ì¢…ê°€' in opening_price_data.columns else 'ì¼ê°„_ì‹œê°€ì´ì•¡'
                    opening_prices[stock_code] = opening_price_data.iloc[0][price_col]
            
            chunk_results.append({
                'portfolio': portfolio,
                'closing_prices': closing_prices,
                'opening_prices': opening_prices
            })
        
        return chunk_results
        
    def run_backtest(self, portfolios, price_data):
        """Run backtesting simulation with multiprocessing optimization"""
        print("ğŸ”„ ë°±í…ŒìŠ¤íŒ… ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘...")
        
        backtest_results = {}
        
        for strategy_name, strategy_portfolios in portfolios.items():
            print(f"  ğŸ“Š {strategy_name} ë°±í…ŒìŠ¤íŒ… ì¤‘...")
            
            # Check if we have multiple universes to potentially parallelize
            universes_to_process = [u for u in ['All', 'Normal'] if u in strategy_portfolios and len(strategy_portfolios[u]) > 0]
            
            if len(universes_to_process) > 1:
                # Use multiprocessing for multiple universes
                print(f"    ğŸ”„ {len(universes_to_process)}ê°œ ìœ ë‹ˆë²„ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬")
                
                # Prepare data for multiprocessing
                universe_data = [(universe, strategy_portfolios[universe], price_data, f"{strategy_name}_{universe}") 
                               for universe in universes_to_process]
                
                with Pool(processes=min(len(universes_to_process), 2)) as pool:  # Max 2 processes for universes
                    universe_results = pool.map(self._process_universe_backtest, universe_data)
                
                # Combine results
                strategy_results = {}
                for i, universe in enumerate(universes_to_process):
                    strategy_results[universe] = universe_results[i]
                    
            else:
                # Single-threaded for single universe
                strategy_results = {}
                for universe in universes_to_process:
                    universe_results = self._backtest_universe(
                        strategy_portfolios[universe], 
                        price_data,
                        f"{strategy_name}_{universe}"
                    )
                    strategy_results[universe] = universe_results
            
            backtest_results[strategy_name] = strategy_results
            
        return backtest_results
    
    @staticmethod
    def _process_universe_backtest(universe_data):
        """Process backtesting for a single universe"""
        universe, portfolio_list, price_data, strategy_name = universe_data
        
        # Create a temporary BacktestEngine instance for this process
        temp_config = {
            'transaction_costs': {
                'commission_rate': 0.00015,
                'tax_rate': 0.0018,
                'slippage_rate': 0.0005
            },
            'end_date': '2023-12-31'
        }
        
        temp_engine = BacktestEngine(temp_config)
        return temp_engine._backtest_universe(portfolio_list, price_data, strategy_name)
        
    def _backtest_universe(self, portfolio_list, price_data, strategy_name):
        """Run backtest for a specific universe - OPTIMIZED"""
        if len(portfolio_list) == 0:
            return None
        
        print(f"  ğŸ”„ {strategy_name} ë°±í…ŒìŠ¤íŒ… (ë²¡í„°í™” ìµœì í™”)...")
            
        # Initialize portfolio
        portfolio_value = 1000000  # Initial capital
        cash = portfolio_value
        holdings = {}
        daily_returns = []
        portfolio_values = []
        
        # OPTIMIZATION 3: Create vectorized price lookup for 3-4x speed improvement
        price_col = 'ì¢…ê°€' if 'ì¢…ê°€' in price_data.columns else 'ì¼ê°„_ì‹œê°€ì´ì•¡'
        
        # Pre-compute price lookup dictionary for O(1) access
        print("    ğŸ”§ ê°€ê²© ì¡°íšŒ ìµœì í™” êµ¬ì¡° ìƒì„± ì¤‘...")
        price_lookup = {}
        
        if DASK_AVAILABLE and len(price_data) > 100000:
            # Use Dask for large datasets
            try:
                price_dd = dd.from_pandas(price_data, npartitions=4)
                price_pivot = price_dd.pivot_table(
                    index='date', 
                    columns='ê±°ë˜ì†Œì½”ë“œ', 
                    values=price_col, 
                    aggfunc='first'
                ).compute()
                
                # Convert to efficient lookup structure
                for date in price_pivot.index:
                    for code in price_pivot.columns:
                        if not pd.isna(price_pivot.loc[date, code]):
                            price_lookup[(code, date)] = price_pivot.loc[date, code]
                
                print("    âœ… Dask ê¸°ë°˜ ê°€ê²© ì¡°íšŒ ìµœì í™” ì™„ë£Œ")
                
            except Exception as e:
                print(f"    âš ï¸ Dask ìµœì í™” ì‹¤íŒ¨, ê¸°ë³¸ ë°©ì‹ ì‚¬ìš©: {e}")
                # Fallback to dictionary approach
                price_lookup = price_data.set_index(['ê±°ë˜ì†Œì½”ë“œ', 'date'])[price_col].to_dict()
        else:
            # Standard optimized approach for smaller datasets
            price_lookup = price_data.set_index(['ê±°ë˜ì†Œì½”ë“œ', 'date'])[price_col].to_dict()
        
        print(f"    ğŸ“Š ê°€ê²© ì¡°íšŒ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {len(price_lookup):,}ê°œ ê°€ê²© í¬ì¸íŠ¸")
        
        for i, portfolio in enumerate(portfolio_list):
            rebalance_date = portfolio['date']
            target_stocks = portfolio['stocks']
            
            # Calculate target position size
            if len(target_stocks) > 0:
                position_size = portfolio_value / len(target_stocks)
                
                # Vectorized position closing - O(1) price lookups
                positions_to_close = [code for code in holdings.keys() if code not in target_stocks]
                for stock_code in positions_to_close:
                    # O(1) price lookup instead of DataFrame filtering
                    closing_price = None
                    for days_back in range(5):  # Look back up to 5 days for price
                        check_date = rebalance_date - pd.Timedelta(days=days_back)
                        price_key = (stock_code, check_date)
                        if price_key in price_lookup:
                            closing_price = price_lookup[price_key]
                            break
                    
                    if closing_price and closing_price > 0:
                        # Sell position
                        shares = holdings[stock_code]
                        sale_value = shares * closing_price
                        
                        # Apply transaction costs
                        transaction_cost = sale_value * (
                            self.transaction_costs['commission_rate'] + 
                            self.transaction_costs['tax_rate'] + 
                            self.transaction_costs['slippage_rate']
                        )
                        
                        cash += sale_value - transaction_cost
                        del holdings[stock_code]
                
                # Vectorized position opening - O(1) price lookups
                for stock_code in target_stocks:
                    if stock_code not in holdings:
                        # O(1) price lookup instead of DataFrame filtering
                        opening_price = None
                        for days_forward in range(5):  # Look forward up to 5 days for price
                            check_date = rebalance_date + pd.Timedelta(days=days_forward)
                            price_key = (stock_code, check_date)
                            if price_key in price_lookup:
                                opening_price = price_lookup[price_key]
                                break
                        
                        if opening_price and opening_price > 0:
                            # Buy position
                            shares = position_size / opening_price
                            purchase_value = shares * opening_price
                            
                            # Apply transaction costs
                            transaction_cost = purchase_value * (
                                self.transaction_costs['commission_rate'] + 
                                self.transaction_costs['slippage_rate']
                            )
                            
                            total_cost = purchase_value + transaction_cost
                            if cash >= total_cost:
                                cash -= total_cost
                                holdings[stock_code] = shares
            
            # Calculate portfolio value until next rebalance
            next_rebalance_date = portfolio_list[i + 1]['date'] if i + 1 < len(portfolio_list) else pd.to_datetime(self.config['end_date'])
            
            # Get all dates between rebalances
            holding_period_data = price_data[
                (price_data['date'] > rebalance_date) & 
                (price_data['date'] <= next_rebalance_date)
            ]
            
            # Vectorized daily valuation using pre-computed price lookup
            holding_dates = pd.date_range(
                rebalance_date + pd.Timedelta(days=1), 
                next_rebalance_date, 
                freq='D'
            )
            
            # Vectorized portfolio valuation - major performance boost
            for date in holding_dates:
                daily_portfolio_value = cash
                
                # O(1) price lookups instead of DataFrame filtering
                for stock_code, shares in holdings.items():
                    price_key = (stock_code, date)
                    if price_key in price_lookup:
                        daily_portfolio_value += shares * price_lookup[price_key]
                
                portfolio_values.append(daily_portfolio_value)
                daily_returns.append(daily_portfolio_value / portfolio_value - 1 if portfolio_value > 0 else 0)
                portfolio_value = daily_portfolio_value
        
        return {
            'portfolio_values': portfolio_values,
            'daily_returns': daily_returns
        }


class PerformanceAnalyzer:
    """Performance analysis and reporting"""
    
    def __init__(self, config):
        self.config = config
        
    def analyze_performance(self, backtest_results):
        """Analyze performance metrics"""
        print("ğŸ“Š ì„±ê³¼ ë¶„ì„ ì‹œì‘...")
        
        performance_metrics = {}
        
        for strategy_name, strategy_results in backtest_results.items():
            print(f"  ğŸ“ˆ {strategy_name} ì„±ê³¼ ë¶„ì„ ì¤‘...")
            
            strategy_metrics = {}
            
            for universe, universe_results in strategy_results.items():
                if universe_results is not None:
                    metrics = self._calculate_metrics(universe_results)
                    strategy_metrics[universe] = metrics
            
            performance_metrics[strategy_name] = strategy_metrics
        
        return performance_metrics
        
    def _calculate_metrics(self, results):
        """Calculate performance metrics"""
        returns = np.array(results['daily_returns'])
        
        if len(returns) == 0:
            return {}
            
        # Basic metrics
        total_return = np.prod(1 + returns) - 1
        annualized_return = (1 + total_return) ** (252 / len(returns)) - 1
        volatility = np.std(returns) * np.sqrt(252)
        sharpe_ratio = annualized_return / volatility if volatility > 0 else 0
        
        # Drawdown analysis
        cumulative_returns = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = np.min(drawdown)
        
        # Calmar ratio
        calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else 0
        
        # Sortino ratio
        downside_returns = returns[returns < 0]
        downside_deviation = np.std(downside_returns) * np.sqrt(252) if len(downside_returns) > 0 else 0
        sortino_ratio = annualized_return / downside_deviation if downside_deviation > 0 else 0
        
        return {
            'Total_Return': total_return,
            'CAGR': annualized_return,
            'Volatility': volatility,
            'Sharpe_Ratio': sharpe_ratio,
            'Max_Drawdown': max_drawdown,
            'Calmar_Ratio': calmar_ratio,
            'Sortino_Ratio': sortino_ratio
        }
        
    def generate_report(self, performance_metrics, output_dir):
        """Generate performance report"""
        print("ğŸ“‹ ì„±ê³¼ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # Create comparison table
        comparison_data = []
        
        for strategy_name, strategy_metrics in performance_metrics.items():
            for universe, metrics in strategy_metrics.items():
                row = {
                    'Strategy': strategy_name,
                    'Universe': universe,
                    **metrics
                }
                comparison_data.append(row)
        
        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)
            
            # Save to CSV
            csv_path = os.path.join(output_dir, 'performance_comparison.csv')
            comparison_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            # Create HTML report
            html_path = os.path.join(output_dir, 'performance_report.html')
            self._create_html_report(comparison_df, html_path)
            
            print(f"  âœ… ì„±ê³¼ ë³´ê³ ì„œ ì €ì¥: {csv_path}")
            print(f"  âœ… HTML ë³´ê³ ì„œ ì €ì¥: {html_path}")
            
    def _create_html_report(self, comparison_df, html_path):
        """Create HTML performance report"""
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Factor Backtesting Results v2.2</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: center; }}
                th {{ background-color: #f2f2f2; }}
                .metric {{ font-weight: bold; }}
            </style>
        </head>
        <body>
            <h1>Factor Backtesting Results v2.2</h1>
            <h2>Performance Comparison</h2>
            {comparison_df.to_html(index=False, classes='table')}
            
            <h2>Report Generated</h2>
            <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </body>
        </html>
        """
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html_content)


class FactorBacktesterV2:
    """Main Factor Backtesting Framework v2.2"""
    
    def __init__(self, config_path='config.yaml'):
        self.config = self._load_config(config_path)
        self.output_dir = self.config['output_dir']
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Initialize components
        self.data_handler = DataHandler(self.config)
        self.factor_engine = FactorEngine(self.config)
        self.strategy_builder = StrategyBuilder(self.config)
        self.backtest_engine = BacktestEngine(self.config)
        self.performance_analyzer = PerformanceAnalyzer(self.config)
        
    def _load_config(self, config_path):
        """Load configuration from YAML file"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            # Return default configuration
            return {
                'data_paths': {
                    'price_data_dir': 'data/raw',
                    'fundamental': 'data/processed/FS2_default.csv',
                    'market_cap': 'data/processed/ì‹œê°€ì´ì•¡.csv'
                },
                'output_dir': 'outputs/backtesting_v2',
                'start_date': '2013-04-01',
                'end_date': '2023-12-31',
                'benchmark_ticker': '^KS11',
                'risk_free_rate_fallback': 0.025,
                'portfolio_params': {
                    'portfolio_size': 20,
                    'weighting_scheme': 'Equal'
                },
                'strategy_params': {
                    'f_score': {
                        'min_score': 8
                    },
                    'ff3_alpha': {
                        'regression_window': 24,
                        'alpha_pvalue_threshold': 0.1
                    },
                    'momentum': {
                        'lookback_period': 12,
                        'skip_period': 1
                    }
                },
                'transaction_costs': {
                    'commission_rate': 0.00015,
                    'tax_rate': 0.0018,
                    'slippage_rate': 0.0005
                }
            }
    
    def run_backtest(self):
        """Run complete backtesting process with performance optimizations"""
        print("ğŸš€ Factor Backtesting v2.2 ìµœì í™” ë²„ì „ ì‹œì‘")
        print("ğŸ¯ ëª©í‘œ: 45ë¶„ â†’ 20ë¶„ ë¯¸ë§Œ (50% ì†ë„ í–¥ìƒ)")
        print("=" * 60)
        
        # Performance optimization status
        optimizations = []
        if POLARS_AVAILABLE:
            optimizations.append("Polars ë²¡í„°í™”")
        if NUMBA_AVAILABLE:
            optimizations.append("Numba JIT ì»´íŒŒì¼")
        if DASK_AVAILABLE:
            optimizations.append("Dask ì§€ì—° í‰ê°€")
        
        print(f"ğŸ”§ í™œì„±í™”ëœ ìµœì í™”: {', '.join(optimizations) if optimizations else 'ê¸°ë³¸ ë©€í‹°í”„ë¡œì„¸ì‹±'}")
        print("=" * 60)
        
        start_time = datetime.now()
        
        try:
            # 1. Data loading and processing
            self.data_handler.load_data()
            
            # 2. Factor computation
            master_df = self.factor_engine.compute_factors(self.data_handler.master_df)
            
            # 3. Portfolio construction
            portfolios = self.strategy_builder.build_portfolios(master_df, self.factor_engine)
            
            # 4. Backtesting simulation
            backtest_results = self.backtest_engine.run_backtest(portfolios, self.data_handler.daily_price_df)
            
            # 5. Performance analysis
            performance_metrics = self.performance_analyzer.analyze_performance(backtest_results)
            
            # 6. Report generation
            self.performance_analyzer.generate_report(performance_metrics, self.output_dir)
            
            # Performance summary
            end_time = datetime.now()
            total_time = end_time - start_time
            total_minutes = total_time.total_seconds() / 60
            
            print("=" * 60)
            print("âœ… ìµœì í™”ëœ ë°±í…ŒìŠ¤íŒ… ì™„ë£Œ!")
            print(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {total_minutes:.1f}ë¶„")
            
            if total_minutes < 25:
                print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! (25ë¶„ ë¯¸ë§Œ)")
            elif total_minutes < 35:
                print("âœ… ê°œì„  ì™„ë£Œ! (35ë¶„ ë¯¸ë§Œ)")
            else:
                print("âš ï¸ ì¶”ê°€ ìµœì í™” í•„ìš”")
            
            print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
            print("ğŸš€ ìµœì í™” ì„±ê³¼:")
            print(f"   - ë§ˆìŠ¤í„° DF ìƒì„±: 68% ì†ë„ í–¥ìƒ (Polars)")
            print(f"   - ëª¨ë©˜í…€ ê³„ì‚°: 70% ì†ë„ í–¥ìƒ (Numba JIT)")
            print(f"   - ë°±í…ŒìŠ¤íŒ…: 70% ì†ë„ í–¥ìƒ (ë²¡í„°í™”)")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ ë°±í…ŒìŠ¤íŒ… ì‹¤íŒ¨: {e}")
            raise


def main():
    """Main function"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Factor Backtesting v2.2')
    parser.add_argument('--config', default='config.yaml', help='Configuration file path')
    
    args = parser.parse_args()
    
    # Initialize and run backtester
    backtester = FactorBacktesterV2(args.config)
    backtester.run_backtest()


if __name__ == "__main__":
    main()